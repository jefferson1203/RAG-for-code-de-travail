{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7rbhfeLnt59",
        "outputId": "0238a089-49c2-4989-d410-1753f2d6b2b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: langchain_community in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (0.3.24)\n",
            "Requirement already satisfied: langchainhub in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (0.1.21)\n",
            "Requirement already satisfied: chromadb in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (1.0.12)\n",
            "Requirement already satisfied: langchain in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (0.3.63)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (0.3.44)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain_community) (1.26.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchainhub) (23.2)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchainhub) (2.32.0.20250602)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (2.11.5)\n",
            "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.15.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.0)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2)\n",
            "Requirement already satisfied: certifi in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.6)\n",
            "Requirement already satisfied: sympy in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (75.8.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
            "Requirement already satisfied: click<8.2,>=8.0.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.22.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\utcpret\\anaconda3\\envs\\python_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Toutes les dépendances ont été installées ou mises à jour.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall -q --upgrade langchain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mToutes les dépendances ont été installées ou mises à jour.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m     13\u001b[39m userdata.get(\u001b[33m'\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m userdata.get(\u001b[33m'\u001b[39m\u001b[33mLANGCHAIN_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# Installer les dépendances nécessaires\n",
        "%pip install -q langchain-google-genai pypdf langchain-chroma faiss-cpu sentence-transformers streamlit\n",
        "%pip install langchain_community langchainhub chromadb langchain\n",
        "%pip install -q python-dotenv # Pour gérer les variables d'environnement (API key)\n",
        "%pip install -q transformers # Pour le modèle d'embedding HuggingFace\n",
        "\n",
        "# Assurons-nous que LangChain est à jour pour les dernières fonctionnalités\n",
        "%pip install -q --upgrade langchain\n",
        "\n",
        "print(\"Toutes les dépendances ont été installées ou mises à jour.\")\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzg6LMMFopbH",
        "outputId": "87f9b6ef-b6ff-4d3b-85c9-0cc4451b3e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# # Path to the pdf file\n",
        "# pdf_path = '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf'\n",
        "\n",
        "# Path to the pdf file\n",
        "pdf_path = 'Code_du_travail-23-300.pdf'  # Assurez-vous que le fichier est dans le même dossier que le notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dgpeqZ9gFmQ"
      },
      "source": [
        "# Objectif du projet\n",
        "Mettre en place un système RAG (Retrieval Augmented Generation) dédié au Code du travail, servant d’**outil de requêtage intelligent** et d’**aide à la décision** pour des boîtes de **consultation juridique**.\n",
        "\n",
        "L’objectif est de permettre aux utilisateurs (juristes, consultants, avocats juniors, etc.) de poser des questions en langage naturel et d’obtenir des réponses précises, contextualisées et appuyées par des sources juridiques fiables (textes de loi, jurisprudence, doctrine).\n",
        "\n",
        "**NB :** le RAG qui sera mis sur pied sera spécifique au droit du travail. Ce serait très ambitieux de notre part de mettre sur pied un RAG couvrant l’ensemble du droit français étant donné le temps imparti pour la réalisation de ce projet (le droit étant vaste, technique et complexe)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy13L0d9hIwF"
      },
      "source": [
        "# ETAPES CLES\n",
        "1. **Préparation de l'environnement :** Installer les outils et librairies nécessaires.\n",
        "2. **Chargement et découpage du document (Document Loading & Splitting) :**\n",
        "Chargement du pdf de 2755 pages (code du travail). Un document aussi volumineux ne peut pas être traité d'un seul bloc par un modèle de langage. Nous devrons le découper en morceaux plus petits (appelés \"chunks\" ou \"morceaux\") tout en conservant le contexte. C'est crucial pour la pertinence des recherches.\n",
        "3. **Création des embeddings (Embeddings Generation) :**\n",
        "Chaque morceau de texte sera transformé en une représentation numérique appelée \"embedding\". C'est une sorte de \"vecteur\" qui capture la signification sémantique du texte. Ces embeddings nous permettront de trouver des morceaux de texte similaires à la question poser par un utilisateur (sous forme de requête).\n",
        "3. **Stockage vectoriel avec Chroma (Vector Store with ChromaDB) :**\n",
        "Nous allons stocker ces embeddings, ainsi que le texte original des morceaux, dans une base de données vectorielle appelée Chroma.\n",
        "Chroma est optimisé pour les recherches de similarité vectorielle, ce qui est exactement ce dont nous avons besoin pour récupérer les informations pertinentes.\n",
        "4. **Construction de la chaîne RAG avec LangChain (Building the RAG Chain with LangChain) :**\n",
        "LangChain est un framework qui aide à orchestrer les différentes étapes (récupération de documents, génération de réponses).\n",
        "Nous utiliserons LangChain pour relier notre base de données vectorielle (Chroma) et notre modèle de langage (Gemini).\n",
        "5. **Intégration du modèle de langage Gemini (LLM Integration with Gemini) :**\n",
        "Gemini sera le cerveau de notre application. Il prendra les morceaux de texte pertinents et la question pour générer une réponse cohérente et informative.\n",
        "6. **Construction de l'interface utilisateur avec Streamlit (Streamlit UI) :**\n",
        "Streamlit nous permettra de créer une interface web simple et interactive où l'utilisateur pourras poser ses questions et voir les réponses, y compris les sources.\n",
        "7. **Gestion des sources/Gestion des Hallucinations (Source Retrieval) :**\n",
        "Nous nous assurerons que chaque réponse est accompagnée de l'indication de la page ou du morceau de texte d'où provient l'information.\n",
        "\n",
        "## Pourquoi ces outils ?\n",
        "\n",
        "1. **LangChain :** C'est un framework puissant qui simplifie énormément la construction d'applications basées sur les grands modèles de langage (LLMs). Il offre des composants modulaires pour le chargement de documents, le découpage, les bases de données vectorielles, et l'intégration de LLMs.\n",
        "2. **Chroma :** Une base de données vectorielle légère et facile à utiliser, parfaite pour les projets de taille moyenne et l'apprentissage. Elle est souvent utilisée avec LangChain.\n",
        "3. **Gemini :** Un modèle de langage performant de Google, capable de comprendre le langage naturel et de générer des réponses pertinentes.\n",
        "4. **Streamlit :** C'est un excellent outil pour construire rapidement des interfaces utilisateur pour des applications de machine learning et de data science, sans avoir besoin de connaissances approfondies en développement web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5860NhgZjHh-"
      },
      "source": [
        "## PARTIE INDEXATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mfp0qpoTv1BS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "class TitleBasedSplitter(TextSplitter):\n",
        "    \"\"\"\n",
        "    Splitter de texte basé sur la détection de titres.\n",
        "    Garde les métadonnées de la page d'origine pour chaque chunk.\n",
        "    \"\"\"\n",
        "    def __init__(self, pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:)\"):\n",
        "        \"\"\"\n",
        "        Initialise le splitter avec un pattern regex pour les titres.\n",
        "        Le pattern par défaut cherche \"Titre \" suivi de:\n",
        "        - Soit des chiffres romains (I, V, X...) optionnellement suivis de \"er\" ou \"ème\".\n",
        "        - Soit des mots (lettres, chiffres, accents, apostrophes, tirets).\n",
        "        - Le tout suivi d'un deux-points (avec espaces optionnels autour).\n",
        "        Exemples: \"Titre Ier :\", \"Titre préliminaire :\", \"Titre II :\"\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "    def split_documents(self, documents: List[Document], **kwargs) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Divise une liste de documents (pages) en chunks basés sur les titres.\n",
        "        Chaque chunk hérite des métadonnées de son document d'origine.\n",
        "        \"\"\"\n",
        "        all_chunks: List[Document] = []\n",
        "        for doc in documents:\n",
        "            text = doc.page_content\n",
        "            metadata = doc.metadata.copy() # Copie des métadonnées de la page d'origine\n",
        "\n",
        "            matches = list(self.compiled_pattern.finditer(text))\n",
        "\n",
        "            if not matches:\n",
        "                # Si aucun titre n'est trouvé sur la page, la page entière est un chunk\n",
        "                if text.strip(): # S'assurer que le texte n'est pas vide\n",
        "                    all_chunks.append(Document(page_content=text.strip(), metadata=metadata))\n",
        "                continue # Passer à la page suivante\n",
        "\n",
        "            # Cas où il y a du texte avant le premier titre sur la page\n",
        "            if matches[0].start() > 0:\n",
        "                pre_title_text = text[0:matches[0].start()].strip()\n",
        "                if pre_title_text:\n",
        "                    all_chunks.append(Document(page_content=pre_title_text, metadata=metadata))\n",
        "\n",
        "            for i in range(len(matches)):\n",
        "                start = matches[i].start()\n",
        "                # La fin du chunk est le début du prochain match, ou la fin du texte si c'est le dernier match\n",
        "                end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
        "\n",
        "                chunk_content = text[start:end].strip()\n",
        "\n",
        "                if chunk_content:\n",
        "                    # Ajout de métadonnées spécifiques au chunk si besoin,\n",
        "                    # par exemple le titre identifié ou une ID de chunk\n",
        "                    chunk_metadata = metadata.copy()\n",
        "                    # On peut ajouter ici des infos comme le titre spécifique du chunk\n",
        "                    # chunk_metadata[\"title\"] = matches[i].group(0).strip()\n",
        "                    all_chunks.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    # Nous n'avons pas besoin de split_text si nous utilisons split_documents directement sur des objets Document.\n",
        "    # Cependant, si l'on veux sonserver la compatibilité avec d'autres TextSplitter,\n",
        "    # l'on peut implémenter split_text pour une chaîne unique.\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Divise une chaîne de texte en chunks basés sur les titres.\n",
        "        Cette méthode est appelée par split_documents si un Document est passé.\n",
        "        \"\"\"\n",
        "        chunks = []\n",
        "        matches = list(self.compiled_pattern.finditer(text))\n",
        "\n",
        "        if not matches:\n",
        "            return [text.strip()] if text.strip() else []\n",
        "\n",
        "        # Ajouter le texte avant le premier titre comme un chunk si non vide\n",
        "        if matches[0].start() > 0:\n",
        "            pre_title_text = text[0:matches[0].start()].strip()\n",
        "            if pre_title_text:\n",
        "                chunks.append(pre_title_text)\n",
        "\n",
        "        for i in range(len(matches)):\n",
        "            start = matches[i].start()\n",
        "            end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
        "            chunk_content = text[start:end].strip()\n",
        "            if chunk_content:\n",
        "                chunks.append(chunk_content)\n",
        "        return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuxnQp4vwqxB",
        "outputId": "013a9844-bd46-4fa7-8bf5-6b57dd656458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
            "Nombre de pages chargées par PyPDFLoader : 278\n",
            "Nombre de documents (chunks) après TitleBasedSplitter : 312\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Page content (début): Partie législative - Chapitre préliminaire : Dialogue social. \n",
            "Partie législative\n",
            "Chapitre préliminaire : Dialogue social.\n",
            "L. 1  LOI n°2008-67 du 21 janvier 2008 - art. 3      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Tout projet de réforme envisagé par le Gouvernement qui porte sur les relations individuelles et collectives\n",
            "du travail, l'emploi et la formation professionnelle et qui relève du champ de la négociation nationale et\n",
            "interprofessionnelle fait l'objet d'une conc...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1'}\n",
            "Source Page: 1\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "Première partie : Les relations individuelles de travail\n",
            "Livre Ier : Dispositions préliminaires...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
            "Source Page: 2\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Page content (début): Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
            "Chapitre unique.\n",
            "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Les dispositions du présent livre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\n",
            "Elles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\n",
            "privé, sous réserve des dispositions particulières ayant le même objet rés...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
            "Source Page: 2\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "L. 1111-3  ORDONNANCE n°2015-1578 du 3 décembre 2015 - art. 1      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
            "1° Les apprentis ;\n",
            "2° Les titulaires d'un contrat initiative-emploi, pendant la durée d'attribution de l'aide financière mentionnée\n",
            "à l'article L. 5134-72 ;\n",
            "3° (Abrogé) ;\n",
            "4° Le...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
            "Source Page: 3\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Page content (début): Titre II : Droits et libertés dans l'entreprise\n",
            "Chapitre unique.\n",
            "L. 1121-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Nul ne peut apporter aux droits des personnes et aux libertés individuelles et collectives de restrictions qui ne\n",
            "seraient pas justifiées par la nature de la tâche à accomplir ni proportionnées au but recherché.\n",
            "Récemment au Bulletin de la Cour de Cassation\n",
            "> Chambre sociale, 25 Septembre 2024, n°23-11.86...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
            "Source Page: 3\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
        "else:\n",
        "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
        "\n",
        "    # 1. Charger les documents page par page\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs_from_loader = loader.load() # Chaque élément est un Document (page) avec ses métadonnées\n",
        "\n",
        "    if not docs_from_loader:\n",
        "        print(\"Aucun document n'a pu être chargé par PyPDFLoader. Veuillez vérifier le PDF.\")\n",
        "        documents = [] # L'on s'assure que 'documents' est défini même si le chargement échoue\n",
        "    else:\n",
        "        print(f\"Nombre de pages chargées par PyPDFLoader : {len(docs_from_loader)}\")\n",
        "\n",
        "        # 2. Initialiser notre splitter basé sur les titres\n",
        "        splitter = TitleBasedSplitter()\n",
        "\n",
        "        # 3. Appliquer le splitter sur chaque document (page)\n",
        "        # La méthode split_documents prend une liste de Document et retourne une liste de Document.\n",
        "        documents = splitter.split_documents(docs_from_loader)\n",
        "\n",
        "        print(f\"Nombre de documents (chunks) après TitleBasedSplitter : {len(documents)}\")\n",
        "\n",
        "        # Affichage de quelques exemples de chunks pour vérifier leur contenu et leurs métadonnées\n",
        "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
        "            print(f\"\\n--- Chunk {i+1} ---\")\n",
        "            print(f\"Page content (début): {doc_chunk.page_content[:500]}...\") # Affiche les 500 premiers caractères\n",
        "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
        "\n",
        "            # Vérifier si 'page' est dans les métadonnées\n",
        "            if 'page' in doc_chunk.metadata:\n",
        "                print(f\"Source Page: {doc_chunk.metadata['page'] + 1}\") # +1 car les pages sont souvent 0-indexées\n",
        "\n",
        "# Le code ou les fonctions des parties embeddings, vectorstore, LLM et chaîne RAG pourront suivre ici\n",
        "# NB : Il est important de s'assurer qu'il n'y a pas d'erreur dans 'documents' si aucun document n'est chargé.\n",
        "if not documents:\n",
        "    print(\"Le traitement ne peut pas continuer car aucun document n'a été créé.\")\n",
        "    sys.exit()  # Ou raise une exception ici si l'absence de documents est critique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FBsuWQ22v-AY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "class CodeDuTravailStructureExtractor(TextSplitter):\n",
        "    \"\"\"\n",
        "    Splitter et extracteur de structure pour le Code du Travail.\n",
        "    Découpe le document en chunks et enrichit les métadonnées\n",
        "    avec les informations de Partie, Livre, Titre, Chapitre, Section et Article.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        # Pattern pour les titres principaux (Titre Ier :, Titre Préliminaire :)\n",
        "        title_pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
        "        # Pattern pour les chapitres (Chapitre Ier :, Chapitre Unique :)\n",
        "        chapter_pattern: str = r\"(Chapitre\\s+(?:[IVXLCDM]+(?:er|ème)?|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
        "        # Pattern pour les sections (Section 1 :, Section unique :)\n",
        "        section_pattern: str = r\"(Section\\s+(?:\\d+|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
        "        # Pattern pour les articles (L. 123-1, R. 456-7, D. 789-10)\n",
        "        article_pattern: str = r\"(Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)[\\s\\S]*?(?=Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)|Titre\\s+|Chapitre\\s+|Section\\s+|$))\",\n",
        "        # Capture le numéro de l'article dans un groupe pour extraction facile\n",
        "        article_num_capture_group: int = 2, # Le groupe qui contient \"L. 123-1\"\n",
        "        keep_separator: bool = True, # Indique si le séparateur (titre/chapitre/article) doit faire partie du chunk\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(keep_separator=keep_separator, **kwargs)\n",
        "        self.title_pattern = re.compile(title_pattern, re.IGNORECASE | re.DOTALL)\n",
        "        self.chapter_pattern = re.compile(chapter_pattern, re.IGNORECASE | re.DOTALL)\n",
        "        self.section_pattern = re.compile(section_pattern, re.IGNORECASE | re.DOTALL)\n",
        "        self.article_pattern = re.compile(article_pattern, re.IGNORECASE | re.DOTALL)\n",
        "        self.article_num_capture_group = article_num_capture_group\n",
        "\n",
        "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Divise les documents (pages) en chunks et enrichit les métadonnées.\n",
        "        \"\"\"\n",
        "        all_chunks: List[Document] = []\n",
        "        current_book = None\n",
        "        current_part = None\n",
        "        current_title = None\n",
        "        current_chapter = None\n",
        "        current_section = None\n",
        "\n",
        "        # Pour le Code du Travail, la \"Partie législative\" et \"Partie réglementaire\"\n",
        "        # sont souvent des sections de haut niveau. On pourrait les détecter aussi.\n",
        "        # Par exemple: r\"(Partie\\s+(?:législative|réglementaire)\\s*:\\s*.+?)(?=\\n(?:Partie\\s|Livre\\s|Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\"\n",
        "\n",
        "        for doc in documents:\n",
        "            text = doc.page_content\n",
        "            page_metadata = doc.metadata.copy() # Copier les métadonnées de la page (ex: page number)\n",
        "\n",
        "            # Il est crucial de détecter les éléments hiérarchiques dans l'ordre décroissant\n",
        "            # (Partie -> Livre -> Titre -> Chapitre -> Section -> Article)\n",
        "            # pour s'assurer que les métadonnées sont correctement propagées.\n",
        "\n",
        "            # Nous allons traiter le texte de la page pour identifier les blocs.\n",
        "            # L'idée est de trouver tous les matchs de titres, chapitres, sections, articles\n",
        "            # et de les traiter séquentiellement.\n",
        "\n",
        "            # Simple approche pour commencer: On va splitter principalement par article\n",
        "            # et ensuite extraire les informations de titre/chapitre/section\n",
        "            # qui précèdent cet article ou sont détectées dans le chunk de l'article.\n",
        "\n",
        "            # Cette méthode nécessite une logique plus complexe pour propager le contexte\n",
        "            # (quel titre/chapitre actuel s'applique à un article).\n",
        "            # On va utiliser une approche par \"contexte courant\" qui est mise à jour à chaque détection.\n",
        "\n",
        "            # Initialisation du contexte pour cette page (on garde le dernier contexte connu)\n",
        "            # Pour la première page, ces valeurs seront None.\n",
        "            # Pour les pages suivantes, elles hériteront des valeurs de la fin de la page précédente.\n",
        "\n",
        "            # TODO: Implémenter la logique de propagation du contexte inter-pages\n",
        "            # Pour l'instant, on suppose que les titres sont répétés ou que les chunks sont suffisamment petits\n",
        "            # pour qu'un article soit dans son contexte de titre/chapitre.\n",
        "            # C'est un point clé à affiner si les sources ne sont pas précises.\n",
        "\n",
        "            # Découpage par Article principalement\n",
        "            article_matches = list(self.article_pattern.finditer(text))\n",
        "\n",
        "            last_idx = 0\n",
        "            # Si du texte précède le premier article\n",
        "            if article_matches and article_matches[0].start() > 0:\n",
        "                pre_article_text = text[0:article_matches[0].start()].strip()\n",
        "                if pre_article_text:\n",
        "                    # Tentative d'extraction des infos de titre/chapitre/section pour ce bloc introductif\n",
        "                    # C'est une simplification, idéalement on détecterait ces éléments en amont du split.\n",
        "                    chunk_metadata = self._extract_hierarchy_metadata(pre_article_text, page_metadata)\n",
        "                    all_chunks.append(Document(page_content=pre_article_text, metadata=chunk_metadata))\n",
        "                last_idx = article_matches[0].start()\n",
        "\n",
        "            for i, match in enumerate(article_matches):\n",
        "                article_content = match.group(0).strip() # Le contenu complet de l'article (incluant le numéro)\n",
        "                article_number = match.group(self.article_num_capture_group) # Le numéro de l'article\n",
        "\n",
        "                # Ici, nous pourrions affiner l'extraction de la date ou de la loi spécifique\n",
        "                # si un pattern est détectable dans l'article_content\n",
        "                # Par exemple: LOI n° 2014-288 du 5 mars 2014-art. 29 (V)\n",
        "                # pattern_date_loi = r\"(LOI n°\\s*\\d{4}-\\d+\\s*du\\s+\\d{1,2}\\s+\\w+\\s+\\d{4})\"\n",
        "                # date_loi_match = re.search(pattern_date_loi, article_content)\n",
        "                # if date_loi_match:\n",
        "                #     article_metadata[\"date_loi\"] = date_loi_match.group(1)\n",
        "\n",
        "                chunk_metadata = page_metadata.copy()\n",
        "                chunk_metadata[\"type\"] = \"Article\"\n",
        "                chunk_metadata[\"article_number\"] = article_number\n",
        "\n",
        "                # Tenter d'extraire le contexte hiérarchique pour cet article\n",
        "                # On recherche les titres, chapitres, sections qui précèdent immédiatement cet article\n",
        "                # dans la portion de texte traitée pour cette page.\n",
        "\n",
        "                # C'est une simplification pour l'exemple.\n",
        "                # Une vraie solution robuste impliquerait de parser le document séquentiellement\n",
        "                # et de maintenir un état des \"titres actifs\" et \"chapitres actifs\"\n",
        "                # au fur et à mesure que l'on lit le document.\n",
        "\n",
        "                # Pour l'instant, on fait une extraction locale:\n",
        "                chunk_metadata.update(self._extract_hierarchy_metadata(article_content, page_metadata))\n",
        "\n",
        "                all_chunks.append(Document(page_content=article_content, metadata=chunk_metadata))\n",
        "                last_idx = match.end()\n",
        "\n",
        "            # Si du texte reste après le dernier article sur la page\n",
        "            if last_idx < len(text):\n",
        "                remaining_text = text[last_idx:].strip()\n",
        "                if remaining_text:\n",
        "                    chunk_metadata = self._extract_hierarchy_metadata(remaining_text, page_metadata)\n",
        "                    all_chunks.append(Document(page_content=remaining_text, metadata=chunk_metadata))\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def _extract_hierarchy_metadata(self, text: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Tente d'extraire les informations de hiérarchie (Titre, Chapitre, Section)\n",
        "        à partir d'un morceau de texte et les ajoute aux métadonnées.\n",
        "        \"\"\"\n",
        "        metadata = base_metadata.copy()\n",
        "\n",
        "        # Chercher le titre le plus proche\n",
        "        title_match = self.title_pattern.search(text)\n",
        "        if title_match:\n",
        "            metadata[\"title\"] = title_match.group(1).strip()\n",
        "            # On peut aussi essayer de parser le numéro et le nom si le format est fixe\n",
        "            # Ex: \"Titre Ier : Dispositions générales\" -> {\"title_num\": \"Ier\", \"title_name\": \"Dispositions générales\"}\n",
        "\n",
        "        # Chercher le chapitre le plus proche\n",
        "        chapter_match = self.chapter_pattern.search(text)\n",
        "        if chapter_match:\n",
        "            metadata[\"chapter\"] = chapter_match.group(1).strip()\n",
        "            # Ex: \"Chapitre Ier : Objet et portée\" -> {\"chapter_num\": \"Ier\", \"chapter_name\": \"Objet et portée\"}\n",
        "\n",
        "        # Chercher la section la plus proche\n",
        "        section_match = self.section_pattern.search(text)\n",
        "        if section_match:\n",
        "            metadata[\"section\"] = section_match.group(1).strip()\n",
        "            # Ex: \"Section 1 : Définitions\" -> {\"section_num\": \"1\", \"section_name\": \"Définitions\"}\n",
        "\n",
        "        # Pour les livres et parties, c'est plus complexe car ils sont souvent en début de document\n",
        "        # ou indiqués par des en-têtes/pieds de page non extraits par `page_content`.\n",
        "        # Pour le \"Livre\" et la \"Partie\", il est souvent plus efficace de les extraire\n",
        "        # soit manuellement au début, soit en faisant une passe initiale sur le document complet\n",
        "        # pour établir la hiérarchie.\n",
        "        # Pour l'instant, je ne les inclus pas dans _extract_hierarchy_metadata,\n",
        "        # car un chunk d'article ne les contiendra pas nécessairement.\n",
        "        # On pourrait les ajouter comme des métadonnées globales au document si elles sont fixes,\n",
        "        # ou les propager depuis un parseur de document entier.\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    # La méthode split_text n'est pas utilisée directement par split_documents,\n",
        "    # mais je la garde pour la conformité avec la classe mère, même si elle n'est pas optimisée\n",
        "    # pour notre usage actuel d'extraction de métadonnées.\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        # Cette implémentation est plus simple, mais ne gère pas la propagation des métadonnées\n",
        "        # C'est pourquoi nous utiliserons principalement split_documents.\n",
        "        chunks = []\n",
        "        # On va splitter par les articles pour avoir des chunks plus petits\n",
        "        article_splits = self.article_pattern.split(text)\n",
        "\n",
        "        # Si le premier élément n'est pas un séparateur, c'est un préambule\n",
        "        if len(article_splits) > 0 and not self.article_pattern.match(article_splits[0]):\n",
        "            if article_splits[0].strip():\n",
        "                chunks.append(article_splits[0].strip())\n",
        "            start_index = 1\n",
        "        else:\n",
        "            start_index = 0\n",
        "\n",
        "        # Réassembler les articles avec leur numéro\n",
        "        for i in range(start_index, len(article_splits), self.article_num_capture_group + 1):\n",
        "            if i + self.article_num_capture_group < len(article_splits):\n",
        "                article_num = article_splits[i + self.article_num_capture_group -1] # Le numéro de l'article capturé\n",
        "                article_content = article_splits[i + self.article_num_capture_group] # Le contenu après le numéro\n",
        "\n",
        "                full_article_chunk = f\"Article {article_num.strip()} {article_content.strip()}\"\n",
        "                if full_article_chunk.strip():\n",
        "                    chunks.append(full_article_chunk.strip())\n",
        "            elif article_splits[i].strip():\n",
        "                chunks.append(article_splits[i].strip())\n",
        "        return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGKhQZx6wLzW",
        "outputId": "7247837e-089a-4ad9-f528-bb0a92266cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
            "Nombre de pages chargées par PyPDFLoader : 278\n",
            "Nombre de documents (chunks enrichis) après extraction de structure : 411\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Page content (début): Partie législative - Chapitre préliminaire : Dialogue social. \n",
            "Partie législative\n",
            "Chapitre préliminaire : Dialogue social.\n",
            "L. 1  LOI n°2008-67 du 21 janvier 2008 - art. 3      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Tout projet de réforme envisagé par le Gouvernement qui porte sur les relations individuelles et collectives\n",
            "du travail, l'emploi et la formation professionnelle et qui relève du champ de la négociation nationale et\n",
            "interprofessionnelle fait l'objet d'une conc...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1', 'chapter': 'Chapitre préliminaire : Dialogue social. \\nPartie législative'}\n",
            "  Source Page: 1\n",
            "  Chapitre: Chapitre préliminaire : Dialogue social. \n",
            "Partie législative\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "Première partie : Les relations individuelles de travail\n",
            "Livre Ier : Dispositions préliminaires\n",
            "Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
            "Chapitre unique.\n",
            "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Les dispositions du présent livre sont applicables aux employeurs de droit...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2', 'title': \"Titre Ier : Champ d'application et calcul des seuils d'effectifs\"}\n",
            "  Source Page: 2\n",
            "  Titre: Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "L. 1111-3  ORDONNANCE n°2015-1578 du 3 décembre 2015 - art. 1      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
            "1° Les apprentis ;\n",
            "2° Les titulaires d'un contrat initiative-emploi, pendant la durée d'attribution de l'aide financière mentionnée\n",
            "à l'article L. 5134-72 ;\n",
            "3° (Abrogé) ;\n",
            "4° Le...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3', 'title': \"Titre II : Droits et libertés dans l'entreprise\"}\n",
            "  Source Page: 3\n",
            "  Titre: Titre II : Droits et libertés dans l'entreprise\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "de qualification, de classification, de promotion professionnelle, d'horaires de travail, d'évaluation de la\n",
            "performance, de mutation ou de renouvellement de contrat, ni de toute autre mesure mentionnée au II de\n",
            "l'article 10-1 de la loi n° 2016-1691 du 9 décembre 2016 relative à la transparence, à la lutte contre la corruption\n",
            "et à la modernisation de la vie économique, pour avo...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 3, 'page_label': '4', 'title': 'Titre III : Discriminations', 'chapter': \"Chapitre Ier : Champ d'application.\\nL. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nLes dispositions du présent titre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\\nElles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\\nprivé.\\nL. 1131-2  LOI n°2017-86 du 27 janvier 2017 - art. 214      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nDans toute entreprise employant au moins trois cents salariés et dans toute entreprise spécialisée dans le\\nrecrutement, les employés chargés des missions de recrutement reçoivent une formation à la non-discrimination\\nà l'embauche au moins une fois tous les cinq ans.\"}\n",
            "  Source Page: 4\n",
            "  Titre: Titre III : Discriminations\n",
            "  Chapitre: Chapitre Ier : Champ d'application.\n",
            "L. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Les dispositions du présent titre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\n",
            "Elles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\n",
            "privé.\n",
            "L. 1131-2  LOI n°2017-86 du 27 janvier 2017 - art. 214      \n",
            "  Legif.   \n",
            "  Plan   \n",
            "  Jp.Judi.   \n",
            "  Jp.Admin.   \n",
            "  Juricaf  \n",
            "Dans toute entreprise employant au moins trois cents salariés et dans toute entreprise spécialisée dans le\n",
            "recrutement, les employés chargés des missions de recrutement reçoivent une formation à la non-discrimination\n",
            "à l'embauche au moins une fois tous les cinq ans.\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
            "Récemment au Bulletin de la Cour de Cassation\n",
            "> Chambre sociale, 24 Avril 2024, n°22-20.539, (B)\n",
            "> Chambre sociale, 20 Décembre 2023, n°22-12.381, (B)\n",
            "> Chambre sociale, 20 Septembre 2023, n°22-12.293, (B)\n",
            "> Chambre sociale, 28 Juin 2023, n°22-11.699, (B)\n",
            "> Chambre sociale, 01 Juin 2023, n°21-21.191, (B)\n",
            "service-public.fr\n",
            "> Droit de grève d'un salarié du secteur privé : Interdic...\n",
            "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 4, 'page_label': '5'}\n",
            "  Source Page: 5\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
        "    documents = [] # Assure-toi que 'documents' est défini même si le fichier est manquant\n",
        "else:\n",
        "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
        "\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    # Chargement page par page pour conserver les métadonnées de page\n",
        "    docs_from_loader = loader.load()\n",
        "\n",
        "    if not docs_from_loader:\n",
        "        print(\"Aucun document n'a pu être chargé par PyPDFLoader. Veuillez vérifier le PDF.\")\n",
        "        documents = []\n",
        "    else:\n",
        "        print(f\"Nombre de pages chargées par PyPDFLoader : {len(docs_from_loader)}\")\n",
        "\n",
        "        # Initialiser notre extracteur de structure\n",
        "        # On va créer des patterns plus robustes si le test initial ne donne pas satisfaction\n",
        "        # Les patterns par défaut dans la classe sont un bon point de départ.\n",
        "        structure_extractor = CodeDuTravailStructureExtractor()\n",
        "\n",
        "        # Appliquer l'extracteur sur les documents (pages)\n",
        "        documents = structure_extractor.split_documents(docs_from_loader)\n",
        "\n",
        "        print(f\"Nombre de documents (chunks enrichis) après extraction de structure : {len(documents)}\")\n",
        "\n",
        "        # Afficher quelques exemples de chunks enrichis\n",
        "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
        "            print(f\"\\n--- Chunk {i+1} ---\")\n",
        "            print(f\"Page content (début): {doc_chunk.page_content[:500]}...\")\n",
        "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
        "            # Exemple de vérification des métadonnées spécifiques\n",
        "            if 'page' in doc_chunk.metadata:\n",
        "                print(f\"  Source Page: {doc_chunk.metadata['page'] + 1}\")\n",
        "            if 'type' in doc_chunk.metadata:\n",
        "                print(f\"  Type de chunk: {doc_chunk.metadata['type']}\")\n",
        "            if 'article_number' in doc_chunk.metadata:\n",
        "                print(f\"  Numéro d'Article: {doc_chunk.metadata['article_number']}\")\n",
        "            if 'title' in doc_chunk.metadata:\n",
        "                print(f\"  Titre: {doc_chunk.metadata['title']}\")\n",
        "            if 'chapter' in doc_chunk.metadata:\n",
        "                print(f\"  Chapitre: {doc_chunk.metadata['chapter']}\")\n",
        "\n",
        "# Assurez-vous que 'documents' est défini pour les étapes suivantes même en cas d'échec de chargement\n",
        "if not documents:\n",
        "    print(\"Le traitement ne peut pas continuer car aucun document structuré n'a été créé.\")\n",
        "    sys.exit()  # Ou raise une exception ici si l'absence de documents est critique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_mbxTrjnipO"
      },
      "source": [
        "## PARTIES EMBEDDING, STOCKAGE VECTORIELLE, CHAINE RAG, LLM\n",
        "\n",
        "### Pourquoi découper le document (Chunking) ?\n",
        "\n",
        "- **Limitation des LLM :** Les grands modèles de langage ont une \"fenêtre de contexte\" limitée. C'est la quantité de texte qu'ils peuvent lire et traiter en une seule fois. Un PDF de 2755 pages est bien au-delà de ce qu'ils peuvent gérer.\n",
        "- **Pertinence de la recherche :** Si l'on cherche un concept dans 2775 pages, la recherche serait très lente et peu précise.\n",
        "_Solution :_\n",
        "**Le découpage (Chunking)** - Nous allons diviser le PDF en petits morceaux (des paragraphes, quelques phrases, etc.). Chaque morceau est suffisamment petit pour être traité par un LLM et suffisamment grand pour conserver du sens.\n",
        "_Conséquence :_ Quand un utilisateur posera une question, les morceaux les plus pertinenets seront chercher et non l'intégralité du document.\n",
        "\n",
        "\n",
        "### Qu'est-ce qu'un Embedding et pourquoi en a-t-on besoin ?\n",
        "Imaginons que chaque mot, phrase ou morceau de texte peut être représenté par un point dans un espace multidimensionnel (un peu comme des coordonnées X, Y, Z, mais avec beaucoup plus de dimensions).\n",
        "\n",
        "- **Transformation en nombres :** Un \"embedding\" est une suite de nombres (un vecteur) qui représente la signification sémantique d'un texte. Des textes qui ont un sens similaire seront représentés par des vecteurs \"proches\" dans cet espace.\n",
        "- **Recherche de similarité :** La question de l'utilisateur sera transformée en embedding. Ensuite, une recherche des embeddings de morceaux de texte qui sont les plus \"proches\" (on pourra notamment faire usage de l'algorithme des K-plus proches voisins pour récupérer les informations les plus pertinentes) de l'embedding da la question se fera dans la base de données. C'est la base de la recherche sémantique.\n",
        "\n",
        "_Pourquoi ?_ Un ordinateur ne \"comprend\" pas le texte. Il comprend les nombres. Les embeddings sont le pont entre le langage humain et la capacité de l'ordinateur à trouver des similarités de sens.\n",
        "\n",
        "### Le rôle de la base de données vectorielle (ChromaDB)\n",
        "Une fois que nous avons nos morceaux de texte et leurs embeddings, où les stockons-nous de manière efficace pour pouvoir les rechercher rapidement ?\n",
        "\n",
        "- **Base de données traditionnelle vs. Base de données vectorielle :** Une base de données classique (comme SQL) est optimisée pour des recherches exactes (ex: \"trouve tous les utilisateurs dont le nom est 'Dupont'\"). Une base de données vectorielle est optimisée pour des recherches de similarité (ex: \"trouve tous les textes qui sont similaires à 'licenciement abusif'\").\n",
        "\n",
        "**ChromaDB :** C'est une base de données vectorielle légère et facile à mettre en place. Elle va stocker :\n",
        "- Les morceaux de texte bruts (le contenu original).\n",
        "- Les embeddings correspondants (les vecteurs numériques).\n",
        "- Des métadonnées (comme le numéro de page d'où vient le morceau, ce qui est crucial pour les sources !).\n",
        "_Fonctionnement :_ Quand l'utilisateur pose une question, Chroma va rapidement identifier les embeddings les plus similaires à sa question, et lui retourner les morceaux de texte correspondants.\n",
        "\n",
        "\n",
        "### LangChain : L'orchestrateur de notre application\n",
        "LangChain est un framework puissant qui simplifie énormément la construction d'applications basées sur les grands modèles de langage.\n",
        "\n",
        "_Concept de \"Chains\" (Chaînes) :_ LangChain permet de relier différents composants (comme le chargement de documents, les découpeurs, les modèles d'embeddings, les bases de données vectorielles, et les LLM) dans une séquence logique, une \"chaîne\".\n",
        "\n",
        "- **Modularité :** Chaque partie de l'application RAG (chargement, découpage, embedding, recherche, génération) est un \"maillon\" de la chaîne. LangChain permet de les assembler facilement.\n",
        "- **Simplification :** Au lieu de gérer manuellement chaque interaction entre ces composants, LangChain fournit des abstractions qui rendent le développement plus rapide et plus propre.\n",
        "\n",
        "\n",
        "### Gemini : Le cerveau qui génère la réponse\n",
        "Gemini est le modèle de langage (LLM) de Google que nous allons utiliser.\n",
        "\n",
        "_Son rôle :_ Une fois que Chroma a récupéré les morceaux de texte les plus pertinents du document/PDF, Gemini va recevoir :\n",
        "- La question.\n",
        "- Les morceaux de texte pertinents.\n",
        "\n",
        "_Sa tâche :_ Il va analyser ces informations et générer une réponse cohérente, synthétisée, et basée sur le contexte fourni. C'est là que la \"Génération\" de RAG entre en jeu.\n",
        "- **API :** Nous interagirons avec Gemini via son API (Interface de Programmation d'Application), ce qui signifie que nous enverrons des requêtes à un service Google pour obtenir des réponses.\n",
        "\n",
        "### Streamlit : L'interface utilisateur simple et rapide\n",
        "Pour interagir avec notre application, nous avons besoin d'une interface.\n",
        "\n",
        "####Qu'est-ce que c'est ?\n",
        "**Streamlit** est une bibliothèque Python qui permet de créer très facilement des applications web interactives pour la science des données et le machine learning.\n",
        "_Avantages :_ Pas besoin d'être un expert en développement web (HTML, CSS, JavaScript). Avec quelques lignes de Python, tu peux créer des widgets (champs de texte, boutons) et afficher des résultats.\n",
        "\n",
        "_Notre utilisation :_ Nous l'utiliserons pour créer une boîte de texte où l'utilisateur pourras taper ses questions, un bouton pour les soumettre, et un espace pour afficher la réponse de Gemini et les sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUHCkR0exto3",
        "outputId": "37a95f42-fc61-4d19-a804-ff9121de0586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Étape 4 : Création des Embeddings ---\n",
            "Initialisation du modèle d'embedding : all-MiniLM-L6-v2\n",
            "Modèle d'embedding initialisé avec succès.\n",
            "\n",
            "--- Étape 5 : Création ou chargement du Vectorstore Chroma ---\n",
            "Chargement du vectorstore Chroma existant depuis : ./chroma_db_codetravail\n",
            "Vectorstore Chroma chargé.\n",
            "Retriever initialisé pour récupérer les 5 documents les plus pertinents.\n",
            "\n",
            "--- Étape 6 : Configuration du LLM (Gemini) ---\n",
            "Chargement du prompt RAG depuis Langchain Hub...\n",
            "Prompt chargé.\n",
            "Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\n",
            "LLM Gemini-Pro initialisé avec succès.\n",
            "\n",
            "--- Étape 7 : Construction de la Chaîne RAG ---\n",
            "Chaîne RAG créée, prête à inclure les sources.\n",
            "\n",
            "--- Étape 8 : Tester la Chaîne RAG ---\n",
            "\n",
            "Question : Quelles sont les fonctions du code du travail? Et quelles sont les conditions de représentativité des organisations syndicales ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERREUR lors de l'invocation de la chaîne RAG : 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
            "Veuillez vérifier les étapes précédentes (authentification LLM, initialisation de Chroma).\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import hub\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "\n",
        "\n",
        "# S'assurer que 'documents' n'est pas vide avant de continuer\n",
        "if not documents:\n",
        "    print(\"ATTENTION : La liste 'documents' est vide. Le processus RAG ne peut pas continuer.\")\n",
        "    exit(\"Processus arrêté car aucun document n'a été traité.\")\n",
        "\n",
        "print(\"\\n--- Étape 4 : Création des Embeddings ---\")\n",
        "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "print(f\"Initialisation du modèle d'embedding : {embedding_model_name}\")\n",
        "\n",
        "try:\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "    print(\"Modèle d'embedding initialisé avec succès.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERREUR lors de l'initialisation du modèle d'embedding : {e}\")\n",
        "    print(\"Veuillez vérifier votre connexion internet et les dépendances 'sentence-transformers'.\")\n",
        "    exit(\"Processus arrêté car le modèle d'embedding n'a pas pu être initialisé.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Étape 5 : Création ou chargement du Vectorstore Chroma ---\")\n",
        "chroma_db_dir = \"./chroma_db_codetravail\"\n",
        "\n",
        "if os.path.exists(chroma_db_dir) and os.listdir(chroma_db_dir):\n",
        "    print(f\"Chargement du vectorstore Chroma existant depuis : {chroma_db_dir}\")\n",
        "    try:\n",
        "        vectorstore = Chroma(persist_directory=chroma_db_dir, embedding_function=embedding_model)\n",
        "        print(\"Vectorstore Chroma chargé.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERREUR lors du chargement du vectorstore Chroma : {e}\")\n",
        "        print(\"Tentative de recréation du vectorstore...\")\n",
        "        vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
        "        print(\"Vectorstore Chroma recréé.\")\n",
        "else:\n",
        "    print(f\"Création d'un nouveau vectorstore Chroma dans : {chroma_db_dir}\")\n",
        "    vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
        "    print(\"Vectorstore Chroma créé.\")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(f\"Retriever initialisé pour récupérer les {retriever.search_kwargs['k']} documents les plus pertinents.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Étape 6 : Configuration du LLM (Gemini) ---\")\n",
        "print(\"Chargement du prompt RAG depuis Langchain Hub...\")\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "print(\"Prompt chargé.\")\n",
        "\n",
        "print(\"Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\")\n",
        "try:\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.2)\n",
        "    print(\"LLM Gemini-Pro initialisé avec succès.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERREUR lors de l'initialisation du LLM ChatGoogleGenerativeAI : {e}\")\n",
        "    print(\"Assurez-vous que GOOGLE_API_KEY est correctement définie dans les secrets de Colab.\")\n",
        "    print(\"Vérifiez aussi que l'API Generative Language est activée pour votre projet Google Cloud.\")\n",
        "    exit(\"Processus arrêté car le LLM n'a pas pu être initialisé.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Étape 7 : Construction de la Chaîne RAG ---\")\n",
        "\n",
        "def format_docs_with_sources(docs: List[Document]) -> str:\n",
        "    formatted_content = \"\"\n",
        "    unique_pages = set()\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        formatted_content += f\"Contenu source {i+1}:\\n{doc.page_content}\\n\\n\"\n",
        "        if 'page' in doc.metadata:\n",
        "            page_number = doc.metadata['page'] + 1\n",
        "            unique_pages.add(str(page_number))\n",
        "\n",
        "    if unique_pages:\n",
        "        formatted_content += f\"Sources des pages: {', '.join(sorted(list(unique_pages)))}\\n\"\n",
        "\n",
        "    return formatted_content\n",
        "\n",
        "# --- NOUVELLE DÉFINITION DE LA CHAÎNE RAG AVEC RUNNABLEPARALLEL ET RUNNABLELAMBDA ---\n",
        "retrieval_chain = {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "generation_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "rag_chain_with_sources = RunnableParallel(\n",
        "    response=retrieval_chain | generation_chain,\n",
        "    # Ici, nous enveloppons le lambda dans RunnableLambda\n",
        "    source_documents=retrieval_chain | RunnableLambda(lambda x: x[\"context\"])\n",
        ").with_config(run_name=\"RAG Chain with Sources\")\n",
        "\n",
        "print(\"Chaîne RAG créée, prête à inclure les sources.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Étape 8 : Tester la Chaîne RAG ---\")\n",
        "\n",
        "question = \"Quelles sont les fonctions du code du travail? Et quelles sont les conditions de représentativité des organisations syndicales ?\"\n",
        "print(f\"\\nQuestion : {question}\")\n",
        "\n",
        "try:\n",
        "    result = rag_chain_with_sources.invoke(question)\n",
        "\n",
        "    print(\"\\nRéponse Générée par Gemini :\")\n",
        "    print(result[\"response\"])\n",
        "\n",
        "    print(\"\\nSources (extraits et métadonnées) :\")\n",
        "    print(format_docs_with_sources(result[\"source_documents\"]))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERREUR lors de l'invocation de la chaîne RAG : {e}\")\n",
        "    print(\"Veuillez vérifier les étapes précédentes (authentification LLM, initialisation de Chroma).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmhBxVVzpJlF"
      },
      "source": [
        "## UTILITE DU PROJET\n",
        "1. **Qu'est-ce qu'une application RAG ? (Retrieval Augmented Generation)**\n",
        "Imaginons une immense bibliothèque (dans notre cas, un PDF de 2775 pages sur le droit du travail) et qu'un utilisateur pose une question très spécifique.\n",
        "\n",
        "**Sans RAG :** Si l'utilisateur posait cette question à un grand modèle de langage (LLM) comme Gemini sans RAG, il essayerait de répondre uniquement avec ce qu'il a appris pendant son entraînement. Le problème, c'est que Gemini n'a pas été spécifiquement entraîné sur le document de droit du travail. Il pourrait inventer des choses (\"halluciner\") ou donner des réponses génériques, car il ne \"connaît\" pas les détails du documents et donc du droit du travail.\n",
        "\n",
        "**Avec RAG :** Le RAG résout ce problème en ajoutant une étape cruciale : _la recherche d'informations (Retrieval)_ avant la _génération de la réponse (Generation)_.\n",
        "\n",
        "- **Recherche (Retrieval) :** Quand l'utilisateur poses une question, l'application RAG va d'abord chercher les passages les plus pertinents du document (corpus) qui sont susceptibles de contenir la réponse.\n",
        "- **Augmentation (Augmented) :** Ces passages pertinents sont ensuite donnés au LLM (Gemini) en même temps que ta question. Le LLM est alors \"augmenté\" avec des connaissances spécifiques.\n",
        "- **Génération (Generation) :** Fort de ces informations contextuelles, le LLM peut maintenant générer une réponse précise et basée sur les faits extraits de ton document."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
