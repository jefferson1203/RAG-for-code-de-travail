{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60162,
     "status": "ok",
     "timestamp": 1749897484933,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "O7rbhfeLnt59",
    "outputId": "ab16d58b-c9fd-4957-d528-f18e4e2e7901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/dist-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.12)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.65)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.4.20250611)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.5)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.22.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour.\n"
     ]
    }
   ],
   "source": [
    "# Installation des d√©pendances n√©cessaires\n",
    "!pip install -q langchain-google-genai pypdf langchain-chroma faiss-cpu sentence-transformers streamlit\n",
    "!pip install langchain_community langchainhub chromadb langchain\n",
    "!pip install -q python-dotenv # Pour g√©rer les variables d'environnement (API key)\n",
    "!pip install -q transformers # Pour le mod√®le d'embedding HuggingFace\n",
    "\n",
    "# Assurons-nous que LangChain est √† jour pour les derni√®res fonctionnalit√©s\n",
    "!pip install -q --upgrade langchain\n",
    "\n",
    "print(\"Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour.\")\n",
    "\n",
    "from google.colab import userdata\n",
    "userdata.get('GOOGLE_API_KEY')\n",
    "userdata.get('LANGCHAIN_API_KEY')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1766,
     "status": "ok",
     "timestamp": 1749897486731,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "fzg6LMMFopbH",
    "outputId": "57c50f12-dfed-4bb6-e236-f68e0d59a113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Path to the pdf file\n",
    "pdf_path = '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf' # chemin d'acc√®s au document qui constitura notre corpus\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dgpeqZ9gFmQ"
   },
   "source": [
    "# Objectif du projet\n",
    "L'objectif de ce projet est celui de la mise en place d'un syst√®me RAG (Retrieval Augmented Generation) d√©di√© √† la partie l√©gislative du Code du travail (plus pr√©cisement jusqu'√† la Partie II - Livre II : La n√©gociation collective - Les conventions et accords collectifs de travail, Sous-section 3, paragraphe 1er - Egalit√© professionnelle entre les femmes et les hommes), servant d‚Äô**outil de requ√™tage intelligent** et d‚Äô**aide √† la d√©cision** pour des bo√Ætes de **consultation juridique**.\n",
    "\n",
    "L‚Äôobjectif est de permettre aux utilisateurs (juristes, consultants, avocats juniors, etc.) de poser des questions en langage naturel et d‚Äôobtenir des r√©ponses pr√©cises, contextualis√©es et appuy√©es par des sources juridiques fiables (textes de loi, jurisprudence, doctrine).\n",
    "\n",
    "**NB :** le RAG qui sera mis sur pied sera sp√©cifique a un sous ensemble de la partie l√©gislative du code droit du travail. Ce serait tr√®s ambitieux de notre part de mettre sur pied un RAG couvrant l‚Äôensemble du droit fran√ßais √©tant donn√© le temps imparti pour la r√©alisation de ce projet (le droit √©tant vaste, technique et complexe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy13L0d9hIwF"
   },
   "source": [
    "# ETAPES CLES\n",
    "1. **Pr√©paration de l'environnement :** Installer les outils et librairies n√©cessaires.\n",
    "2. **Chargement et d√©coupage du document (Document Loading & Splitting) :**\n",
    "Chargement du pdf de 278 pages (code du travail). Un document aussi volumineux ne peut pas √™tre trait√© d'un seul bloc par un mod√®le de langage. Nous devrons le d√©couper en morceaux plus petits (appel√©s \"chunks\" ou \"morceaux\") tout en conservant le contexte. C'est crucial pour la pertinence des recherches.\n",
    "3. **Cr√©ation des embeddings (Embeddings Generation) :**\n",
    "Chaque morceau de texte sera transform√© en une repr√©sentation num√©rique appel√©e \"embedding\". C'est une sorte de \"vecteur\" qui capture la signification s√©mantique du texte. Ces embeddings nous permettront de trouver des morceaux de texte similaires √† la question poser par un utilisateur (sous forme de requ√™te).\n",
    "3. **Stockage vectoriel avec Chroma (Vector Store with ChromaDB) :**\n",
    "Nous allons stocker ces embeddings, ainsi que le texte original des morceaux, dans une base de donn√©es vectorielle appel√©e Chroma.\n",
    "Chroma est optimis√© pour les recherches de similarit√© vectorielle, ce qui est exactement ce dont nous avons besoin pour r√©cup√©rer les informations pertinentes.\n",
    "4. **Construction de la cha√Æne RAG avec LangChain (Building the RAG Chain with LangChain) :**\n",
    "LangChain est un framework qui aide √† orchestrer les diff√©rentes √©tapes (r√©cup√©ration de documents, g√©n√©ration de r√©ponses).\n",
    "Nous utiliserons LangChain pour relier notre base de donn√©es vectorielle (Chroma) et notre mod√®le de langage (Gemini).\n",
    "5. **Int√©gration du mod√®le de langage Gemini (LLM Integration with Gemini) :**\n",
    "Gemini sera le cerveau de notre application. Il prendra les morceaux de texte pertinents et la question pour g√©n√©rer une r√©ponse coh√©rente et informative.\n",
    "6. **Construction de l'interface utilisateur avec Streamlit (Streamlit UI) :**\n",
    "Streamlit nous permettra de cr√©er une interface web simple et interactive o√π l'utilisateur pourras poser ses questions et voir les r√©ponses, y compris les sources.\n",
    "7. **Gestion des sources/Gestion des Hallucinations (Source Retrieval) :**\n",
    "Nous nous assurerons que chaque r√©ponse est accompagn√©e de l'indication de la page ou du morceau de texte d'o√π provient l'information.\n",
    "\n",
    "## Pourquoi ces outils ?\n",
    "\n",
    "1. **LangChain :** C'est un framework puissant qui simplifie √©norm√©ment la construction d'applications bas√©es sur les grands mod√®les de langage (LLMs). Il offre des composants modulaires pour le chargement de documents, le d√©coupage, les bases de donn√©es vectorielles, et l'int√©gration de LLMs.\n",
    "2. **Chroma :** Une base de donn√©es vectorielle l√©g√®re et facile √† utiliser, parfaite pour les projets de taille moyenne et l'apprentissage. Elle est souvent utilis√©e avec LangChain.\n",
    "3. **Gemini :** Un mod√®le de langage performant de Google, capable de comprendre le langage naturel et de g√©n√©rer des r√©ponses pertinentes.\n",
    "4. **Streamlit :** C'est un excellent outil pour construire rapidement des interfaces utilisateur pour des applications de machine learning et de data science, sans avoir besoin de connaissances approfondies en d√©veloppement web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5860NhgZjHh-"
   },
   "source": [
    "## PARTIE INDEXATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfp0qpoTv1BS"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class TitleBasedSplitter(TextSplitter):\n",
    "    \"\"\"\n",
    "    Splitter de texte bas√© sur la d√©tection de titres.\n",
    "    Garde les m√©tadonn√©es de la page d'origine pour chaque chunk.\n",
    "    \"\"\"\n",
    "    def __init__(self, pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|√®me)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:)\"):\n",
    "        \"\"\"\n",
    "        Initialise le splitter avec un pattern regex pour les titres.\n",
    "        Le pattern par d√©faut cherche \"Titre \" suivi de:\n",
    "        - Soit des chiffres romains (I, V, X...) optionnellement suivis de \"er\" ou \"√®me\".\n",
    "        - Soit des mots (lettres, chiffres, accents, apostrophes, tirets).\n",
    "        - Le tout suivi d'un deux-points (avec espaces optionnels autour).\n",
    "        Exemples: \"Titre Ier :\", \"Titre pr√©liminaire :\", \"Titre II :\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    def split_documents(self, documents: List[Document], **kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Divise une liste de documents (pages) en chunks bas√©s sur les titres.\n",
    "        Chaque chunk h√©rite des m√©tadonn√©es de son document d'origine.\n",
    "        \"\"\"\n",
    "        all_chunks: List[Document] = []\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            metadata = doc.metadata.copy() # Copie des m√©tadonn√©es de la page d'origine\n",
    "\n",
    "            matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "            if not matches:\n",
    "                # Si aucun titre n'est trouv√© sur la page, la page enti√®re est un chunk\n",
    "                if text.strip(): # S'assurer que le texte n'est pas vide\n",
    "                    all_chunks.append(Document(page_content=text.strip(), metadata=metadata))\n",
    "                continue # Passer √† la page suivante\n",
    "\n",
    "            # Cas o√π il y a du texte avant le premier titre sur la page\n",
    "            if matches[0].start() > 0:\n",
    "                pre_title_text = text[0:matches[0].start()].strip()\n",
    "                if pre_title_text:\n",
    "                    all_chunks.append(Document(page_content=pre_title_text, metadata=metadata))\n",
    "\n",
    "            for i in range(len(matches)):\n",
    "                start = matches[i].start()\n",
    "                # La fin du chunk est le d√©but du prochain match, ou la fin du texte si c'est le dernier match\n",
    "                end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "                chunk_content = text[start:end].strip()\n",
    "\n",
    "                if chunk_content:\n",
    "                    # Ajout de m√©tadonn√©es sp√©cifiques au chunk si besoin,\n",
    "                    # par exemple le titre identifi√© ou une ID de chunk\n",
    "                    chunk_metadata = metadata.copy()\n",
    "                    # On peut ajouter ici des infos comme le titre sp√©cifique du chunk\n",
    "                    # chunk_metadata[\"title\"] = matches[i].group(0).strip()\n",
    "                    all_chunks.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "    # Nous n'avons pas besoin de split_text si nous utilisons split_documents directement sur des objets Document.\n",
    "    # Cependant, si l'on veux conserver la compatibilit√© avec d'autres TextSplitter,\n",
    "    # l'on peut impl√©menter split_text pour une cha√Æne unique.\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Divise une cha√Æne de texte en chunks bas√©s sur les titres.\n",
    "        Cette m√©thode est appel√©e par split_documents si un Document est pass√©.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "        if not matches:\n",
    "            return [text.strip()] if text.strip() else []\n",
    "\n",
    "        # Ajouter le texte avant le premier titre comme un chunk si non vide\n",
    "        if matches[0].start() > 0:\n",
    "            pre_title_text = text[0:matches[0].start()].strip()\n",
    "            if pre_title_text:\n",
    "                chunks.append(pre_title_text)\n",
    "\n",
    "        for i in range(len(matches)):\n",
    "            start = matches[i].start()\n",
    "            end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "            chunk_content = text[start:end].strip()\n",
    "            if chunk_content:\n",
    "                chunks.append(chunk_content)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25310,
     "status": "ok",
     "timestamp": 1749897512639,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "IuxnQp4vwqxB",
    "outputId": "f66d7af6-652e-4792-8624-975e5706a003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
      "Nombre de pages charg√©es par PyPDFLoader : 278\n",
      "Nombre de documents (chunks) apr√®s TitleBasedSplitter : 312\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Page content (d√©but): Partie l√©gislative - Chapitre pr√©liminaire : Dialogue social. \n",
      "Partie l√©gislative\n",
      "Chapitre pr√©liminaire : Dialogue social.\n",
      "L. 1  LOI n¬∞2008-67 du 21 janvier 2008 - art. 3      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Tout projet de r√©forme envisag√© par le Gouvernement qui porte sur les relations individuelles et collectives\n",
      "du travail, l'emploi et la formation professionnelle et qui rel√®ve du champ de la n√©gociation nationale et\n",
      "interprofessionnelle fait l'objet d'une conc...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1'}\n",
      "Source Page: 1\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "Premi√®re partie : Les relations individuelles de travail\n",
      "Livre Ier : Dispositions pr√©liminaires...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
      "Source Page: 2\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Page content (d√©but): Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "Chapitre unique.\n",
      "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du pr√©sent livre sont applicables aux employeurs de droit priv√© ainsi qu'√† leurs salari√©s.\n",
      "Elles sont √©galement applicables au personnel des personnes publiques employ√© dans les conditions du droit\n",
      "priv√©, sous r√©serve des dispositions particuli√®res ayant le m√™me objet r√©s...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
      "Source Page: 2\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "L. 1111-3  ORDONNANCE n¬∞2015-1578 du 3 d√©cembre 2015 - art. 1      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
      "1¬∞ Les apprentis ;\n",
      "2¬∞ Les titulaires d'un contrat initiative-emploi, pendant la dur√©e d'attribution de l'aide financi√®re mentionn√©e\n",
      "√† l'article L. 5134-72 ;\n",
      "3¬∞ (Abrog√©) ;\n",
      "4¬∞ Le...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
      "Source Page: 3\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Page content (d√©but): Titre II : Droits et libert√©s dans l'entreprise\n",
      "Chapitre unique.\n",
      "L. 1121-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Nul ne peut apporter aux droits des personnes et aux libert√©s individuelles et collectives de restrictions qui ne\n",
      "seraient pas justifi√©es par la nature de la t√¢che √† accomplir ni proportionn√©es au but recherch√©.\n",
      "R√©cemment au Bulletin de la Cour de Cassation\n",
      "> Chambre sociale, 25 Septembre 2024, n¬∞23-11.86...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
      "Source Page: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin.\")\n",
    "else:\n",
    "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "\n",
    "    # 1. On charge les documents page par page\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs_from_loader = loader.load() # Chaque √©l√©ment est un Document (page) avec ses m√©tadonn√©es\n",
    "\n",
    "    if not docs_from_loader:\n",
    "        print(\"Aucun document n'a pu √™tre charg√© par PyPDFLoader. Veuillez v√©rifier le PDF.\")\n",
    "        documents = [] # L'on s'assure que 'documents' est d√©fini m√™me si le chargement √©choue\n",
    "    else:\n",
    "        print(f\"Nombre de pages charg√©es par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "\n",
    "        # 2. Initialiser notre splitter bas√© sur les titres\n",
    "        splitter = TitleBasedSplitter()\n",
    "\n",
    "        # 3. Appliquer le splitter sur chaque document (page)\n",
    "        # La m√©thode split_documents prend une liste de Document et retourne une liste de Document.\n",
    "        documents = splitter.split_documents(docs_from_loader)\n",
    "\n",
    "        print(f\"Nombre de documents (chunks) apr√®s TitleBasedSplitter : {len(documents)}\")\n",
    "\n",
    "        # Affichage de quelques exemples de chunks pour v√©rifier leur contenu et leurs m√©tadonn√©es\n",
    "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
    "            print(f\"\\n--- Chunk {i+1} ---\")\n",
    "            print(f\"Page content (d√©but): {doc_chunk.page_content[:500]}...\") # Affiche les 500 premiers caract√®res\n",
    "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
    "\n",
    "            # V√©rifier si 'page' est dans les m√©tadonn√©es\n",
    "            if 'page' in doc_chunk.metadata:\n",
    "                print(f\"Source Page: {doc_chunk.metadata['page'] + 1}\") # +1 car les pages sont souvent 0-index√©es\n",
    "\n",
    "# NB : Il est important de s'assurer qu'il n'y a pas d'erreur dans 'documents' si aucun document n'est charg√©.\n",
    "if not documents:\n",
    "    print(\"Le traitement ne peut pas continuer car aucun document n'a √©t√© cr√©√©.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBsuWQ22v-AY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class CodeDuTravailStructureExtractor(TextSplitter):\n",
    "    \"\"\"\n",
    "    Splitter et extracteur de structure pour le Code du Travail.\n",
    "    D√©coupe le document en chunks et enrichit les m√©tadonn√©es\n",
    "    avec les informations de Partie, Livre, Titre, Chapitre, Section et Article.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Pattern pour les titres principaux (Titre Ier :, Titre Pr√©liminaire :)\n",
    "        title_pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|√®me)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les chapitres (Chapitre Ier :, Chapitre Unique :)\n",
    "        chapter_pattern: str = r\"(Chapitre\\s+(?:[IVXLCDM]+(?:er|√®me)?|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les sections (Section 1 :, Section unique :)\n",
    "        section_pattern: str = r\"(Section\\s+(?:\\d+|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les articles (L. 123-1, R. 456-7, D. 789-10)\n",
    "        article_pattern: str = r\"(Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)[\\s\\S]*?(?=Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)|Titre\\s+|Chapitre\\s+|Section\\s+|$))\",\n",
    "        # Capture le num√©ro de l'article dans un groupe pour extraction facile\n",
    "        article_num_capture_group: int = 2, # Le groupe qui contient \"L. 123-1\"\n",
    "        keep_separator: bool = True, # Indique si le s√©parateur (titre/chapitre/article) doit faire partie du chunk\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "        self.title_pattern = re.compile(title_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.chapter_pattern = re.compile(chapter_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.section_pattern = re.compile(section_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.article_pattern = re.compile(article_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.article_num_capture_group = article_num_capture_group\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Divise les documents (pages) en chunks et enrichit les m√©tadonn√©es.\n",
    "        \"\"\"\n",
    "        all_chunks: List[Document] = []\n",
    "        current_book = None\n",
    "        current_part = None\n",
    "        current_title = None\n",
    "        current_chapter = None\n",
    "        current_section = None\n",
    "\n",
    "        # Pour le Code du Travail, la \"Partie l√©gislative\" et \"Partie r√©glementaire\"\n",
    "        # sont souvent des sections de haut niveau. On pourrait les d√©tecter aussi.\n",
    "        # Par exemple: r\"(Partie\\s+(?:l√©gislative|r√©glementaire)\\s*:\\s*.+?)(?=\\n(?:Partie\\s|Livre\\s|Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\"\n",
    "\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            page_metadata = doc.metadata.copy() # Copier les m√©tadonn√©es de la page (ex: page number)\n",
    "\n",
    "            # Il est crucial de d√©tecter les √©l√©ments hi√©rarchiques dans l'ordre d√©croissant\n",
    "            # (Partie -> Livre -> Titre -> Chapitre -> Section -> Article)\n",
    "            # pour s'assurer que les m√©tadonn√©es sont correctement propag√©es.\n",
    "\n",
    "            # Nous allons traiter le texte de la page pour identifier les blocs.\n",
    "            # L'id√©e est de trouver tous les matchs de titres, chapitres, sections, articles\n",
    "            # et de les traiter s√©quentiellement.\n",
    "\n",
    "            # Simple approche pour commencer: On va splitter principalement par article\n",
    "            # et ensuite extraire les informations de titre/chapitre/section\n",
    "            # qui pr√©c√®dent cet article ou sont d√©tect√©es dans le chunk de l'article.\n",
    "\n",
    "            # Cette m√©thode n√©cessite une logique plus complexe pour propager le contexte\n",
    "            # (quel titre/chapitre actuel s'applique √† un article).\n",
    "            # On va utiliser une approche par \"contexte courant\" qui est mise √† jour √† chaque d√©tection.\n",
    "\n",
    "            # Initialisation du contexte pour cette page (on garde le dernier contexte connu)\n",
    "            # Pour la premi√®re page, ces valeurs seront None.\n",
    "            # Pour les pages suivantes, elles h√©riteront des valeurs de la fin de la page pr√©c√©dente.\n",
    "\n",
    "\n",
    "            # Pour l'instant, on suppose que les titres sont r√©p√©t√©s ou que les chunks sont suffisamment petits\n",
    "            # pour qu'un article soit dans son contexte de titre/chapitre.\n",
    "            # C'est un point cl√© √† affiner si les sources ne sont pas pr√©cises.\n",
    "\n",
    "            # D√©coupage par Article principalement\n",
    "            article_matches = list(self.article_pattern.finditer(text))\n",
    "\n",
    "            last_idx = 0\n",
    "            # Si du texte pr√©c√®de le premier article\n",
    "            if article_matches and article_matches[0].start() > 0:\n",
    "                pre_article_text = text[0:article_matches[0].start()].strip()\n",
    "                if pre_article_text:\n",
    "                    # Tentative d'extraction des infos de titre/chapitre/section pour ce bloc introductif\n",
    "                    # C'est une simplification, id√©alement on d√©tecterait ces √©l√©ments en amont du split.\n",
    "                    chunk_metadata = self._extract_hierarchy_metadata(pre_article_text, page_metadata)\n",
    "                    all_chunks.append(Document(page_content=pre_article_text, metadata=chunk_metadata))\n",
    "                last_idx = article_matches[0].start()\n",
    "\n",
    "            for i, match in enumerate(article_matches):\n",
    "                article_content = match.group(0).strip() # Le contenu complet de l'article (incluant le num√©ro)\n",
    "                article_number = match.group(self.article_num_capture_group) # Le num√©ro de l'article\n",
    "\n",
    "                # Ici, nous pourrions affiner l'extraction de la date ou de la loi sp√©cifique\n",
    "                # si un pattern est d√©tectable dans l'article_content\n",
    "                # Par exemple: LOI n¬∞ 2014-288 du 5 mars 2014-art. 29 (V)\n",
    "                # pattern_date_loi = r\"(LOI n¬∞\\s*\\d{4}-\\d+\\s*du\\s+\\d{1,2}\\s+\\w+\\s+\\d{4})\"\n",
    "                # date_loi_match = re.search(pattern_date_loi, article_content)\n",
    "                # if date_loi_match:\n",
    "                #     article_metadata[\"date_loi\"] = date_loi_match.group(1)\n",
    "\n",
    "                chunk_metadata = page_metadata.copy()\n",
    "                chunk_metadata[\"type\"] = \"Article\"\n",
    "                chunk_metadata[\"article_number\"] = article_number\n",
    "\n",
    "                # Tenter d'extraire le contexte hi√©rarchique pour cet article\n",
    "                # On recherche les titres, chapitres, sections qui pr√©c√®dent imm√©diatement cet article\n",
    "                # dans la portion de texte trait√©e pour cette page.\n",
    "\n",
    "                # C'est une simplification pour l'exemple.\n",
    "                # Une vraie solution robuste impliquerait de parser le document s√©quentiellement\n",
    "                # et de maintenir un √©tat des \"titres actifs\" et \"chapitres actifs\"\n",
    "                # au fur et √† mesure que l'on lit le document.\n",
    "\n",
    "                # Pour l'instant, on fait une extraction locale:\n",
    "                chunk_metadata.update(self._extract_hierarchy_metadata(article_content, page_metadata))\n",
    "\n",
    "                all_chunks.append(Document(page_content=article_content, metadata=chunk_metadata))\n",
    "                last_idx = match.end()\n",
    "\n",
    "            # Si du texte reste apr√®s le dernier article sur la page\n",
    "            if last_idx < len(text):\n",
    "                remaining_text = text[last_idx:].strip()\n",
    "                if remaining_text:\n",
    "                    chunk_metadata = self._extract_hierarchy_metadata(remaining_text, page_metadata)\n",
    "                    all_chunks.append(Document(page_content=remaining_text, metadata=chunk_metadata))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "    def _extract_hierarchy_metadata(self, text: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tente d'extraire les informations de hi√©rarchie (Titre, Chapitre, Section)\n",
    "        √† partir d'un morceau de texte et les ajoute aux m√©tadonn√©es.\n",
    "        \"\"\"\n",
    "        metadata = base_metadata.copy()\n",
    "\n",
    "        # Chercher le titre le plus proche\n",
    "        title_match = self.title_pattern.search(text)\n",
    "        if title_match:\n",
    "            metadata[\"title\"] = title_match.group(1).strip()\n",
    "\n",
    "        # Chercher le chapitre le plus proche\n",
    "        chapter_match = self.chapter_pattern.search(text)\n",
    "        if chapter_match:\n",
    "            metadata[\"chapter\"] = chapter_match.group(1).strip()\n",
    "\n",
    "        # Chercher la section la plus proche\n",
    "        section_match = self.section_pattern.search(text)\n",
    "        if section_match:\n",
    "            metadata[\"section\"] = section_match.group(1).strip()\n",
    "\n",
    "\n",
    "        # Pour les livres et parties, c'est plus complexe car ils sont souvent en d√©but de document\n",
    "        # ou indiqu√©s par des en-t√™tes/pieds de page non extraits par `page_content`.\n",
    "        # Pour le \"Livre\" et la \"Partie\", il est souvent plus efficace de les extraire\n",
    "        # soit manuellement au d√©but, soit en faisant une passe initiale sur le document complet\n",
    "        # pour √©tablir la hi√©rarchie.\n",
    "        # Pour l'instant, je ne les inclus pas dans _extract_hierarchy_metadata,\n",
    "        # car un chunk d'article ne les contiendra pas n√©cessairement.\n",
    "        # On pourrait les ajouter comme des m√©tadonn√©es globales au document si elles sont fixes,\n",
    "        # ou les propager depuis un parseur de document entier.\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    # La m√©thode split_text n'est pas utilis√©e directement par split_documents,\n",
    "    # mais on la garde pour la conformit√© avec la classe m√®re, m√™me si elle n'est pas optimis√©e\n",
    "    # pour notre usage actuel d'extraction de m√©tadonn√©es.\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        # Cette impl√©mentation est plus simple, mais ne g√®re pas la propagation des m√©tadonn√©es\n",
    "        # C'est pourquoi nous utiliserons principalement split_documents.\n",
    "        chunks = []\n",
    "        # On va splitter par les articles pour avoir des chunks plus petits\n",
    "        article_splits = self.article_pattern.split(text)\n",
    "\n",
    "        # Si le premier √©l√©ment n'est pas un s√©parateur, c'est un pr√©ambule\n",
    "        if len(article_splits) > 0 and not self.article_pattern.match(article_splits[0]):\n",
    "            if article_splits[0].strip():\n",
    "                chunks.append(article_splits[0].strip())\n",
    "            start_index = 1\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "        # R√©assembler les articles avec leur num√©ro\n",
    "        for i in range(start_index, len(article_splits), self.article_num_capture_group + 1):\n",
    "            if i + self.article_num_capture_group < len(article_splits):\n",
    "                article_num = article_splits[i + self.article_num_capture_group -1] # Le num√©ro de l'article captur√©\n",
    "                article_content = article_splits[i + self.article_num_capture_group] # Le contenu apr√®s le num√©ro\n",
    "\n",
    "                full_article_chunk = f\"Article {article_num.strip()} {article_content.strip()}\"\n",
    "                if full_article_chunk.strip():\n",
    "                    chunks.append(full_article_chunk.strip())\n",
    "            elif article_splits[i].strip():\n",
    "                chunks.append(article_splits[i].strip())\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13082,
     "status": "ok",
     "timestamp": 1749897525777,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "SGKhQZx6wLzW",
    "outputId": "7be3fe04-f7f3-49d3-e327-325869e29a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
      "Nombre de pages charg√©es par PyPDFLoader : 278\n",
      "Nombre de documents (chunks enrichis) apr√®s extraction de structure : 411\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Page content (d√©but): Partie l√©gislative - Chapitre pr√©liminaire : Dialogue social. \n",
      "Partie l√©gislative\n",
      "Chapitre pr√©liminaire : Dialogue social.\n",
      "L. 1  LOI n¬∞2008-67 du 21 janvier 2008 - art. 3      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Tout projet de r√©forme envisag√© par le Gouvernement qui porte sur les relations individuelles et collectives\n",
      "du travail, l'emploi et la formation professionnelle et qui rel√®ve du champ de la n√©gociation nationale et\n",
      "interprofessionnelle fait l'objet d'une conc...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1', 'chapter': 'Chapitre pr√©liminaire : Dialogue social. \\nPartie l√©gislative'}\n",
      "  Source Page: 1\n",
      "  Chapitre: Chapitre pr√©liminaire : Dialogue social. \n",
      "Partie l√©gislative\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "Premi√®re partie : Les relations individuelles de travail\n",
      "Livre Ier : Dispositions pr√©liminaires\n",
      "Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "Chapitre unique.\n",
      "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du pr√©sent livre sont applicables aux employeurs de droit...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2', 'title': \"Titre Ier : Champ d'application et calcul des seuils d'effectifs\"}\n",
      "  Source Page: 2\n",
      "  Titre: Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "L. 1111-3  ORDONNANCE n¬∞2015-1578 du 3 d√©cembre 2015 - art. 1      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
      "1¬∞ Les apprentis ;\n",
      "2¬∞ Les titulaires d'un contrat initiative-emploi, pendant la dur√©e d'attribution de l'aide financi√®re mentionn√©e\n",
      "√† l'article L. 5134-72 ;\n",
      "3¬∞ (Abrog√©) ;\n",
      "4¬∞ Le...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3', 'title': \"Titre II : Droits et libert√©s dans l'entreprise\"}\n",
      "  Source Page: 3\n",
      "  Titre: Titre II : Droits et libert√©s dans l'entreprise\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "de qualification, de classification, de promotion professionnelle, d'horaires de travail, d'√©valuation de la\n",
      "performance, de mutation ou de renouvellement de contrat, ni de toute autre mesure mentionn√©e au II de\n",
      "l'article 10-1 de la loi n¬∞ 2016-1691 du 9 d√©cembre 2016 relative √† la transparence, √† la lutte contre la corruption\n",
      "et √† la modernisation de la vie √©conomique, pour avo...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 3, 'page_label': '4', 'title': 'Titre III : Discriminations', 'chapter': \"Chapitre Ier : Champ d'application.\\nL. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nLes dispositions du pr√©sent titre sont applicables aux employeurs de droit priv√© ainsi qu'√† leurs salari√©s.\\nElles sont √©galement applicables au personnel des personnes publiques employ√© dans les conditions du droit\\npriv√©.\\nL. 1131-2  LOI n¬∞2017-86 du 27 janvier 2017 - art. 214      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nDans toute entreprise employant au moins trois cents salari√©s et dans toute entreprise sp√©cialis√©e dans le\\nrecrutement, les employ√©s charg√©s des missions de recrutement re√ßoivent une formation √† la non-discrimination\\n√† l'embauche au moins une fois tous les cinq ans.\"}\n",
      "  Source Page: 4\n",
      "  Titre: Titre III : Discriminations\n",
      "  Chapitre: Chapitre Ier : Champ d'application.\n",
      "L. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du pr√©sent titre sont applicables aux employeurs de droit priv√© ainsi qu'√† leurs salari√©s.\n",
      "Elles sont √©galement applicables au personnel des personnes publiques employ√© dans les conditions du droit\n",
      "priv√©.\n",
      "L. 1131-2  LOI n¬∞2017-86 du 27 janvier 2017 - art. 214      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Dans toute entreprise employant au moins trois cents salari√©s et dans toute entreprise sp√©cialis√©e dans le\n",
      "recrutement, les employ√©s charg√©s des missions de recrutement re√ßoivent une formation √† la non-discrimination\n",
      "√† l'embauche au moins une fois tous les cinq ans.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Page content (d√©but): Partie l√©gislative - Premi√®re partie : Les relations individuelles de travail - Livre Ier : Dispositions pr√©liminaires\n",
      "R√©cemment au Bulletin de la Cour de Cassation\n",
      "> Chambre sociale, 24 Avril 2024, n¬∞22-20.539, (B)\n",
      "> Chambre sociale, 20 D√©cembre 2023, n¬∞22-12.381, (B)\n",
      "> Chambre sociale, 20 Septembre 2023, n¬∞22-12.293, (B)\n",
      "> Chambre sociale, 28 Juin 2023, n¬∞22-11.699, (B)\n",
      "> Chambre sociale, 01 Juin 2023, n¬∞21-21.191, (B)\n",
      "service-public.fr\n",
      "> Droit de gr√®ve d'un salari√© du secteur priv√© : Interdic...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 4, 'page_label': '5'}\n",
      "  Source Page: 5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin.\")\n",
    "    documents = [] # On s'assure que 'documents' est d√©fini m√™me si le fichier est manquant\n",
    "else:\n",
    "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    # Chargement page par page pour conserver les m√©tadonn√©es de page\n",
    "    docs_from_loader = loader.load()\n",
    "\n",
    "    if not docs_from_loader:\n",
    "        print(\"Aucun document n'a pu √™tre charg√© par PyPDFLoader. Veuillez v√©rifier le PDF.\")\n",
    "        documents = []\n",
    "    else:\n",
    "        print(f\"Nombre de pages charg√©es par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "\n",
    "        # Initialisation de l'extracteur de structure\n",
    "        # On va cr√©er des patterns plus robustes si le test initial ne donne pas satisfaction\n",
    "        # Les patterns par d√©faut dans la classe sont un bon point de d√©part.\n",
    "        structure_extractor = CodeDuTravailStructureExtractor()\n",
    "\n",
    "        # Appliquer l'extracteur sur les documents (pages)\n",
    "        documents = structure_extractor.split_documents(docs_from_loader)\n",
    "\n",
    "        print(f\"Nombre de documents (chunks enrichis) apr√®s extraction de structure : {len(documents)}\")\n",
    "\n",
    "        # Afficher quelques exemples de chunks enrichis\n",
    "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
    "            print(f\"\\n--- Chunk {i+1} ---\")\n",
    "            print(f\"Page content (d√©but): {doc_chunk.page_content[:500]}...\")\n",
    "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
    "            # Exemple de v√©rification des m√©tadonn√©es sp√©cifiques\n",
    "            if 'page' in doc_chunk.metadata:\n",
    "                print(f\"  Source Page: {doc_chunk.metadata['page'] + 1}\")\n",
    "            if 'type' in doc_chunk.metadata:\n",
    "                print(f\"  Type de chunk: {doc_chunk.metadata['type']}\")\n",
    "            if 'article_number' in doc_chunk.metadata:\n",
    "                print(f\"  Num√©ro d'Article: {doc_chunk.metadata['article_number']}\")\n",
    "            if 'title' in doc_chunk.metadata:\n",
    "                print(f\"  Titre: {doc_chunk.metadata['title']}\")\n",
    "            if 'chapter' in doc_chunk.metadata:\n",
    "                print(f\"  Chapitre: {doc_chunk.metadata['chapter']}\")\n",
    "\n",
    "# NB : S'assurer que 'documents' est d√©fini pour les √©tapes suivantes m√™me en cas d'√©chec de chargement\n",
    "if not documents:\n",
    "    print(\"Le traitement ne peut pas continuer car aucun document structur√© n'a √©t√© cr√©√©.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mbxTrjnipO"
   },
   "source": [
    "## PARTIES EMBEDDING, STOCKAGE VECTORIELLE, CHAINE RAG, LLM\n",
    "\n",
    "### Pourquoi d√©couper le document (Chunking) ?\n",
    "\n",
    "- **Limitation des LLM :** Les grands mod√®les de langage ont une \"fen√™tre de contexte\" limit√©e. C'est la quantit√© de texte qu'ils peuvent lire et traiter en une seule fois. Un PDF de 2755 pages est bien au-del√† de ce qu'ils peuvent g√©rer.\n",
    "- **Pertinence de la recherche :** Si l'on cherche un concept dans 2775 pages, la recherche sera tr√®s lente et peu pr√©cise.\n",
    "*  _Solution :_\n",
    "  - **Le d√©coupage (Chunking)**\n",
    "     - Nous allons diviser le PDF en petits morceaux (des paragraphes, quelques phrases, etc.). Chaque morceau est suffisamment petit pour √™tre trait√© par un LLM et suffisamment grand pour conserver du sens.\n",
    "*  _Cons√©quence :_ Quand un utilisateur posera une question, les morceaux les plus pertinenets seront chercher et non l'int√©gralit√© du document.\n",
    "\n",
    "\n",
    "### Qu'est-ce qu'un Embedding et pourquoi en a-t-on besoin ?\n",
    "Imaginons que chaque mot, phrase ou morceau de texte peut √™tre repr√©sent√© par un point dans un espace multidimensionnel (un peu comme des coordonn√©es X, Y, Z, mais avec beaucoup plus de dimensions).\n",
    "\n",
    "- **Transformation en nombres :** Un \"embedding\" est une suite de nombres (un vecteur) qui repr√©sente la signification s√©mantique d'un texte. Des textes qui ont un sens similaire seront repr√©sent√©s par des vecteurs \"proches\" dans cet espace.\n",
    "- **Recherche de similarit√© :** La question de l'utilisateur sera transform√©e en embedding. Ensuite, une recherche des embeddings de morceaux de texte qui sont les plus \"proches\" (on pourra notamment faire usage de l'algorithme des K-plus proches voisins pour r√©cup√©rer les informations les plus pertinentes) de l'embedding da la question se fera dans la base de donn√©es. C'est la base de la recherche s√©mantique.\n",
    "\n",
    "* _Pourquoi ?_\n",
    "  - Un ordinateur ne \"comprend\" pas le texte. Il comprend les nombres. Les embeddings sont le pont entre le langage humain et la capacit√© de l'ordinateur √† trouver des similarit√©s de sens.\n",
    "\n",
    "### Le r√¥le de la base de donn√©es vectorielle (ChromaDB)\n",
    "Une fois que nous avons nos morceaux de texte et leurs embeddings, o√π les stockons-nous de mani√®re efficace pour pouvoir les rechercher rapidement ?\n",
    "\n",
    "- **Base de donn√©es traditionnelle vs. Base de donn√©es vectorielle :** Une base de donn√©es classique (comme SQL) est optimis√©e pour des recherches exactes (ex: \"trouve tous les utilisateurs dont le nom est 'Dupont'\"). Une base de donn√©es vectorielle est optimis√©e pour des recherches de similarit√© (ex: \"trouve tous les textes qui sont similaires √† 'licenciement abusif'\").\n",
    "\n",
    "**ChromaDB :** C'est une base de donn√©es vectorielle l√©g√®re et facile √† mettre en place. Elle va stocker :\n",
    "- Les morceaux de texte bruts (le contenu original).\n",
    "- Les embeddings correspondants (les vecteurs num√©riques).\n",
    "- Des m√©tadonn√©es (comme le num√©ro de page d'o√π vient le morceau, ce qui est crucial pour les sources !).\n",
    "_Fonctionnement :_ Quand l'utilisateur pose une question, Chroma va rapidement identifier les embeddings les plus similaires √† sa question, et lui retourner les morceaux de texte correspondants.\n",
    "\n",
    "\n",
    "### LangChain : L'orchestrateur de notre application\n",
    "LangChain est un framework puissant qui simplifie √©norm√©ment la construction d'applications bas√©es sur les grands mod√®les de langage.\n",
    "\n",
    "_Concept de \"Chains\" (Cha√Ænes) :_ LangChain permet de relier diff√©rents composants (comme le chargement de documents, les d√©coupeurs, les mod√®les d'embeddings, les bases de donn√©es vectorielles, et les LLM) dans une s√©quence logique, une \"cha√Æne\".\n",
    "\n",
    "- **Modularit√© :** Chaque partie de l'application RAG (chargement, d√©coupage, embedding, recherche, g√©n√©ration) est un \"maillon\" de la cha√Æne. LangChain permet de les assembler facilement.\n",
    "- **Simplification :** Au lieu de g√©rer manuellement chaque interaction entre ces composants, LangChain fournit des abstractions qui rendent le d√©veloppement plus rapide et plus propre.\n",
    "\n",
    "\n",
    "### Gemini : Le cerveau qui g√©n√®re la r√©ponse\n",
    "Gemini est le mod√®le de langage (LLM) de Google que nous allons utiliser.\n",
    "\n",
    "_Son r√¥le :_ Une fois que Chroma a r√©cup√©r√© les morceaux de texte les plus pertinents du document/PDF, Gemini va recevoir :\n",
    "- La question.\n",
    "- Les morceaux de texte pertinents.\n",
    "\n",
    "_Sa t√¢che :_ Il va analyser ces informations et g√©n√©rer une r√©ponse coh√©rente, synth√©tis√©e, et bas√©e sur le contexte fourni. C'est l√† que la \"G√©n√©ration\" de RAG entre en jeu.\n",
    "- **API :** Nous interagirons avec Gemini via son API (Interface de Programmation d'Application), ce qui signifie que nous enverrons des requ√™tes √† un service Google pour obtenir des r√©ponses.\n",
    "\n",
    "### Streamlit : L'interface utilisateur simple et rapide\n",
    "Pour interagir avec notre application, nous avons besoin d'une interface.\n",
    "\n",
    "####Qu'est-ce que c'est ?\n",
    "**Streamlit** est une biblioth√®que Python qui permet de cr√©er tr√®s facilement des applications web interactives pour la science des donn√©es et le machine learning.\n",
    "*  _Avantages :_ Pas besoin d'√™tre un expert en d√©veloppement web (HTML, CSS, JavaScript). Avec quelques lignes de Python, tu peux cr√©er des widgets (champs de texte, boutons) et afficher des r√©sultats.\n",
    "\n",
    "_Notre utilisation :_ Nous l'utiliserons pour cr√©er une bo√Æte de texte o√π l'utilisateur pourras taper ses questions, un bouton pour les soumettre, et un espace pour afficher la r√©ponse de Gemini et les sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbDKApJPHzQ_"
   },
   "outputs": [],
   "source": [
    "!pip install -q rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7549,
     "status": "ok",
     "timestamp": 1749898688059,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "QUHCkR0exto3",
    "outputId": "1139f342-01fa-46d6-ac78-6e8a38d300b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âtape 4 : Cr√©ation des Embeddings ---\n",
      "Initialisation du mod√®le d'embedding : all-MiniLM-L6-v2\n",
      "Mod√®le d'embedding initialis√© avec succ√®s.\n",
      "\n",
      "--- √âtape 5 : Cr√©ation ou chargement du Vectorstore Chroma ---\n",
      "Chargement du vectorstore Chroma existant depuis : ./chroma_db_codetravail\n",
      "Vectorstore Chroma charg√©.\n",
      "\n",
      "--- √âtape 5.5 : Configuration des Retrievers Hybrides ---\n",
      "Retrievers hybrides (BM25 et S√©mantique) configur√©s avec succ√®s.\n",
      "La recherche utilisera la fusion des rangs.\n",
      "\n",
      "--- √âtape 6 : Configuration du LLM (Gemini) ---\n",
      "Chargement du prompt RAG depuis Langchain Hub...\n",
      "Prompt charg√©.\n",
      "Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\n",
      "LLM Gemini-Pro initialis√© avec succ√®s.\n",
      "\n",
      "--- √âtape 7 : Construction de la Cha√Æne RAG ---\n",
      "\n",
      "--- √âtape 6.5 : Cha√Æne de Transformation de Requ√™te Multi-Requ√™tes ---\n",
      "Cha√Æne de g√©n√©ration de requ√™tes multiples initialis√©e.\n",
      "\n",
      "--- √âtape 7 : Construction de la Cha√Æne RAG ---\n",
      "Cha√Æne RAG construite avec succ√®s : Multi-Query Retrieval, Self-Querying, et fusion des rags.\n",
      "\n",
      "--- √âtape 8 : Tester la Cha√Æne RAG ---\n",
      "\n",
      "Question : Salut\n",
      "\n",
      "R√©ponse G√©n√©r√©e par Gemini :\n",
      "I am sorry, but I cannot answer in French. The provided documents do not contain information about the definition of \"Salut\". Therefore, I am unable to provide a relevant answer to your question.\n",
      "\n",
      "Sources (extraits et m√©tadonn√©es) :\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "# S'assurer que 'documents' n'est pas vide avant de continuer\n",
    "if not documents:\n",
    "    print(\"ATTENTION : La liste 'documents' est vide. Le processus RAG ne peut pas continuer.\")\n",
    "    exit(\"Processus arr√™t√© car aucun document n'a √©t√© trait√©.\")\n",
    "\n",
    "print(\"\\n--- √âtape 4 : Cr√©ation des Embeddings ---\")\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "print(f\"Initialisation du mod√®le d'embedding : {embedding_model_name}\")\n",
    "\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    print(\"Mod√®le d'embedding initialis√© avec succ√®s.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'initialisation du mod√®le d'embedding : {e}\")\n",
    "    print(\"Veuillez v√©rifier votre connexion internet et les d√©pendances 'sentence-transformers'.\")\n",
    "    exit(\"Processus arr√™t√© car le mod√®le d'embedding n'a pas pu √™tre initialis√©.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- √âtape 5 : Cr√©ation ou chargement du Vectorstore Chroma ---\")\n",
    "chroma_db_dir = \"./chroma_db_codetravail\"\n",
    "\n",
    "if os.path.exists(chroma_db_dir) and os.listdir(chroma_db_dir):\n",
    "    print(f\"Chargement du vectorstore Chroma existant depuis : {chroma_db_dir}\")\n",
    "    try:\n",
    "        vectorstore = Chroma(persist_directory=chroma_db_dir, embedding_function=embedding_model)\n",
    "        print(\"Vectorstore Chroma charg√©.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors du chargement du vectorstore Chroma : {e}\")\n",
    "        print(\"Tentative de recr√©ation du vectorstore...\")\n",
    "        vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "        print(\"Vectorstore Chroma recr√©√©.\")\n",
    "else:\n",
    "    print(f\"Cr√©ation d'un nouveau vectorstore Chroma dans : {chroma_db_dir}\")\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "    print(\"Vectorstore Chroma cr√©√©.\")\n",
    "\n",
    "\n",
    "# --- Configuration des Retrievers Hybrides ---\n",
    "print(\"\\n--- √âtape 5.5 : Configuration des Retrievers Hybrides ---\")\n",
    "\n",
    "# 1. Retriever s√©mantique (Chroma Retriever)\n",
    "semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. Retriever bas√© sur les mots-cl√©s (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5 # Nombre de documents √† r√©cup√©rer pour BM25\n",
    "\n",
    "# 3. Combinaison des retrievers avec EnsembleRetriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, semantic_retriever],\n",
    "    weights=[0.5, 0.5] # Poids √©gaux pour commencer\n",
    ")\n",
    "print(\"Retrievers hybrides (BM25 et S√©mantique) configur√©s avec succ√®s.\")\n",
    "print(\"La recherche utilisera la fusion des rangs.\")\n",
    "\n",
    "# LE RETRIEVER PRINCIPAL UTILIS√â DANS LA CHA√éNE EST MAINTENANT LE HYBRIDE\n",
    "retriever = hybrid_retriever\n",
    "\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "# print(f\"Retriever initialis√© pour r√©cup√©rer les {retriever.search_kwargs['k']} documents les plus pertinents.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- √âtape 6 : Configuration du LLM (Gemini) ---\")\n",
    "print(\"Chargement du prompt RAG depuis Langchain Hub...\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"Prompt charg√©.\")\n",
    "\n",
    "print(\"Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\")\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.2)\n",
    "    print(\"LLM Gemini-Pro initialis√© avec succ√®s.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'initialisation du LLM ChatGoogleGenerativeAI : {e}\")\n",
    "    print(\"Assurez-vous que GOOGLE_API_KEY est correctement d√©finie dans les secrets de Colab.\")\n",
    "    print(\"V√©rifiez aussi que l'API Generative Language est activ√©e pour votre projet Google Cloud.\")\n",
    "    exit(\"Processus arr√™t√© car le LLM n'a pas pu √™tre initialis√©.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- √âtape 7 : Construction de la Cha√Æne RAG ---\")\n",
    "\n",
    "# # --- NOUVEAU : Cha√Æne de R√©√©criture de Requ√™te ---\n",
    "# print(\"\\n--- √âtape 6.5 : Cha√Æne de R√©√©criture de Requ√™te ---\")\n",
    "# query_rewriter_template = \"\"\"You are an expert at rephrasing questions to be optimal for retrieving relevant documents.\n",
    "# Given the following user question, rephrase it to be a standalone search query.\n",
    "# If the question is already a good search query, return it as is.\n",
    "\n",
    "# User question: {question}\n",
    "# Rephrased query:\"\"\"\n",
    "\n",
    "# query_rewriter_prompt = ChatPromptTemplate.from_template(query_rewriter_template)\n",
    "# query_rewriting_chain = query_rewriter_prompt | llm | StrOutputParser()\n",
    "# print(\"Cha√Æne de r√©√©criture de la question initialis√©e.\")\n",
    "\n",
    "# --- : Cha√Æne de G√©n√©ration de Requ√™tes Multiples --- MULTI_QUERY, RAG FUSION\n",
    "print(\"\\n--- √âtape 6.5 : Cha√Æne de Transformation de Requ√™te Multi-Requ√™tes ---\")\n",
    "multi_query_template = \"\"\"You are an AI assistant that generates multiple search queries based on a single input question.\n",
    "Your goal is to create diverse queries that cover various aspects of the user's original question,\n",
    "to improve the chances of retrieving relevant documents.\n",
    "Generate 3 distinct search queries related to the user's question, separated by newlines.\n",
    "Do not number the queries.\n",
    "\n",
    "Original question: {question}\n",
    "Search queries:\"\"\"\n",
    "\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(multi_query_template)\n",
    "multi_query_chain = multi_query_prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "print(\"Cha√Æne de g√©n√©ration de requ√™tes multiples initialis√©e.\")\n",
    "\n",
    "# --- D√âFINITION FINALE DE LA CHA√éNE RAG AVEC MULTI-QUERY ET SELF-QUERYING ---\n",
    "print(\"\\n--- √âtape 7 : Construction de la Cha√Æne RAG ---\")\n",
    "\n",
    "# Fonction de formatage des documents\n",
    "def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "    formatted_content = \"\"\n",
    "    unique_pages = set()\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted_content += f\"Contenu source {i+1}:\\n{doc.page_content}\\n\\n\"\n",
    "        if 'page' in doc.metadata:\n",
    "            page_number = doc.metadata['page'] + 1\n",
    "            unique_pages.add(str(page_number))\n",
    "\n",
    "    if unique_pages:\n",
    "        formatted_content += f\"Sources des pages: {', '.join(sorted(list(unique_pages)))}\\n\"\n",
    "\n",
    "    return formatted_content\n",
    "\n",
    "# # --- NOUVELLE D√âFINITION DE LA CHA√éNE RAG AVEC R√â√âCRITURE ET RECHERCHE HYBRIDE ---\n",
    "# # Pr√©pare les inputs : la question originale et la question r√©√©crite pour le retriever\n",
    "# prepare_inputs = {\n",
    "#     \"question_originale\": RunnablePassthrough(),\n",
    "#     \"question_pour_retrieval\": query_rewriting_chain,\n",
    "# }\n",
    "\n",
    "# # La cha√Æne de r√©cup√©ration utilise la question r√©√©crite et passe le contexte + la question originale\n",
    "# retrieval_chain_with_rewriting = {\n",
    "#     \"context\": (lambda x: x[\"question_pour_retrieval\"]) | retriever, # Le retriever utilise la question r√©√©crite\n",
    "#     \"question\": (lambda x: x[\"question_originale\"]) # Le LLM utilise la question originale\n",
    "# }\n",
    "\n",
    "# # La cha√Æne de g√©n√©ration de r√©ponse\n",
    "# generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# # La cha√Æne RAG finale\n",
    "# # L'entr√©e de rag_chain_with_sources est la question originale.\n",
    "# # prepare_inputs prend cette question et la transforme en dictionnaire.\n",
    "# # retrieval_chain_with_rewriting prend ce dictionnaire et utilise les cl√©s \"question_pour_retrieval\" et \"question_originale\".\n",
    "# rag_chain_with_sources = (\n",
    "#     prepare_inputs\n",
    "#     | RunnableParallel(\n",
    "#         response=retrieval_chain_with_rewriting | generation_chain,\n",
    "#         source_documents=retrieval_chain_with_rewriting | RunnableLambda(lambda x: x[\"context\"])\n",
    "#     )\n",
    "# ).with_config(run_name=\"RAG Chain with Query Transformation and Sources\")\n",
    "\n",
    "# print(\"Cha√Æne RAG mise √† jour avec la transformation de requ√™te et la recherche hybride, pr√™te √† inclure les sources.\")\n",
    "\n",
    "# 1. Pr√©paration des inputs: la question originale et la liste des questions g√©n√©r√©es\n",
    "prepare_inputs = {\n",
    "    \"question_originale\": RunnablePassthrough(), # La question initiale de l'utilisateur\n",
    "    \"questions_pour_retrieval\": multi_query_chain, # La liste des requ√™tes g√©n√©r√©es\n",
    "}\n",
    "\n",
    "# 2. Cha√Æne de r√©cup√©ration : pour chaque requ√™te g√©n√©r√©e, appeler le retriever, puis aplatir et d√©dupliquer\n",
    "# Note: 'retriever' est maintenant l'EnsembleRetriever (BM25 + Self-Querying)\n",
    "# retrieval_and_aggregation_chain = (\n",
    "#     RunnableLambda(lambda x: x[\"questions_pour_retrieval\"]) # Prend la liste de questions\n",
    "#     | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries]) # Invoque l'EnsembleRetriever pour chaque requ√™te\n",
    "#     | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist]) # Aplatit la liste de listes de docs\n",
    "#     | RunnableLambda(lambda docs: list(set(docs))) # Supprime les doublons de Document\n",
    "# )\n",
    "\n",
    "# D√©duplication des documents\n",
    "retrieval_and_aggregation_chain = (\n",
    "    RunnableLambda(lambda x: x[\"questions_pour_retrieval\"])\n",
    "    | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries])\n",
    "    | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist])\n",
    "    # Logique pour la d√©duplication :\n",
    "    | RunnableLambda(\n",
    "        lambda docs: list(\n",
    "            {\n",
    "                (doc.page_content, frozenset(doc.metadata.items())): doc\n",
    "                for doc in docs\n",
    "            }.values()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Construction de la cha√Æne qui fournira le contexte et la question au LLM\n",
    "# Elle prend en entr√©e le dictionnaire de 'prepare_inputs'\n",
    "context_and_question_for_llm = {\n",
    "    \"context\": retrieval_and_aggregation_chain, # Le contexte est le r√©sultat agr√©g√© du retrieval\n",
    "    \"question\": RunnableLambda(lambda x: x[\"question_originale\"]) # La question pour le LLM (la question l'originale)\n",
    "}\n",
    "\n",
    "# 4. Cha√Æne de g√©n√©ration de r√©ponse (prompt | llm | output_parser)\n",
    "generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 5. La cha√Æne RAG finale\n",
    "rag_chain_with_sources = (\n",
    "    prepare_inputs # L'entr√©e est la question de l'utilisateur\n",
    "    | RunnableParallel( # Ex√©cute la g√©n√©ration de r√©ponse et la r√©cup√©ration des sources en parall√®le\n",
    "        response=context_and_question_for_llm | generation_chain, # La branche qui g√©n√®re la r√©ponse\n",
    "        source_documents=context_and_question_for_llm | RunnableLambda(lambda x: x[\"context\"]) # La branche qui passe les documents sources\n",
    "    )\n",
    ").with_config(run_name=\"RAG Chain with Multi-Query & Self-Query\")\n",
    "\n",
    "print(\"Cha√Æne RAG construite avec succ√®s : Multi-Query Retrieval, Self-Querying, et fusion des rags.\")\n",
    "\n",
    "print(\"\\n--- √âtape 8 : Tester la Cha√Æne RAG ---\")\n",
    "\n",
    "# question = \"Quelles sont les fonctions du code du travail? Et quelles sont les conditions de repr√©sentativit√© des organisations syndicales ?\"\n",
    "# Testons une question plus conversationnelle pour voir la r√©√©criture en action\n",
    "# question_conv = \"Quelle √©tait la r√©forme mentionn√©e au d√©but du document ? Et les conditions de repr√©sentativit√© ?\"\n",
    "\n",
    "question_conv = \"Salut\"\n",
    "question = question_conv\n",
    "\n",
    "print(f\"\\nQuestion : {question}\")\n",
    "\n",
    "try:\n",
    "    result = rag_chain_with_sources.invoke(question)\n",
    "\n",
    "    print(\"\\nR√©ponse G√©n√©r√©e par Gemini :\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "    # print(\"\\nSources (extraits et m√©tadonn√©es) :\")\n",
    "    # print(format_docs_with_sources(result[\"source_documents\"]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'invocation de la cha√Æne RAG : {e}\")\n",
    "    print(\"Veuillez v√©rifier les √©tapes pr√©c√©dentes (authentification LLM, initialisation de Chroma).\")\n",
    "    print(\"Erreur d√©taill√©e:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78_kD_ymheio"
   },
   "source": [
    "# Application streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG7cOKiRcORv"
   },
   "outputs": [],
   "source": [
    "# # Installation des d√©pendances n√©cessaires\n",
    "# !pip install -q langchain-google-genai pypdf langchain-chroma faiss-cpu sentence-transformers streamlit langchain_huggingface\n",
    "# !pip install langchain_community langchainhub chromadb langchain langchain_chroma rank_bm25\n",
    "# !pip install -q python-dotenv\n",
    "# !pip install -q transformers\n",
    "# !pip install -q --upgrade langchain\n",
    "\n",
    "# print(\"Toutes les d√©pendances ont √©t√© install√©es ou mises √† jour.\")\n",
    "\n",
    "# # Installation de localtunnel (pour exposer l'application)\n",
    "# !apt-get update\n",
    "# !apt-get install -y nodejs npm\n",
    "# !npm install -g localtunnel\n",
    "\n",
    "# print(\"Localtunnel install√©.\")\n",
    "\n",
    "# # Configuration des cl√©s API via Google Colab Secrets\n",
    "# from google.colab import userdata\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
    "#     os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
    "#     os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "#     os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "#     print(\"Cl√©s API charg√©es depuis les secrets Colab.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ATTENTION : Erreur lors du chargement des cl√©s API depuis les secrets Colab : {e}\")\n",
    "#     print(\"Assurez-vous que 'GOOGLE_API_KEY' et 'LANGCHAIN_API_KEY' sont bien d√©finies dans les secrets et activ√©es pour ce notebook.\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"Google Drive mont√©.\")\n",
    "\n",
    "# # V√©rification du chemin du PDF\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# if not os.path.exists(pdf_path):\n",
    "#     print(f\"ERREUR CRITIQUE : Le fichier PDF '{pdf_path}' n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin.\")\n",
    "# else:\n",
    "#     print(f\"Chemin du PDF v√©rifi√© : {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmJOTMw9OsI4"
   },
   "outputs": [],
   "source": [
    "# # %%writefile app.py\n",
    "\n",
    "# import streamlit as st\n",
    "# import os\n",
    "# import re\n",
    "# from typing import List, Optional, Dict, Any\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "# from langchain.schema import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain import hub\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.retrievers import BM25Retriever\n",
    "# from langchain.retrievers import EnsembleRetriever\n",
    "# import sys\n",
    "\n",
    "# # --- Configuration Streamlit (DOIT √™tre la premi√®re commande Streamlit) ---\n",
    "# st.set_page_config(page_title=\"Assistant Juridique RAG (Code du Travail)\", layout=\"wide\")\n",
    "\n",
    "# # --- Chemins et configuration ---\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# # pdf_path = \"data/data_50_page.pdf\"\n",
    "# chroma_db_dir = \"./chroma_db_codetravail\"\n",
    "# embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# # --- Interface Streamlit ---\n",
    "# st.title(\"‚öñÔ∏è Assistant Juridique\")\n",
    "# st.markdown(\"\"\"\n",
    "# Bienvenue dans votre outil d'aide √† la d√©cision juridique. Posez des questions sur le Code du Travail\n",
    "# et obtenez des r√©ponses pr√©cises bas√©es sur les documents officiels.\n",
    "# \"\"\")\n",
    "\n",
    "# st.write(\"Cha√Æne RAG construite avec succ√®s : Multi-Query Retrieval, Self-Querying, et fusion des rags.\")\n",
    "\n",
    "# if \"params\" not in st.session_state:\n",
    "#     st.session_state.params = {\n",
    "#         \"temperature\": 0.2,\n",
    "#         \"top_p\": 10,\n",
    "#         \"model\": \"gemini-2.5-flash-preview-05-20\"\n",
    "#     }\n",
    "\n",
    "# # --- Classe TitleBasedSplitter ---\n",
    "# class TitleBasedSplitter(TextSplitter):\n",
    "#     def __init__(self, pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|√®me)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:)\"):\n",
    "#         super().__init__()\n",
    "#         self.compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "#     def split_documents(self, documents: List[Document], **kwargs) -> List[Document]:\n",
    "#         all_chunks: List[Document] = []\n",
    "#         for doc in documents:\n",
    "#             text = doc.page_content\n",
    "#             metadata = doc.metadata.copy()\n",
    "\n",
    "#             matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "#             if not matches:\n",
    "#                 if text.strip():\n",
    "#                     all_chunks.append(Document(page_content=text.strip(), metadata=metadata))\n",
    "#                 continue\n",
    "\n",
    "#             if matches[0].start() > 0:\n",
    "#                 pre_title_text = text[0:matches[0].start()].strip()\n",
    "#                 if pre_title_text:\n",
    "#                     all_chunks.append(Document(page_content=pre_title_text, metadata=metadata))\n",
    "\n",
    "#             for i in range(len(matches)):\n",
    "#                 start = matches[i].start()\n",
    "#                 end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "#                 chunk_content = text[start:end].strip()\n",
    "\n",
    "#                 if chunk_content:\n",
    "#                     chunk_metadata = metadata.copy()\n",
    "#                     all_chunks.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
    "#         return all_chunks\n",
    "\n",
    "#     def split_text(self, text: str) -> List[str]:\n",
    "#         chunks = []\n",
    "#         matches = list(self.compiled_pattern.finditer(text))\n",
    "#         if not matches:\n",
    "#             return [text.strip()] if text.strip() else []\n",
    "\n",
    "#         if matches[0].start() > 0:\n",
    "#             pre_title_text = text[0:matches[0].start()].strip()\n",
    "#             if pre_title_text:\n",
    "#                 chunks.append(pre_title_text)\n",
    "\n",
    "#         for i in range(len(matches)):\n",
    "#             start = matches[i].start()\n",
    "#             end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "#             chunk_content = text[start:end].strip()\n",
    "#             if chunk_content:\n",
    "#                 chunks.append(chunk_content)\n",
    "#         return chunks\n",
    "\n",
    "# # --- Classe CodeDuTravailStructureExtractor ---\n",
    "# class CodeDuTravailStructureExtractor(TextSplitter):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         title_pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|√®me)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         chapter_pattern: str = r\"(Chapitre\\s+(?:[IVXLCDM]+(?:er|√®me)?|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         section_pattern: str = r\"(Section\\s+(?:\\d+|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         article_pattern: str = r\"(Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)[\\s\\S]*?(?=Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)|Titre\\s+|Chapitre\\s+|Section\\s+|$))\",\n",
    "#         article_num_capture_group: int = 2,\n",
    "#         keep_separator: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "#         self.title_pattern = re.compile(title_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.chapter_pattern = re.compile(chapter_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.section_pattern = re.compile(section_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.article_pattern = re.compile(article_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.article_num_capture_group = article_num_capture_group\n",
    "\n",
    "#     def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "#         all_chunks: List[Document] = []\n",
    "#         current_book = None\n",
    "#         current_part = None\n",
    "#         current_title = None\n",
    "#         current_chapter = None\n",
    "#         current_section = None\n",
    "\n",
    "#         for doc in documents:\n",
    "#             text = doc.page_content\n",
    "#             page_metadata = doc.metadata.copy()\n",
    "\n",
    "#             article_matches = list(self.article_pattern.finditer(text))\n",
    "\n",
    "#             last_idx = 0\n",
    "#             if article_matches and article_matches[0].start() > 0:\n",
    "#                 pre_article_text = text[0:article_matches[0].start()].strip()\n",
    "#                 if pre_article_text:\n",
    "#                     chunk_metadata = self._extract_hierarchy_metadata(pre_article_text, page_metadata)\n",
    "#                     all_chunks.append(Document(page_content=pre_article_text, metadata=chunk_metadata))\n",
    "#                 last_idx = article_matches[0].start()\n",
    "\n",
    "#             for i in range(len(article_matches)):\n",
    "#                 article_content = article_matches[i].group(0).strip()\n",
    "#                 article_number = article_matches[i].group(self.article_num_capture_group)\n",
    "\n",
    "#                 chunk_metadata = page_metadata.copy()\n",
    "#                 chunk_metadata[\"type\"] = \"Article\"\n",
    "#                 chunk_metadata[\"article_number\"] = article_number\n",
    "\n",
    "#                 chunk_metadata.update(self._extract_hierarchy_metadata(article_content, page_metadata))\n",
    "\n",
    "#                 all_chunks.append(Document(page_content=article_content, metadata=chunk_metadata))\n",
    "#                 last_idx = article_matches[i].end()\n",
    "\n",
    "#             if last_idx < len(text):\n",
    "#                 remaining_text = text[last_idx:].strip()\n",
    "#                 if remaining_text:\n",
    "#                     chunk_metadata = self._extract_hierarchy_metadata(remaining_text, page_metadata)\n",
    "#                     all_chunks.append(Document(page_content=remaining_text, metadata=chunk_metadata))\n",
    "#         return all_chunks\n",
    "\n",
    "\n",
    "#     def _extract_hierarchy_metadata(self, text: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#         metadata = base_metadata.copy()\n",
    "\n",
    "#         title_match = self.title_pattern.search(text)\n",
    "#         if title_match:\n",
    "#             metadata[\"title\"] = title_match.group(1).strip()\n",
    "\n",
    "#         chapter_match = self.chapter_pattern.search(text)\n",
    "#         if chapter_match:\n",
    "#             metadata[\"chapter\"] = chapter_match.group(1).strip()\n",
    "\n",
    "#         section_match = self.section_pattern.search(text)\n",
    "#         if section_match:\n",
    "#             metadata[\"section\"] = section_match.group(1).strip()\n",
    "\n",
    "#         return metadata\n",
    "\n",
    "#     def split_text(self, text: str) -> List[str]:\n",
    "#         chunks = []\n",
    "#         article_splits = self.article_pattern.split(text)\n",
    "\n",
    "#         if len(article_splits) > 0 and not self.article_pattern.match(article_splits[0]):\n",
    "#             if article_splits[0].strip():\n",
    "#                 chunks.append(article_splits[0].strip())\n",
    "#             start_index = 1\n",
    "#         else:\n",
    "#             start_index = 0\n",
    "\n",
    "#         for i in range(start_index, len(article_splits), self.article_num_capture_group + 1):\n",
    "#             if i + self.article_num_capture_group < len(article_splits):\n",
    "#                 article_num = article_splits[i + self.article_num_capture_group -1]\n",
    "#                 article_content = article_splits[i + self.article_num_capture_group]\n",
    "#                 full_article_chunk = f\"Article {article_num.strip()} {article_content.strip()}\"\n",
    "#                 if full_article_chunk.strip():\n",
    "#                     chunks.append(full_article_chunk.strip())\n",
    "#             elif article_splits[i].strip():\n",
    "#                 chunks.append(article_splits[i].strip())\n",
    "#         return chunks\n",
    "\n",
    "# # --- Chargement et Traitement du PDF ---\n",
    "# documents = []\n",
    "# if not os.path.exists(pdf_path):\n",
    "#     st.error(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin.\")\n",
    "#     st.stop() # Arr√™te l'ex√©cution de l'application Streamlit\n",
    "# else:\n",
    "#     st.write(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "#     loader = PyPDFLoader(pdf_path)\n",
    "#     docs_from_loader = loader.load()\n",
    "\n",
    "#     if not docs_from_loader:\n",
    "#         st.error(\"Aucun document n'a pu √™tre charg√© par PyPDFLoader. Veuillez v√©rifier le PDF.\")\n",
    "#         st.stop() # Arr√™te l'ex√©cution de l'application Streamlit\n",
    "#     else:\n",
    "#         st.write(f\"Nombre de pages charg√©es par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "#         structure_extractor = CodeDuTravailStructureExtractor()\n",
    "#         documents = structure_extractor.split_documents(docs_from_loader)\n",
    "#         st.write(f\"Nombre de documents (chunks enrichis) apr√®s extraction de structure : {len(documents)}\")\n",
    "\n",
    "# if not documents:\n",
    "#     st.error(\"Le traitement ne peut pas continuer car aucun document structur√© n'a √©t√© cr√©√©.\")\n",
    "#     st.stop() # Arr√™te l'ex√©cution de l'application Streamlit\n",
    "\n",
    "# # --- Cr√©ation des Embeddings ---\n",
    "# try:\n",
    "#     embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "# except Exception as e:\n",
    "#     st.error(f\"ERREUR lors de l'initialisation du mod√®le d'embedding : {e}\")\n",
    "#     st.info(\"Veuillez v√©rifier votre connexion internet et les d√©pendances 'sentence-transformers'.\")\n",
    "#     st.stop() # Arr√™te l'ex√©cution de l'application Streamlit\n",
    "\n",
    "# # --- Cr√©ation ou chargement du Vectorstore Chroma ---\n",
    "# try:\n",
    "#     if os.path.exists(chroma_db_dir) and os.listdir(chroma_db_dir):\n",
    "#         try:\n",
    "#             vectorstore = Chroma(persist_directory=chroma_db_dir, embedding_function=embedding_model)\n",
    "#             st.success(\"Base de donn√©es vectorielle existante charg√©e avec succ√®s.\")\n",
    "#         except Exception as e:\n",
    "#             st.warning(f\"Erreur lors du chargement de la base existante : {e}. Cr√©ation d'une nouvelle base...\")\n",
    "#             vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "#             st.success(\"Nouvelle base de donn√©es vectorielle cr√©√©e avec succ√®s.\")\n",
    "#     else:\n",
    "#         vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "#         st.success(\"Nouvelle base de donn√©es vectorielle cr√©√©e avec succ√®s.\")\n",
    "# except Exception as e:\n",
    "#     st.error(f\"Erreur critique lors de l'initialisation de ChromaDB : {e}\")\n",
    "#     st.info(\"Veuillez v√©rifier que tous les packages sont correctement install√©s :\")\n",
    "#     st.code(\"pip install -U langchain-huggingface langchain-chroma chromadb jsonschema\")\n",
    "#     st.stop()\n",
    "\n",
    "# # --- Configuration des Retrievers Hybrides ---\n",
    "# semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": st.session_state.params['top_p']})\n",
    "# bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "# bm25_retriever.k = 5\n",
    "# hybrid_retriever = EnsembleRetriever(\n",
    "#     retrievers=[bm25_retriever, semantic_retriever],\n",
    "#     weights=[0.5, 0.5]\n",
    "# )\n",
    "# retriever = hybrid_retriever\n",
    "# st.write(\"Retrievers hybrides (BM25 et S√©mantique) configur√©s.\")\n",
    "\n",
    "# # --- Configuration du LLM (Gemini) ---\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# try:\n",
    "#     llm = ChatGoogleGenerativeAI(\n",
    "#         model=st.session_state.params['model'],\n",
    "#         temperature=st.session_state.params['temperature'])\n",
    "# except Exception as e:\n",
    "#     st.error(f\"ERREUR lors de l'initialisation du LLM ChatGoogleGenerativeAI : {e}\")\n",
    "#     st.info(\"Assurez-vous que GOOGLE_API_KEY est correctement d√©finie dans les secrets de Colab et que l'API Generative Language est activ√©e.\")\n",
    "#     st.stop()\n",
    "\n",
    "# # --- Cha√Æne de G√©n√©ration de Requ√™tes Multiples ---\n",
    "# multi_query_template = \"\"\"You are an AI assistant that generates multiple search queries based on a single input question. Your goal is to create diverse queries that cover various aspects of the user's original question, to improve the chances of retrieving relevant documents. Generate 3 distinct search queries related to the user's question, separated by newlines. Do not number the queries. Original question: {question} Search queries:\"\"\"\n",
    "# multi_query_prompt = ChatPromptTemplate.from_template(multi_query_template)\n",
    "# multi_query_chain = multi_query_prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "# st.write(\"Cha√Æne de g√©n√©ration de requ√™tes multiples initialis√©e.\")\n",
    "\n",
    "# # --- D√âFINITION FINALE DE LA CHA√éNE RAG AVEC MULTI-QUERY ET SELF-QUERYING ---\n",
    "# def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "#     formatted_content = \"\"\n",
    "#     unique_pages = set()\n",
    "\n",
    "#     for i, doc in enumerate(docs):\n",
    "#         formatted_content += f\"Contenu source {i+1}:\\n{doc.page_content}\\n\\n\"\n",
    "#         if 'page' in doc.metadata:\n",
    "#             page_number = doc.metadata['page'] + 1\n",
    "#             unique_pages.add(str(page_number))\n",
    "\n",
    "#     if unique_pages:\n",
    "#         formatted_content += f\"Sources des pages: {', '.join(sorted(list(unique_pages)))}\\n\"\n",
    "#     return formatted_content\n",
    "\n",
    "# prepare_inputs = {\n",
    "#     \"question_originale\": RunnablePassthrough(),\n",
    "#     \"questions_pour_retrieval\": multi_query_chain,\n",
    "# }\n",
    "\n",
    "# retrieval_and_aggregation_chain = (\n",
    "#     RunnableLambda(lambda x: x[\"questions_pour_retrieval\"])\n",
    "#     | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries])\n",
    "#     | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist])\n",
    "#     | RunnableLambda(\n",
    "#         lambda docs: list(\n",
    "#             {\n",
    "#                 (doc.page_content, frozenset(doc.metadata.items())): doc\n",
    "#                 for doc in docs\n",
    "#             }.values()\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# context_and_question_for_llm = {\n",
    "#     \"context\": retrieval_and_aggregation_chain,\n",
    "#     \"question\": RunnableLambda(lambda x: x[\"question_originale\"])\n",
    "# }\n",
    "\n",
    "# generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# rag_chain_with_sources = (\n",
    "#     prepare_inputs\n",
    "#     | RunnableParallel(\n",
    "#         response=context_and_question_for_llm | generation_chain,\n",
    "#         source_documents=context_and_question_for_llm | RunnableLambda(lambda x: x[\"context\"])\n",
    "#     )\n",
    "# ).with_config(run_name=\"RAG Chain with Multi-Query & Self-Query\")\n",
    "\n",
    "\n",
    "# if \"messages\" not in st.session_state:\n",
    "#     st.session_state.messages = []\n",
    "\n",
    "# for message in st.session_state.messages:\n",
    "#     with st.chat_message(message[\"role\"]):\n",
    "#         st.markdown(message[\"content\"])\n",
    "\n",
    "# if prompt_input := st.chat_input(\"Posez votre question sur le Code du Travail...\"):\n",
    "#     st.session_state.messages.append({\"role\": \"user\", \"content\": prompt_input})\n",
    "#     with st.chat_message(\"user\"):\n",
    "#         st.markdown(prompt_input)\n",
    "\n",
    "#     with st.chat_message(\"assistant\"):\n",
    "#         with st.spinner(\"Recherche de r√©ponse...\"):\n",
    "#             full_response = rag_chain_with_sources.invoke(prompt_input)\n",
    "#             st.markdown(full_response[\"response\"]) # Acc√©der √† la cl√© 'response'\n",
    "\n",
    "#             # Afficher les sources\n",
    "#             if \"source_documents\" in full_response and full_response[\"source_documents\"]:\n",
    "#                 st.subheader(\"Sources:\")\n",
    "#                 st.markdown(format_docs_with_sources(full_response[\"source_documents\"]))\n",
    "\n",
    "#         st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response[\"response\"]}) # Stocker la r√©ponse\n",
    "#         # Optionnel: stocker aussi les sources si tu veux les afficher √† chaque re-run\n",
    "#         # st.session_state.messages.append({\"role\": \"assistant_sources\", \"content\": format_docs_with_sources(full_response[\"source_documents\"])})\n",
    "\n",
    "\n",
    "# with st.sidebar:\n",
    "#     st.header(\"Options\")\n",
    "#     st.subheader(\"param√®tre du mod√®le\")\n",
    "\n",
    "#     model = st.selectbox(\n",
    "#         \"Mod√®le Gemini\",\n",
    "#         options=[\n",
    "#             \"gemini-2.5-flash-preview-05-20\",\n",
    "#             \"gemini-1.5-pro\",\n",
    "#             \"gemini-1.5-flash\"\n",
    "#         ],\n",
    "#         index=0,\n",
    "#         help=\"Choisissez le mod√®le Gemini √† utiliser\"\n",
    "#     )\n",
    "\n",
    "#     temperature = st.slider(\n",
    "#         \"Temperature\",\n",
    "#         min_value=0.0,\n",
    "#         max_value=1.0,\n",
    "#         value=st.session_state.params['temperature'],\n",
    "#         step=0.1,\n",
    "#         help=\"Contr√¥le la cr√©ativit√© des r√©ponses (0 = plus pr√©cis, 1 = plus cr√©atif)\"\n",
    "#     )\n",
    "#     top_p = st.slider(\n",
    "#         \"Top_p\",\n",
    "#         min_value=1,\n",
    "#         max_value=15,\n",
    "#         value=st.session_state.params['top_p'],\n",
    "#         step=1,\n",
    "#         help=\"Nombre de documents √† r√©cup√©rer pour la r√©ponse\"\n",
    "#     )\n",
    "\n",
    "#     st.session_state.params['model']=model\n",
    "#     st.session_state.params['temperature'] = temperature\n",
    "#     st.session_state.params['top_p'] = top_p\n",
    "\n",
    "#     if st.button(\"Effacer l'historique du chat\"):\n",
    "#         st.session_state.messages = []\n",
    "#         st.rerun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPnds2rEcjGD"
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import time\n",
    "# import re\n",
    "\n",
    "# print(\"Lancement de l'application Streamlit en arri√®re-plan...\")\n",
    "# # Lance Streamlit en arri√®re-plan sur le port 8501\n",
    "# streamlit_process = subprocess.Popen([\n",
    "#     \"streamlit\", \"run\", \"app.py\",\n",
    "#     \"--server.port\", \"8501\",\n",
    "#     \"--server.enableCORS\", \"false\",\n",
    "#     \"--server.enableXsrfProtection\", \"false\"\n",
    "# ])\n",
    "\n",
    "# print(\"Attente du d√©marrage de Streamlit (environ 5 secondes)...\")\n",
    "# time.sleep(5) # Donne √† Streamlit le temps de d√©marrer\n",
    "\n",
    "# print(\"Cr√©ation du tunnel localtunnel...\")\n",
    "# # Lance localtunnel pour exposer le port 8501\n",
    "# localtunnel_process = subprocess.Popen(['lt', '--port', '8501'], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "# public_url = None\n",
    "# for _ in range(30):\n",
    "#     line = localtunnel_process.stdout.readline()\n",
    "#     print(f\"localtunnel output: {line.strip()}\")\n",
    "#     match = re.search(r'https?://[^\\s/$.?#].[^\\s]*\\.loca\\.lt', line)\n",
    "#     if match:\n",
    "#         public_url = match.group(0)\n",
    "#         break\n",
    "#     time.sleep(1)\n",
    "\n",
    "# if public_url:\n",
    "#     print(f\"üéâ Votre application Streamlit est disponible √† l'adresse : {public_url}\")\n",
    "# else:\n",
    "#     print(\"Erreur : Impossible d'obtenir l'URL de localtunnel. V√©rifie la sortie ci-dessus pour les erreurs.\")\n",
    "#     print(\"V√©rifie si localtunnel est bien install√© (`!npm install -g localtunnel`) et si Streamlit a d√©marr√©.\")\n",
    "#     streamlit_process.terminate() # Tente de tuer le processus Streamlit si localtunnel √©choue\n",
    "#     localtunnel_process.terminate() # Tente de tuer le processus localtunnel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNRD7DI3p6Og"
   },
   "outputs": [],
   "source": [
    "# if 'streamlit_process' in locals() and streamlit_process.poll() is None:\n",
    "#     streamlit_process.terminate()\n",
    "#     print(\"Processus Streamlit termin√©.\")\n",
    "# if 'localtunnel_process' in locals() and localtunnel_process.poll() is None:\n",
    "#     localtunnel_process.terminate()\n",
    "#     print(\"Processus localtunnel termin√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGGosCV0o5Fr"
   },
   "outputs": [],
   "source": [
    "# !curl https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASo4_Gohn0L"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749897660494,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "lpTpR1tCIIbT",
    "outputId": "85839df3-71b4-47ea-9c34-f910deabe278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de donn√©es d'√©valuation cr√©√© avec 2 questions.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"question\": \"\n",
    "        \"ground_truth_answer\": \"La loi n¬∞ 2008-67 du 21 janvier 2008 pr√©voit une concertation pr√©alable obligatoire pour tout projet de r√©forme du Gouvernement concernant les relations individuelles et collectives du travail, l'emploi et la formation professionnelle qui rel√®ve de la n√©gociation nationale et interprofessionnelle. Cette concertation s'adresse aux organisations syndicales de salari√©s et d'employeurs repr√©sentatives au niveau national et interprofessionnel. Le Gouvernement doit leur communiquer un document d'orientation. Cette proc√©dure n'est pas applicable en cas d'urgence.\",\n",
    "        \"ground_truth_relevant_document_pages\": [0] # Page 1 du document\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Quels sont les sujets principaux de la n√©gociation triennale sur l'√©galit√© professionnelle suite √† l'ordonnance de 2017 ?\",\n",
    "        \"ground_truth_answer\": \"L'ordonnance n¬∞ 2017-1385 du 22 septembre 2017 a modifi√© la n√©gociation triennale sur l'√©galit√© professionnelle entre les femmes et les hommes. Cette n√©gociation porte notamment sur les conditions d'acc√®s √† l'emploi, √† la formation et √† la promotion professionnelle, ainsi que sur les conditions de travail et d'emploi, en particulier celles des salari√©s √† temps partiel. Lorsque les mesures de rattrapage portent sur des mesures salariales, leur mise en ≈ìuvre est suivie dans le cadre de la n√©gociation annuelle obligatoire sur les salaires.\",\n",
    "        \"ground_truth_relevant_document_pages\": [277] # Page 278 du document\n",
    "    }\n",
    "    # {\n",
    "    #     \"question\": \"√ânum√©rez les sept crit√®res l√©gaux de repr√©sentativit√© des organisations syndicales de salari√©s.\",\n",
    "    #     \"ground_truth_answer\": \"Les sept crit√®res l√©gaux de repr√©sentativit√© des organisations syndicales de salari√©s sont : 1) Le respect des valeurs r√©publicaines, 2) L'ind√©pendance, 3) La transparence financi√®re, 4) Une anciennet√© minimale de deux ans dans le champ professionnel et g√©ographique de la n√©gociation, 5) L'audience (mesur√©e par les r√©sultats aux √©lections professionnelles), 6) L'influence (caract√©ris√©e par l'activit√© et l'exp√©rience), 7) Les effectifs d'adh√©rents et les cotisations.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [25] # Page 25 du document (Bas√© sur L. 2121-1)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelle est la mission des inspecteurs du travail et quel article du Code du travail la d√©finit ?\",\n",
    "    #     \"ground_truth_answer\": \"Les inspecteurs du travail ont pour mission de veiller √† l'application des dispositions l√©gales relatives au r√©gime du travail. Ils exercent un r√¥le de contr√¥le et de conseil. L'article L. 8112-1 du Code du travail d√©finit cette mission.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [10] # Page 10 du document (Bas√© sur L. 8112-1)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"En dehors de l'√©galit√© professionnelle, quels autres sujets sont abord√©s dans la n√©gociation triennale ?\",\n",
    "    #     \"ground_truth_answer\": \"Outre l'√©galit√© professionnelle, la n√©gociation triennale aborde √©galement les conditions de travail et la gestion pr√©visionnelle des emplois et des parcours professionnels (GPEC), notamment au travers du paragraphe 2 de l'article L. 2241-11 sur les conditions de travail et gestion pr√©visionnelle des emplois.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5] # Page 5 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"La loi n¬∞ 2020-1525 du 7 d√©cembre 2020 est mentionn√©e en page 1. Quel est son impact sur le dialogue social ?\",\n",
    "    #     \"ground_truth_answer\": \"La loi n¬∞ 2020-1525 du 7 d√©cembre 2020 est mentionn√©e en page 1 en lien avec le chapitre pr√©liminaire sur le dialogue social. Cependant, les extraits de la page 1 ne d√©crivent pas en d√©tail l'impact sp√©cifique de cette loi sur le dialogue social, mais la situent dans ce contexte.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1] # Page 1 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quand le Gouvernement peut-il d√©roger √† la proc√©dure de concertation pr√©alable pour une r√©forme du travail, et que doit-il faire dans ce cas ?\",\n",
    "    #     \"ground_truth_answer\": \"Le pr√©sent article (L.1) n'est pas applicable en cas d'urgence. Lorsque le Gouvernement d√©cide de mettre en ≈ìuvre un projet de r√©forme en l'absence de proc√©dure de concertation en raison de l'urgence, il doit faire conna√Ætre cette d√©cision aux organisations syndicales et d'employeurs en la motivant dans un document transmis avant de prendre toute mesure.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1] # Page 1 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelles informations sont n√©cessaires √† la n√©gociation sur l'√©galit√© professionnelle ?\",\n",
    "    #     \"ground_truth_answer\": \"Les informations n√©cessaires √† la n√©gociation sur l'√©galit√© professionnelle entre les femmes et les hommes sont d√©termin√©es par voie r√©glementaire.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5] # Page 5 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Qu'est-ce que l'audience √©lectorale dans le cadre de la repr√©sentativit√© syndicale, et √† quel article fait-elle r√©f√©rence pour la n√©gociation annuelle obligatoire ?\",\n",
    "    #     \"ground_truth_answer\": \"L'audience √©lectorale est un crit√®re de repr√©sentativit√© des organisations syndicales, mesur√© notamment lors des √©lections professionnelles. La mise en ≈ìuvre des mesures de rattrapage salariales issues de la n√©gociation sur l'√©galit√© professionnelle est suivie dans le cadre de la n√©gociation annuelle obligatoire sur les salaires, pr√©vue √† l'article L. 2241-8 du Code du travail.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5, 25] # Pages 5 et 25 (exemple de multi-sources)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelle est l'importance de la transparence financi√®re pour une organisation syndicale ?\",\n",
    "    #     \"ground_truth_answer\": \"La transparence financi√®re est l'un des sept crit√®res de repr√©sentativit√© des organisations syndicales de salari√©s, essentiel pour garantir leur l√©gitimit√© et leur int√©grit√©.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [25] # Page 25 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Comment la loi de 2008 et l'ordonnance de 2017 ont-elles affect√© le Code du travail concernant le dialogue social et l'√©galit√© professionnelle ?\",\n",
    "    #     \"ground_truth_answer\": \"La loi n¬∞ 2008-67 du 21 janvier 2008 a introduit des dispositions concernant le chapitre pr√©liminaire sur le dialogue social, notamment la concertation pr√©alable pour les r√©formes du Gouvernement. L'ordonnance n¬∞ 2017-1385 du 22 septembre 2017, quant √† elle, a modifi√© les mesures tendant √† assurer l'√©galit√© professionnelle entre les femmes et les hommes, impactant les n√©gociations triennales sur ce sujet, y compris les conditions d'acc√®s √† l'emploi et de travail.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1, 5] # Exemple de question n√©cessitant de multiples sources\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(f\"Jeu de donn√©es d'√©valuation cr√©√© avec {len(evaluation_dataset)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21335,
     "status": "ok",
     "timestamp": 1749897681820,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "n1cRh_C2Lw0I",
    "outputId": "534f67f2-3630-4f3e-d05f-73e64c9baaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m184.3/190.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5215,
     "status": "ok",
     "timestamp": 1749897687037,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "m-e171sWQjbQ",
    "outputId": "16d70241-d0c9-448e-944e-abe9bcadefae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ragas\n",
      "Version: 0.2.15\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: appdirs, datasets, diskcache, langchain, langchain-community, langchain-core, langchain_openai, nest-asyncio, numpy, openai, pydantic, tiktoken\n",
      "Required-by: \n",
      "---\n",
      "Name: datasets\n",
      "Version: 3.6.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: ragas, torchtune\n"
     ]
    }
   ],
   "source": [
    "!pip show ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5391,
     "status": "ok",
     "timestamp": 1749897692431,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "tlXJiET6IXjE",
    "outputId": "5034fe27-c126-485b-9638-7d6d8b5a153e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âvaluation du Retrieval ---\n",
      "  Q1: 'Selon la loi de 2008, quelle est la proc√©dure de concertation pr√©alabl...'\n",
      "    GT Pages: [0], Retrieved Pages: [0, 21, 22, 37, 61, 68, 78, 79, 85, 92, 107, 181, 182, 187, 222, 255, 256]\n",
      "    Hit: True, Recall: 1.00, MRR: 1.00\n",
      "  Q2: 'Quels sont les sujets principaux de la n√©gociation triennale sur l'√©ga...'\n",
      "    GT Pages: [277], Retrieved Pages: [12, 39, 114, 118, 121, 169, 170, 216, 217, 222, 228, 229, 253, 255, 261, 264, 272, 275, 276, 277]\n",
      "    Hit: True, Recall: 1.00, MRR: 1.00\n",
      "\n",
      "--- R√©sultats G√©n√©raux du Retrieval ---\n",
      "Average Hit Rate: 1.00\n",
      "Average Recall: 1.00\n",
      "Average MRR: 1.00\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda # Pour la conversion de la cha√Æne en liste\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def evaluate_retrieval(retriever_instance, eval_data: List[Dict]):\n",
    "    \"\"\"\n",
    "    √âvalue les m√©triques de r√©cup√©ration (Hit Rate, Recall, MRR) d'un retriever.\n",
    "\n",
    "    Args:\n",
    "        retriever_instance: L'instance du retriever LangChain √† √©valuer (ton hybrid_retriever).\n",
    "        eval_data: Le jeu de donn√©es d'√©valuation.\n",
    "    \"\"\"\n",
    "    hit_rates = []\n",
    "    recalls_at_k = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    print(\"\\n--- √âvaluation du Retrieval ---\")\n",
    "    for i, item in enumerate(eval_data):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth_pages = set(item[\"ground_truth_relevant_document_pages\"])\n",
    "\n",
    "        try:\n",
    "            # Il est primordial de s'assurer que la question est un dictionnaire si la cha√Æne d'entr√©e l'attend\n",
    "            # Si le rag_chain_with_sources.invoke(question) prend une string,\n",
    "            # alors le retriever.invoke() prendra aussi la string.\n",
    "            # Cependant, avec Multi-Query, le retriever est appel√© via la cha√Æne interne.\n",
    "            # Pour l'√©valuation du retriever seul, nous devons lui passer la question directe.\n",
    "            # Ici, nous allons simuler l'appel √† la cha√Æne de multi-query + retriever agr√©g√©.\n",
    "\n",
    "            # √âtape de multi-query\n",
    "            # L'on s'assure que multi_query_chain attend un dictionnaire comme entr√©e si elle fait partie d'une RunnableSequence\n",
    "            generated_queries = multi_query_chain.invoke({\"question\": question})\n",
    "\n",
    "            # Ex√©cution de l'EnsembleRetriever pour chaque query et agr√©gation\n",
    "            all_retrieved_docs = []\n",
    "            for q_gen in generated_queries:\n",
    "                # C'est ici que l'EnsembleRetriever (qui contient SelfQueryRetriever) est invoqu√©.\n",
    "                # Il prend directement la requ√™te en string.\n",
    "                # Le retriever_instance est l'EnsembleRetriever\n",
    "                retrieved_for_query = retriever_instance.invoke(q_gen)\n",
    "                all_retrieved_docs.extend(retrieved_for_query)\n",
    "\n",
    "            # D√©duplication des documents r√©cup√©r√©s\n",
    "            # Un set de tuples (page_content, metadata_tuple) pour √™tre s√ªr de la d√©duplication\n",
    "            unique_docs = []\n",
    "            seen_doc_ids = set() # Utilise un identifiant unique si disponible, sinon page_content\n",
    "            for doc in all_retrieved_docs:\n",
    "                doc_id = (doc.page_content, frozenset(doc.metadata.items())) # Cr√©e un identifiant unique bas√© sur content + metadata\n",
    "                if doc_id not in seen_doc_ids:\n",
    "                    unique_docs.append(doc)\n",
    "                    seen_doc_ids.add(doc_id)\n",
    "\n",
    "            retrieved_docs = unique_docs\n",
    "\n",
    "            # Extraction des pages des documents r√©cup√©r√©s\n",
    "            retrieved_pages = set([doc.metadata.get('page', -1) for doc in retrieved_docs if 'page' in doc.metadata])\n",
    "\n",
    "            # --- Calcul des m√©triques ---\n",
    "            # Hit Rate\n",
    "            # V√©rifie si AU MOINS une page r√©cup√©r√©e est dans les pages attendues\n",
    "            hit = any(p in ground_truth_pages for p in retrieved_pages)\n",
    "            hit_rates.append(1 if hit else 0)\n",
    "\n",
    "            # Recall@k (k est le nombre total de documents pertinents dans le ground truth)\n",
    "            # Mesure la proportion de documents pertinents attendus qui ont √©t√© effectivement r√©cup√©r√©s.\n",
    "            if len(ground_truth_pages) > 0:\n",
    "                relevant_retrieved = len(ground_truth_pages.intersection(retrieved_pages))\n",
    "                recalls_at_k.append(relevant_retrieved / len(ground_truth_pages))\n",
    "            else: # Si pas de documents pertinents attendus, recall est 1 si 0 documents sont r√©cup√©r√©s\n",
    "                recalls_at_k.append(1 if len(retrieved_pages) == 0 else 0)\n",
    "\n",
    "            # MRR (Mean Reciprocal Rank)\n",
    "            # Mesure o√π le premier document pertinent appara√Æt dans le classement.\n",
    "            mrr_score = 0\n",
    "            # On cherche la premi√®re page pertinente dans la liste ORDONN√âE des documents r√©cup√©r√©s\n",
    "            # L'ordre ici d√©pend de la fusion des rangs de l'EnsembleRetriever\n",
    "            for rank, doc in enumerate(retrieved_docs):\n",
    "                if doc.metadata.get('page', -1) in ground_truth_pages:\n",
    "                    mrr_score = 1.0 / (rank + 1)\n",
    "                    break\n",
    "            mrr_scores.append(mrr_score)\n",
    "\n",
    "            print(f\"  Q{i+1}: '{question[:70]}...'\")\n",
    "            print(f\"    GT Pages: {sorted(list(ground_truth_pages))}, Retrieved Pages: {sorted(list(retrieved_pages))}\")\n",
    "            print(f\"    Hit: {hit}, Recall: {recalls_at_k[-1]:.2f}, MRR: {mrr_scores[-1]:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur lors de l'√©valuation de la question '{question[:70]}...': {e}\")\n",
    "            hit_rates.append(0)\n",
    "            recalls_at_k.append(0)\n",
    "            mrr_scores.append(0)\n",
    "\n",
    "\n",
    "    print(\"\\n--- R√©sultats G√©n√©raux du Retrieval ---\")\n",
    "    # On √©vite la division par z√©ro si eval_data est vide\n",
    "    num_questions = len(eval_data)\n",
    "    if num_questions > 0:\n",
    "        print(f\"Average Hit Rate: {sum(hit_rates) / num_questions:.2f}\")\n",
    "        print(f\"Average Recall: {sum(recalls_at_k) / num_questions:.2f}\")\n",
    "        print(f\"Average MRR: {sum(mrr_scores) / num_questions:.2f}\")\n",
    "    else:\n",
    "        print(\"Aucune question dans le jeu de donn√©es d'√©valuation.\")\n",
    "\n",
    "\n",
    "# --- Appel de la fonction d'√©valuation apr√®s la d√©finition de toutes les cha√Ænes ---\n",
    "# L'on s'assure que 'retriever' et 'multi_query_chain' sont d√©finis avant cet appel.\n",
    "evaluate_retrieval(retriever, evaluation_dataset)\n",
    "\n",
    "\n",
    "# --- Int√©gration de RAGAS pour l'√©valuation de la g√©n√©ration ---\n",
    "# NB: ragas doit √™tre bien installer - !pip install -q ragas datasets\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ContextRelevance,\n",
    "    AnswerCorrectness\n",
    ")\n",
    "\n",
    "def evaluate_generation_with_ragas(rag_chain, eval_data: List[Dict], llm_model, embedding_model_ragas):\n",
    "    \"\"\"\n",
    "    √âvalue la g√©n√©ration de la r√©ponse RAG en utilisant RAGAS.\n",
    "    \"\"\"\n",
    "    data_for_ragas = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [], # Liste de listes de strings (contenu des docs)\n",
    "        \"ground_truth\": [] # Les r√©ponses de r√©f√©rence humaines\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Pr√©paration des donn√©es pour RAGAS (ex√©cution du RAG pour chaque question) ---\")\n",
    "    for i, item in enumerate(eval_data):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth_answer = item.get(\"ground_truth_answer\", None)\n",
    "\n",
    "        try:\n",
    "            # L'entr√©e de rag_chain_with_sources est directement la string de la question\n",
    "            result = rag_chain.invoke(question) # Appelle du RAG complet\n",
    "\n",
    "            data_for_ragas[\"question\"].append(question)\n",
    "            data_for_ragas[\"answer\"].append(result.get(\"response\", \"No response generated\"))\n",
    "            # ragas attend une liste de strings pour le contexte\n",
    "            data_for_ragas[\"contexts\"].append([doc.page_content for doc in result.get(\"source_documents\", [])])\n",
    "            data_for_ragas[\"ground_truth\"].append(ground_truth_answer)\n",
    "            print(f\"  Processed Q{i+1}: '{question[:70]}...'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERREUR lors de l'ex√©cution du RAG pour la question '{question[:70]}...': {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Ajout des valeurs vides pour ne pas casser RAGAS\n",
    "            data_for_ragas[\"question\"].append(question)\n",
    "            data_for_ragas[\"answer\"].append(\"ERROR: RAG chain failed\")\n",
    "            data_for_ragas[\"contexts\"].append([])\n",
    "            data_for_ragas[\"ground_truth\"].append(ground_truth_answer)\n",
    "\n",
    "    # S'assurer qu'il y a des donn√©es avant de cr√©er le Dataset\n",
    "    if not data_for_ragas[\"question\"]:\n",
    "         print(\"Aucune donn√©e n'a pu √™tre pr√©par√©e pour RAGAS.\")\n",
    "         return\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_for_ragas)\n",
    "\n",
    "    print(\"\\n--- Ex√©cution de l'√©valuation RAGAS ---\")\n",
    "    # Liste des m√©triques √† √©valuer\n",
    "    metrics_to_evaluate = [\n",
    "        Faithfulness,\n",
    "        ContextRelevance,\n",
    "        AnswerCorrectness\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result_ragas = evaluate(\n",
    "            ragas_dataset,\n",
    "            metrics=metrics_to_evaluate,\n",
    "            llm=llm_model, # Utilisation du LLM pour les m√©triques bas√©es sur LLM-as-a-Judge\n",
    "            embeddings=embedding_model_ragas, # Utilisation du mod√®le d'embedding pour les m√©triques bas√©es sur embedding-as-a-Judge\n",
    "        )\n",
    "\n",
    "        df_results_ragas = result_ragas.to_dataframe()\n",
    "        print(\"\\n--- R√©sultats RAGAS Moyens ---\")\n",
    "        print(df_results_ragas.mean(numeric_only=True))\n",
    "        print(\"\\n--- D√©tail des R√©sultats RAGAS ---\")\n",
    "        print(df_results_ragas)\n",
    "\n",
    "    except Exception as e:\n",
    "          print(f\"ERREUR lors de l'ex√©cution de l'√©valuation RAGAS: {e}\")\n",
    "          import traceback\n",
    "          traceback.print_exc() # Afficher la trace compl√®te de l'erreur pour le debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9051,
     "status": "ok",
     "timestamp": 1749897701487,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "RlHH9s0tvbAV",
    "outputId": "704eec54-2d1b-45d0-f40f-09adeefe5c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pr√©paration des donn√©es pour RAGAS (ex√©cution du RAG pour chaque question) ---\n",
      "  Processed Q1: 'Selon la loi de 2008, quelle est la proc√©dure de concertation pr√©alabl...'\n",
      "  Processed Q2: 'Quels sont les sujets principaux de la n√©gociation triennale sur l'√©ga...'\n",
      "\n",
      "--- Ex√©cution de l'√©valuation RAGAS ---\n",
      "ERREUR lors de l'ex√©cution de l'√©valuation RAGAS: 'property' object has no attribute 'get'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-17-3883995461>\", line 176, in evaluate_generation_with_ragas\n",
      "    result_ragas = evaluate(\n",
      "                   ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/_analytics.py\", line 227, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/evaluation.py\", line 176, in evaluate\n",
      "    validate_required_columns(dataset, metrics)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/validation.py\", line 60, in validate_required_columns\n",
      "    required_columns = set(m.required_columns.get(metric_type, []))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'property' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "# Appel de la fonction d'√©valuation RAGAS\n",
    "# L'on s'assure que rag_chain_with_sources, llm, et embedding_model sont d√©finis.\n",
    "\n",
    "if 'rag_chain_with_sources' in globals() and 'llm' in globals() and 'embedding_model' in globals() and 'evaluation_dataset' in globals():\n",
    "    evaluate_generation_with_ragas(rag_chain_with_sources, evaluation_dataset, llm, embedding_model)\n",
    "else:\n",
    "    print(\"Impossible d'ex√©cuter l'√©valuation RAGAS : variables n√©cessaires non d√©finies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1749897701491,
     "user": {
      "displayName": "Ruben Mougou√©",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "xvOOo1YKY_4a",
    "outputId": "a9193e10-a3b5-48e0-9070-6571bd6845aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1', 'chapter': 'Chapitre pr√©liminaire : Dialogue social. \\nPartie l√©gislative'}\n",
      "{'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 4, 'page_label': '5'}\n"
     ]
    }
   ],
   "source": [
    "if documents:\n",
    "    print(documents[0].metadata) # Regardez la cl√© 'page' pour le premier document\n",
    "    print(documents[4].metadata) # Regardez la cl√© 'page' pour le cinqui√®me document (si disponible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhBxVVzpJlF"
   },
   "source": [
    "## UTILITE DU PROJET\n",
    "1. **Qu'est-ce qu'une application RAG ? (Retrieval Augmented Generation)**\n",
    "Imaginons une immense biblioth√®que (dans notre cas, un PDF de 2775 pages sur le droit du travail) et qu'un utilisateur pose une question tr√®s sp√©cifique.\n",
    "\n",
    "**Sans RAG :** Si l'utilisateur posait cette question √† un grand mod√®le de langage (LLM) comme Gemini sans RAG, il essayerait de r√©pondre uniquement avec ce qu'il a appris pendant son entra√Ænement. Le probl√®me, c'est que Gemini n'a pas √©t√© sp√©cifiquement entra√Æn√© sur le document de droit du travail. Il pourrait inventer des choses (\"halluciner\") ou donner des r√©ponses g√©n√©riques, car il ne \"conna√Æt\" pas les d√©tails du documents et donc du droit du travail.\n",
    "\n",
    "**Avec RAG :** Le RAG r√©sout ce probl√®me en ajoutant une √©tape cruciale : _la recherche d'informations (Retrieval)_ avant la _g√©n√©ration de la r√©ponse (Generation)_.\n",
    "\n",
    "- **Recherche (Retrieval) :** Quand l'utilisateur poses une question, l'application RAG va d'abord chercher les passages les plus pertinents du document (corpus) qui sont susceptibles de contenir la r√©ponse.\n",
    "- **Augmentation (Augmented) :** Ces passages pertinents sont ensuite donn√©s au LLM (Gemini) en m√™me temps que ta question. Le LLM est alors \"augment√©\" avec des connaissances sp√©cifiques.\n",
    "- **G√©n√©ration (Generation) :** Fort de ces informations contextuelles, le LLM peut maintenant g√©n√©rer une r√©ponse pr√©cise et bas√©e sur les faits extraits de ton document."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
