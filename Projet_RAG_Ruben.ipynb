{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60162,
     "status": "ok",
     "timestamp": 1749897484933,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "O7rbhfeLnt59",
    "outputId": "ab16d58b-c9fd-4957-d528-f18e4e2e7901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/dist-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.12)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.65)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.4.20250611)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.5)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.22.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Toutes les dépendances ont été installées ou mises à jour.\n"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances nécessaires\n",
    "!pip install -q langchain-google-genai pypdf langchain-chroma faiss-cpu sentence-transformers streamlit\n",
    "!pip install langchain_community langchainhub chromadb langchain\n",
    "!pip install -q python-dotenv # Pour gérer les variables d'environnement (API key)\n",
    "!pip install -q transformers # Pour le modèle d'embedding HuggingFace\n",
    "\n",
    "# Assurons-nous que LangChain est à jour pour les dernières fonctionnalités\n",
    "!pip install -q --upgrade langchain\n",
    "\n",
    "print(\"Toutes les dépendances ont été installées ou mises à jour.\")\n",
    "\n",
    "from google.colab import userdata\n",
    "userdata.get('GOOGLE_API_KEY')\n",
    "userdata.get('LANGCHAIN_API_KEY')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1766,
     "status": "ok",
     "timestamp": 1749897486731,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "fzg6LMMFopbH",
    "outputId": "57c50f12-dfed-4bb6-e236-f68e0d59a113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Path to the pdf file\n",
    "pdf_path = '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf' # chemin d'accès au document qui constitura notre corpus\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dgpeqZ9gFmQ"
   },
   "source": [
    "# Objectif du projet\n",
    "L'objectif de ce projet est celui de la mise en place d'un système RAG (Retrieval Augmented Generation) dédié à la partie législative du Code du travail (plus précisement jusqu'à la Partie II - Livre II : La négociation collective - Les conventions et accords collectifs de travail, Sous-section 3, paragraphe 1er - Egalité professionnelle entre les femmes et les hommes), servant d’**outil de requêtage intelligent** et d’**aide à la décision** pour des boîtes de **consultation juridique**.\n",
    "\n",
    "L’objectif est de permettre aux utilisateurs (juristes, consultants, avocats juniors, etc.) de poser des questions en langage naturel et d’obtenir des réponses précises, contextualisées et appuyées par des sources juridiques fiables (textes de loi, jurisprudence, doctrine).\n",
    "\n",
    "**NB :** le RAG qui sera mis sur pied sera spécifique a un sous ensemble de la partie législative du code droit du travail. Ce serait très ambitieux de notre part de mettre sur pied un RAG couvrant l’ensemble du droit français étant donné le temps imparti pour la réalisation de ce projet (le droit étant vaste, technique et complexe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy13L0d9hIwF"
   },
   "source": [
    "# ETAPES CLES\n",
    "1. **Préparation de l'environnement :** Installer les outils et librairies nécessaires.\n",
    "2. **Chargement et découpage du document (Document Loading & Splitting) :**\n",
    "Chargement du pdf de 278 pages (code du travail). Un document aussi volumineux ne peut pas être traité d'un seul bloc par un modèle de langage. Nous devrons le découper en morceaux plus petits (appelés \"chunks\" ou \"morceaux\") tout en conservant le contexte. C'est crucial pour la pertinence des recherches.\n",
    "3. **Création des embeddings (Embeddings Generation) :**\n",
    "Chaque morceau de texte sera transformé en une représentation numérique appelée \"embedding\". C'est une sorte de \"vecteur\" qui capture la signification sémantique du texte. Ces embeddings nous permettront de trouver des morceaux de texte similaires à la question poser par un utilisateur (sous forme de requête).\n",
    "3. **Stockage vectoriel avec Chroma (Vector Store with ChromaDB) :**\n",
    "Nous allons stocker ces embeddings, ainsi que le texte original des morceaux, dans une base de données vectorielle appelée Chroma.\n",
    "Chroma est optimisé pour les recherches de similarité vectorielle, ce qui est exactement ce dont nous avons besoin pour récupérer les informations pertinentes.\n",
    "4. **Construction de la chaîne RAG avec LangChain (Building the RAG Chain with LangChain) :**\n",
    "LangChain est un framework qui aide à orchestrer les différentes étapes (récupération de documents, génération de réponses).\n",
    "Nous utiliserons LangChain pour relier notre base de données vectorielle (Chroma) et notre modèle de langage (Gemini).\n",
    "5. **Intégration du modèle de langage Gemini (LLM Integration with Gemini) :**\n",
    "Gemini sera le cerveau de notre application. Il prendra les morceaux de texte pertinents et la question pour générer une réponse cohérente et informative.\n",
    "6. **Construction de l'interface utilisateur avec Streamlit (Streamlit UI) :**\n",
    "Streamlit nous permettra de créer une interface web simple et interactive où l'utilisateur pourras poser ses questions et voir les réponses, y compris les sources.\n",
    "7. **Gestion des sources/Gestion des Hallucinations (Source Retrieval) :**\n",
    "Nous nous assurerons que chaque réponse est accompagnée de l'indication de la page ou du morceau de texte d'où provient l'information.\n",
    "\n",
    "## Pourquoi ces outils ?\n",
    "\n",
    "1. **LangChain :** C'est un framework puissant qui simplifie énormément la construction d'applications basées sur les grands modèles de langage (LLMs). Il offre des composants modulaires pour le chargement de documents, le découpage, les bases de données vectorielles, et l'intégration de LLMs.\n",
    "2. **Chroma :** Une base de données vectorielle légère et facile à utiliser, parfaite pour les projets de taille moyenne et l'apprentissage. Elle est souvent utilisée avec LangChain.\n",
    "3. **Gemini :** Un modèle de langage performant de Google, capable de comprendre le langage naturel et de générer des réponses pertinentes.\n",
    "4. **Streamlit :** C'est un excellent outil pour construire rapidement des interfaces utilisateur pour des applications de machine learning et de data science, sans avoir besoin de connaissances approfondies en développement web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5860NhgZjHh-"
   },
   "source": [
    "## PARTIE INDEXATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfp0qpoTv1BS"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class TitleBasedSplitter(TextSplitter):\n",
    "    \"\"\"\n",
    "    Splitter de texte basé sur la détection de titres.\n",
    "    Garde les métadonnées de la page d'origine pour chaque chunk.\n",
    "    \"\"\"\n",
    "    def __init__(self, pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:)\"):\n",
    "        \"\"\"\n",
    "        Initialise le splitter avec un pattern regex pour les titres.\n",
    "        Le pattern par défaut cherche \"Titre \" suivi de:\n",
    "        - Soit des chiffres romains (I, V, X...) optionnellement suivis de \"er\" ou \"ème\".\n",
    "        - Soit des mots (lettres, chiffres, accents, apostrophes, tirets).\n",
    "        - Le tout suivi d'un deux-points (avec espaces optionnels autour).\n",
    "        Exemples: \"Titre Ier :\", \"Titre préliminaire :\", \"Titre II :\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    def split_documents(self, documents: List[Document], **kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Divise une liste de documents (pages) en chunks basés sur les titres.\n",
    "        Chaque chunk hérite des métadonnées de son document d'origine.\n",
    "        \"\"\"\n",
    "        all_chunks: List[Document] = []\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            metadata = doc.metadata.copy() # Copie des métadonnées de la page d'origine\n",
    "\n",
    "            matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "            if not matches:\n",
    "                # Si aucun titre n'est trouvé sur la page, la page entière est un chunk\n",
    "                if text.strip(): # S'assurer que le texte n'est pas vide\n",
    "                    all_chunks.append(Document(page_content=text.strip(), metadata=metadata))\n",
    "                continue # Passer à la page suivante\n",
    "\n",
    "            # Cas où il y a du texte avant le premier titre sur la page\n",
    "            if matches[0].start() > 0:\n",
    "                pre_title_text = text[0:matches[0].start()].strip()\n",
    "                if pre_title_text:\n",
    "                    all_chunks.append(Document(page_content=pre_title_text, metadata=metadata))\n",
    "\n",
    "            for i in range(len(matches)):\n",
    "                start = matches[i].start()\n",
    "                # La fin du chunk est le début du prochain match, ou la fin du texte si c'est le dernier match\n",
    "                end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "                chunk_content = text[start:end].strip()\n",
    "\n",
    "                if chunk_content:\n",
    "                    # Ajout de métadonnées spécifiques au chunk si besoin,\n",
    "                    # par exemple le titre identifié ou une ID de chunk\n",
    "                    chunk_metadata = metadata.copy()\n",
    "                    # On peut ajouter ici des infos comme le titre spécifique du chunk\n",
    "                    # chunk_metadata[\"title\"] = matches[i].group(0).strip()\n",
    "                    all_chunks.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "    # Nous n'avons pas besoin de split_text si nous utilisons split_documents directement sur des objets Document.\n",
    "    # Cependant, si l'on veux conserver la compatibilité avec d'autres TextSplitter,\n",
    "    # l'on peut implémenter split_text pour une chaîne unique.\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Divise une chaîne de texte en chunks basés sur les titres.\n",
    "        Cette méthode est appelée par split_documents si un Document est passé.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "        if not matches:\n",
    "            return [text.strip()] if text.strip() else []\n",
    "\n",
    "        # Ajouter le texte avant le premier titre comme un chunk si non vide\n",
    "        if matches[0].start() > 0:\n",
    "            pre_title_text = text[0:matches[0].start()].strip()\n",
    "            if pre_title_text:\n",
    "                chunks.append(pre_title_text)\n",
    "\n",
    "        for i in range(len(matches)):\n",
    "            start = matches[i].start()\n",
    "            end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "            chunk_content = text[start:end].strip()\n",
    "            if chunk_content:\n",
    "                chunks.append(chunk_content)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25310,
     "status": "ok",
     "timestamp": 1749897512639,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "IuxnQp4vwqxB",
    "outputId": "f66d7af6-652e-4792-8624-975e5706a003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
      "Nombre de pages chargées par PyPDFLoader : 278\n",
      "Nombre de documents (chunks) après TitleBasedSplitter : 312\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Page content (début): Partie législative - Chapitre préliminaire : Dialogue social. \n",
      "Partie législative\n",
      "Chapitre préliminaire : Dialogue social.\n",
      "L. 1  LOI n°2008-67 du 21 janvier 2008 - art. 3      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Tout projet de réforme envisagé par le Gouvernement qui porte sur les relations individuelles et collectives\n",
      "du travail, l'emploi et la formation professionnelle et qui relève du champ de la négociation nationale et\n",
      "interprofessionnelle fait l'objet d'une conc...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1'}\n",
      "Source Page: 1\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "Première partie : Les relations individuelles de travail\n",
      "Livre Ier : Dispositions préliminaires...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
      "Source Page: 2\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Page content (début): Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "Chapitre unique.\n",
      "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du présent livre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\n",
      "Elles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\n",
      "privé, sous réserve des dispositions particulières ayant le même objet rés...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2'}\n",
      "Source Page: 2\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "L. 1111-3  ORDONNANCE n°2015-1578 du 3 décembre 2015 - art. 1      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
      "1° Les apprentis ;\n",
      "2° Les titulaires d'un contrat initiative-emploi, pendant la durée d'attribution de l'aide financière mentionnée\n",
      "à l'article L. 5134-72 ;\n",
      "3° (Abrogé) ;\n",
      "4° Le...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
      "Source Page: 3\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Page content (début): Titre II : Droits et libertés dans l'entreprise\n",
      "Chapitre unique.\n",
      "L. 1121-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Nul ne peut apporter aux droits des personnes et aux libertés individuelles et collectives de restrictions qui ne\n",
      "seraient pas justifiées par la nature de la tâche à accomplir ni proportionnées au but recherché.\n",
      "Récemment au Bulletin de la Cour de Cassation\n",
      "> Chambre sociale, 25 Septembre 2024, n°23-11.86...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3'}\n",
      "Source Page: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
    "else:\n",
    "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "\n",
    "    # 1. On charge les documents page par page\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs_from_loader = loader.load() # Chaque élément est un Document (page) avec ses métadonnées\n",
    "\n",
    "    if not docs_from_loader:\n",
    "        print(\"Aucun document n'a pu être chargé par PyPDFLoader. Veuillez vérifier le PDF.\")\n",
    "        documents = [] # L'on s'assure que 'documents' est défini même si le chargement échoue\n",
    "    else:\n",
    "        print(f\"Nombre de pages chargées par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "\n",
    "        # 2. Initialiser notre splitter basé sur les titres\n",
    "        splitter = TitleBasedSplitter()\n",
    "\n",
    "        # 3. Appliquer le splitter sur chaque document (page)\n",
    "        # La méthode split_documents prend une liste de Document et retourne une liste de Document.\n",
    "        documents = splitter.split_documents(docs_from_loader)\n",
    "\n",
    "        print(f\"Nombre de documents (chunks) après TitleBasedSplitter : {len(documents)}\")\n",
    "\n",
    "        # Affichage de quelques exemples de chunks pour vérifier leur contenu et leurs métadonnées\n",
    "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
    "            print(f\"\\n--- Chunk {i+1} ---\")\n",
    "            print(f\"Page content (début): {doc_chunk.page_content[:500]}...\") # Affiche les 500 premiers caractères\n",
    "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
    "\n",
    "            # Vérifier si 'page' est dans les métadonnées\n",
    "            if 'page' in doc_chunk.metadata:\n",
    "                print(f\"Source Page: {doc_chunk.metadata['page'] + 1}\") # +1 car les pages sont souvent 0-indexées\n",
    "\n",
    "# NB : Il est important de s'assurer qu'il n'y a pas d'erreur dans 'documents' si aucun document n'est chargé.\n",
    "if not documents:\n",
    "    print(\"Le traitement ne peut pas continuer car aucun document n'a été créé.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBsuWQ22v-AY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class CodeDuTravailStructureExtractor(TextSplitter):\n",
    "    \"\"\"\n",
    "    Splitter et extracteur de structure pour le Code du Travail.\n",
    "    Découpe le document en chunks et enrichit les métadonnées\n",
    "    avec les informations de Partie, Livre, Titre, Chapitre, Section et Article.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Pattern pour les titres principaux (Titre Ier :, Titre Préliminaire :)\n",
    "        title_pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les chapitres (Chapitre Ier :, Chapitre Unique :)\n",
    "        chapter_pattern: str = r\"(Chapitre\\s+(?:[IVXLCDM]+(?:er|ème)?|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les sections (Section 1 :, Section unique :)\n",
    "        section_pattern: str = r\"(Section\\s+(?:\\d+|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "        # Pattern pour les articles (L. 123-1, R. 456-7, D. 789-10)\n",
    "        article_pattern: str = r\"(Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)[\\s\\S]*?(?=Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)|Titre\\s+|Chapitre\\s+|Section\\s+|$))\",\n",
    "        # Capture le numéro de l'article dans un groupe pour extraction facile\n",
    "        article_num_capture_group: int = 2, # Le groupe qui contient \"L. 123-1\"\n",
    "        keep_separator: bool = True, # Indique si le séparateur (titre/chapitre/article) doit faire partie du chunk\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "        self.title_pattern = re.compile(title_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.chapter_pattern = re.compile(chapter_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.section_pattern = re.compile(section_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.article_pattern = re.compile(article_pattern, re.IGNORECASE | re.DOTALL)\n",
    "        self.article_num_capture_group = article_num_capture_group\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Divise les documents (pages) en chunks et enrichit les métadonnées.\n",
    "        \"\"\"\n",
    "        all_chunks: List[Document] = []\n",
    "        current_book = None\n",
    "        current_part = None\n",
    "        current_title = None\n",
    "        current_chapter = None\n",
    "        current_section = None\n",
    "\n",
    "        # Pour le Code du Travail, la \"Partie législative\" et \"Partie réglementaire\"\n",
    "        # sont souvent des sections de haut niveau. On pourrait les détecter aussi.\n",
    "        # Par exemple: r\"(Partie\\s+(?:législative|réglementaire)\\s*:\\s*.+?)(?=\\n(?:Partie\\s|Livre\\s|Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\"\n",
    "\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            page_metadata = doc.metadata.copy() # Copier les métadonnées de la page (ex: page number)\n",
    "\n",
    "            # Il est crucial de détecter les éléments hiérarchiques dans l'ordre décroissant\n",
    "            # (Partie -> Livre -> Titre -> Chapitre -> Section -> Article)\n",
    "            # pour s'assurer que les métadonnées sont correctement propagées.\n",
    "\n",
    "            # Nous allons traiter le texte de la page pour identifier les blocs.\n",
    "            # L'idée est de trouver tous les matchs de titres, chapitres, sections, articles\n",
    "            # et de les traiter séquentiellement.\n",
    "\n",
    "            # Simple approche pour commencer: On va splitter principalement par article\n",
    "            # et ensuite extraire les informations de titre/chapitre/section\n",
    "            # qui précèdent cet article ou sont détectées dans le chunk de l'article.\n",
    "\n",
    "            # Cette méthode nécessite une logique plus complexe pour propager le contexte\n",
    "            # (quel titre/chapitre actuel s'applique à un article).\n",
    "            # On va utiliser une approche par \"contexte courant\" qui est mise à jour à chaque détection.\n",
    "\n",
    "            # Initialisation du contexte pour cette page (on garde le dernier contexte connu)\n",
    "            # Pour la première page, ces valeurs seront None.\n",
    "            # Pour les pages suivantes, elles hériteront des valeurs de la fin de la page précédente.\n",
    "\n",
    "\n",
    "            # Pour l'instant, on suppose que les titres sont répétés ou que les chunks sont suffisamment petits\n",
    "            # pour qu'un article soit dans son contexte de titre/chapitre.\n",
    "            # C'est un point clé à affiner si les sources ne sont pas précises.\n",
    "\n",
    "            # Découpage par Article principalement\n",
    "            article_matches = list(self.article_pattern.finditer(text))\n",
    "\n",
    "            last_idx = 0\n",
    "            # Si du texte précède le premier article\n",
    "            if article_matches and article_matches[0].start() > 0:\n",
    "                pre_article_text = text[0:article_matches[0].start()].strip()\n",
    "                if pre_article_text:\n",
    "                    # Tentative d'extraction des infos de titre/chapitre/section pour ce bloc introductif\n",
    "                    # C'est une simplification, idéalement on détecterait ces éléments en amont du split.\n",
    "                    chunk_metadata = self._extract_hierarchy_metadata(pre_article_text, page_metadata)\n",
    "                    all_chunks.append(Document(page_content=pre_article_text, metadata=chunk_metadata))\n",
    "                last_idx = article_matches[0].start()\n",
    "\n",
    "            for i, match in enumerate(article_matches):\n",
    "                article_content = match.group(0).strip() # Le contenu complet de l'article (incluant le numéro)\n",
    "                article_number = match.group(self.article_num_capture_group) # Le numéro de l'article\n",
    "\n",
    "                # Ici, nous pourrions affiner l'extraction de la date ou de la loi spécifique\n",
    "                # si un pattern est détectable dans l'article_content\n",
    "                # Par exemple: LOI n° 2014-288 du 5 mars 2014-art. 29 (V)\n",
    "                # pattern_date_loi = r\"(LOI n°\\s*\\d{4}-\\d+\\s*du\\s+\\d{1,2}\\s+\\w+\\s+\\d{4})\"\n",
    "                # date_loi_match = re.search(pattern_date_loi, article_content)\n",
    "                # if date_loi_match:\n",
    "                #     article_metadata[\"date_loi\"] = date_loi_match.group(1)\n",
    "\n",
    "                chunk_metadata = page_metadata.copy()\n",
    "                chunk_metadata[\"type\"] = \"Article\"\n",
    "                chunk_metadata[\"article_number\"] = article_number\n",
    "\n",
    "                # Tenter d'extraire le contexte hiérarchique pour cet article\n",
    "                # On recherche les titres, chapitres, sections qui précèdent immédiatement cet article\n",
    "                # dans la portion de texte traitée pour cette page.\n",
    "\n",
    "                # C'est une simplification pour l'exemple.\n",
    "                # Une vraie solution robuste impliquerait de parser le document séquentiellement\n",
    "                # et de maintenir un état des \"titres actifs\" et \"chapitres actifs\"\n",
    "                # au fur et à mesure que l'on lit le document.\n",
    "\n",
    "                # Pour l'instant, on fait une extraction locale:\n",
    "                chunk_metadata.update(self._extract_hierarchy_metadata(article_content, page_metadata))\n",
    "\n",
    "                all_chunks.append(Document(page_content=article_content, metadata=chunk_metadata))\n",
    "                last_idx = match.end()\n",
    "\n",
    "            # Si du texte reste après le dernier article sur la page\n",
    "            if last_idx < len(text):\n",
    "                remaining_text = text[last_idx:].strip()\n",
    "                if remaining_text:\n",
    "                    chunk_metadata = self._extract_hierarchy_metadata(remaining_text, page_metadata)\n",
    "                    all_chunks.append(Document(page_content=remaining_text, metadata=chunk_metadata))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "    def _extract_hierarchy_metadata(self, text: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tente d'extraire les informations de hiérarchie (Titre, Chapitre, Section)\n",
    "        à partir d'un morceau de texte et les ajoute aux métadonnées.\n",
    "        \"\"\"\n",
    "        metadata = base_metadata.copy()\n",
    "\n",
    "        # Chercher le titre le plus proche\n",
    "        title_match = self.title_pattern.search(text)\n",
    "        if title_match:\n",
    "            metadata[\"title\"] = title_match.group(1).strip()\n",
    "\n",
    "        # Chercher le chapitre le plus proche\n",
    "        chapter_match = self.chapter_pattern.search(text)\n",
    "        if chapter_match:\n",
    "            metadata[\"chapter\"] = chapter_match.group(1).strip()\n",
    "\n",
    "        # Chercher la section la plus proche\n",
    "        section_match = self.section_pattern.search(text)\n",
    "        if section_match:\n",
    "            metadata[\"section\"] = section_match.group(1).strip()\n",
    "\n",
    "\n",
    "        # Pour les livres et parties, c'est plus complexe car ils sont souvent en début de document\n",
    "        # ou indiqués par des en-têtes/pieds de page non extraits par `page_content`.\n",
    "        # Pour le \"Livre\" et la \"Partie\", il est souvent plus efficace de les extraire\n",
    "        # soit manuellement au début, soit en faisant une passe initiale sur le document complet\n",
    "        # pour établir la hiérarchie.\n",
    "        # Pour l'instant, je ne les inclus pas dans _extract_hierarchy_metadata,\n",
    "        # car un chunk d'article ne les contiendra pas nécessairement.\n",
    "        # On pourrait les ajouter comme des métadonnées globales au document si elles sont fixes,\n",
    "        # ou les propager depuis un parseur de document entier.\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    # La méthode split_text n'est pas utilisée directement par split_documents,\n",
    "    # mais on la garde pour la conformité avec la classe mère, même si elle n'est pas optimisée\n",
    "    # pour notre usage actuel d'extraction de métadonnées.\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        # Cette implémentation est plus simple, mais ne gère pas la propagation des métadonnées\n",
    "        # C'est pourquoi nous utiliserons principalement split_documents.\n",
    "        chunks = []\n",
    "        # On va splitter par les articles pour avoir des chunks plus petits\n",
    "        article_splits = self.article_pattern.split(text)\n",
    "\n",
    "        # Si le premier élément n'est pas un séparateur, c'est un préambule\n",
    "        if len(article_splits) > 0 and not self.article_pattern.match(article_splits[0]):\n",
    "            if article_splits[0].strip():\n",
    "                chunks.append(article_splits[0].strip())\n",
    "            start_index = 1\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "        # Réassembler les articles avec leur numéro\n",
    "        for i in range(start_index, len(article_splits), self.article_num_capture_group + 1):\n",
    "            if i + self.article_num_capture_group < len(article_splits):\n",
    "                article_num = article_splits[i + self.article_num_capture_group -1] # Le numéro de l'article capturé\n",
    "                article_content = article_splits[i + self.article_num_capture_group] # Le contenu après le numéro\n",
    "\n",
    "                full_article_chunk = f\"Article {article_num.strip()} {article_content.strip()}\"\n",
    "                if full_article_chunk.strip():\n",
    "                    chunks.append(full_article_chunk.strip())\n",
    "            elif article_splits[i].strip():\n",
    "                chunks.append(article_splits[i].strip())\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13082,
     "status": "ok",
     "timestamp": 1749897525777,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "SGKhQZx6wLzW",
    "outputId": "7be3fe04-f7f3-49d3-e327-325869e29a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du PDF depuis : /content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf\n",
      "Nombre de pages chargées par PyPDFLoader : 278\n",
      "Nombre de documents (chunks enrichis) après extraction de structure : 411\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Page content (début): Partie législative - Chapitre préliminaire : Dialogue social. \n",
      "Partie législative\n",
      "Chapitre préliminaire : Dialogue social.\n",
      "L. 1  LOI n°2008-67 du 21 janvier 2008 - art. 3      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Tout projet de réforme envisagé par le Gouvernement qui porte sur les relations individuelles et collectives\n",
      "du travail, l'emploi et la formation professionnelle et qui relève du champ de la négociation nationale et\n",
      "interprofessionnelle fait l'objet d'une conc...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1', 'chapter': 'Chapitre préliminaire : Dialogue social. \\nPartie législative'}\n",
      "  Source Page: 1\n",
      "  Chapitre: Chapitre préliminaire : Dialogue social. \n",
      "Partie législative\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "Première partie : Les relations individuelles de travail\n",
      "Livre Ier : Dispositions préliminaires\n",
      "Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "Chapitre unique.\n",
      "L. 1111-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du présent livre sont applicables aux employeurs de droit...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 1, 'page_label': '2', 'title': \"Titre Ier : Champ d'application et calcul des seuils d'effectifs\"}\n",
      "  Source Page: 2\n",
      "  Titre: Titre Ier : Champ d'application et calcul des seuils d'effectifs\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "L. 1111-3  ORDONNANCE n°2015-1578 du 3 décembre 2015 - art. 1      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Ne sont pas pris en compte dans le calcul des effectifs de l'entreprise :\n",
      "1° Les apprentis ;\n",
      "2° Les titulaires d'un contrat initiative-emploi, pendant la durée d'attribution de l'aide financière mentionnée\n",
      "à l'article L. 5134-72 ;\n",
      "3° (Abrogé) ;\n",
      "4° Le...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 2, 'page_label': '3', 'title': \"Titre II : Droits et libertés dans l'entreprise\"}\n",
      "  Source Page: 3\n",
      "  Titre: Titre II : Droits et libertés dans l'entreprise\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "de qualification, de classification, de promotion professionnelle, d'horaires de travail, d'évaluation de la\n",
      "performance, de mutation ou de renouvellement de contrat, ni de toute autre mesure mentionnée au II de\n",
      "l'article 10-1 de la loi n° 2016-1691 du 9 décembre 2016 relative à la transparence, à la lutte contre la corruption\n",
      "et à la modernisation de la vie économique, pour avo...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 3, 'page_label': '4', 'title': 'Titre III : Discriminations', 'chapter': \"Chapitre Ier : Champ d'application.\\nL. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nLes dispositions du présent titre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\\nElles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\\nprivé.\\nL. 1131-2  LOI n°2017-86 du 27 janvier 2017 - art. 214      \\n  Legif.   \\n  Plan   \\n  Jp.Judi.   \\n  Jp.Admin.   \\n  Juricaf  \\nDans toute entreprise employant au moins trois cents salariés et dans toute entreprise spécialisée dans le\\nrecrutement, les employés chargés des missions de recrutement reçoivent une formation à la non-discrimination\\nà l'embauche au moins une fois tous les cinq ans.\"}\n",
      "  Source Page: 4\n",
      "  Titre: Titre III : Discriminations\n",
      "  Chapitre: Chapitre Ier : Champ d'application.\n",
      "L. 1131-1  Ordonnance 2007-329 2007-03-12 JORF 13 mars 2007      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Les dispositions du présent titre sont applicables aux employeurs de droit privé ainsi qu'à leurs salariés.\n",
      "Elles sont également applicables au personnel des personnes publiques employé dans les conditions du droit\n",
      "privé.\n",
      "L. 1131-2  LOI n°2017-86 du 27 janvier 2017 - art. 214      \n",
      "  Legif.   \n",
      "  Plan   \n",
      "  Jp.Judi.   \n",
      "  Jp.Admin.   \n",
      "  Juricaf  \n",
      "Dans toute entreprise employant au moins trois cents salariés et dans toute entreprise spécialisée dans le\n",
      "recrutement, les employés chargés des missions de recrutement reçoivent une formation à la non-discrimination\n",
      "à l'embauche au moins une fois tous les cinq ans.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Page content (début): Partie législative - Première partie : Les relations individuelles de travail - Livre Ier : Dispositions préliminaires\n",
      "Récemment au Bulletin de la Cour de Cassation\n",
      "> Chambre sociale, 24 Avril 2024, n°22-20.539, (B)\n",
      "> Chambre sociale, 20 Décembre 2023, n°22-12.381, (B)\n",
      "> Chambre sociale, 20 Septembre 2023, n°22-12.293, (B)\n",
      "> Chambre sociale, 28 Juin 2023, n°22-11.699, (B)\n",
      "> Chambre sociale, 01 Juin 2023, n°21-21.191, (B)\n",
      "service-public.fr\n",
      "> Droit de grève d'un salarié du secteur privé : Interdic...\n",
      "Metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 4, 'page_label': '5'}\n",
      "  Source Page: 5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
    "    documents = [] # On s'assure que 'documents' est défini même si le fichier est manquant\n",
    "else:\n",
    "    print(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    # Chargement page par page pour conserver les métadonnées de page\n",
    "    docs_from_loader = loader.load()\n",
    "\n",
    "    if not docs_from_loader:\n",
    "        print(\"Aucun document n'a pu être chargé par PyPDFLoader. Veuillez vérifier le PDF.\")\n",
    "        documents = []\n",
    "    else:\n",
    "        print(f\"Nombre de pages chargées par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "\n",
    "        # Initialisation de l'extracteur de structure\n",
    "        # On va créer des patterns plus robustes si le test initial ne donne pas satisfaction\n",
    "        # Les patterns par défaut dans la classe sont un bon point de départ.\n",
    "        structure_extractor = CodeDuTravailStructureExtractor()\n",
    "\n",
    "        # Appliquer l'extracteur sur les documents (pages)\n",
    "        documents = structure_extractor.split_documents(docs_from_loader)\n",
    "\n",
    "        print(f\"Nombre de documents (chunks enrichis) après extraction de structure : {len(documents)}\")\n",
    "\n",
    "        # Afficher quelques exemples de chunks enrichis\n",
    "        for i, doc_chunk in enumerate(documents[:5]): # Affiche les 5 premiers chunks\n",
    "            print(f\"\\n--- Chunk {i+1} ---\")\n",
    "            print(f\"Page content (début): {doc_chunk.page_content[:500]}...\")\n",
    "            print(f\"Metadata: {doc_chunk.metadata}\")\n",
    "            # Exemple de vérification des métadonnées spécifiques\n",
    "            if 'page' in doc_chunk.metadata:\n",
    "                print(f\"  Source Page: {doc_chunk.metadata['page'] + 1}\")\n",
    "            if 'type' in doc_chunk.metadata:\n",
    "                print(f\"  Type de chunk: {doc_chunk.metadata['type']}\")\n",
    "            if 'article_number' in doc_chunk.metadata:\n",
    "                print(f\"  Numéro d'Article: {doc_chunk.metadata['article_number']}\")\n",
    "            if 'title' in doc_chunk.metadata:\n",
    "                print(f\"  Titre: {doc_chunk.metadata['title']}\")\n",
    "            if 'chapter' in doc_chunk.metadata:\n",
    "                print(f\"  Chapitre: {doc_chunk.metadata['chapter']}\")\n",
    "\n",
    "# NB : S'assurer que 'documents' est défini pour les étapes suivantes même en cas d'échec de chargement\n",
    "if not documents:\n",
    "    print(\"Le traitement ne peut pas continuer car aucun document structuré n'a été créé.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mbxTrjnipO"
   },
   "source": [
    "## PARTIES EMBEDDING, STOCKAGE VECTORIELLE, CHAINE RAG, LLM\n",
    "\n",
    "### Pourquoi découper le document (Chunking) ?\n",
    "\n",
    "- **Limitation des LLM :** Les grands modèles de langage ont une \"fenêtre de contexte\" limitée. C'est la quantité de texte qu'ils peuvent lire et traiter en une seule fois. Un PDF de 2755 pages est bien au-delà de ce qu'ils peuvent gérer.\n",
    "- **Pertinence de la recherche :** Si l'on cherche un concept dans 2775 pages, la recherche sera très lente et peu précise.\n",
    "*  _Solution :_\n",
    "  - **Le découpage (Chunking)**\n",
    "     - Nous allons diviser le PDF en petits morceaux (des paragraphes, quelques phrases, etc.). Chaque morceau est suffisamment petit pour être traité par un LLM et suffisamment grand pour conserver du sens.\n",
    "*  _Conséquence :_ Quand un utilisateur posera une question, les morceaux les plus pertinenets seront chercher et non l'intégralité du document.\n",
    "\n",
    "\n",
    "### Qu'est-ce qu'un Embedding et pourquoi en a-t-on besoin ?\n",
    "Imaginons que chaque mot, phrase ou morceau de texte peut être représenté par un point dans un espace multidimensionnel (un peu comme des coordonnées X, Y, Z, mais avec beaucoup plus de dimensions).\n",
    "\n",
    "- **Transformation en nombres :** Un \"embedding\" est une suite de nombres (un vecteur) qui représente la signification sémantique d'un texte. Des textes qui ont un sens similaire seront représentés par des vecteurs \"proches\" dans cet espace.\n",
    "- **Recherche de similarité :** La question de l'utilisateur sera transformée en embedding. Ensuite, une recherche des embeddings de morceaux de texte qui sont les plus \"proches\" (on pourra notamment faire usage de l'algorithme des K-plus proches voisins pour récupérer les informations les plus pertinentes) de l'embedding da la question se fera dans la base de données. C'est la base de la recherche sémantique.\n",
    "\n",
    "* _Pourquoi ?_\n",
    "  - Un ordinateur ne \"comprend\" pas le texte. Il comprend les nombres. Les embeddings sont le pont entre le langage humain et la capacité de l'ordinateur à trouver des similarités de sens.\n",
    "\n",
    "### Le rôle de la base de données vectorielle (ChromaDB)\n",
    "Une fois que nous avons nos morceaux de texte et leurs embeddings, où les stockons-nous de manière efficace pour pouvoir les rechercher rapidement ?\n",
    "\n",
    "- **Base de données traditionnelle vs. Base de données vectorielle :** Une base de données classique (comme SQL) est optimisée pour des recherches exactes (ex: \"trouve tous les utilisateurs dont le nom est 'Dupont'\"). Une base de données vectorielle est optimisée pour des recherches de similarité (ex: \"trouve tous les textes qui sont similaires à 'licenciement abusif'\").\n",
    "\n",
    "**ChromaDB :** C'est une base de données vectorielle légère et facile à mettre en place. Elle va stocker :\n",
    "- Les morceaux de texte bruts (le contenu original).\n",
    "- Les embeddings correspondants (les vecteurs numériques).\n",
    "- Des métadonnées (comme le numéro de page d'où vient le morceau, ce qui est crucial pour les sources !).\n",
    "_Fonctionnement :_ Quand l'utilisateur pose une question, Chroma va rapidement identifier les embeddings les plus similaires à sa question, et lui retourner les morceaux de texte correspondants.\n",
    "\n",
    "\n",
    "### LangChain : L'orchestrateur de notre application\n",
    "LangChain est un framework puissant qui simplifie énormément la construction d'applications basées sur les grands modèles de langage.\n",
    "\n",
    "_Concept de \"Chains\" (Chaînes) :_ LangChain permet de relier différents composants (comme le chargement de documents, les découpeurs, les modèles d'embeddings, les bases de données vectorielles, et les LLM) dans une séquence logique, une \"chaîne\".\n",
    "\n",
    "- **Modularité :** Chaque partie de l'application RAG (chargement, découpage, embedding, recherche, génération) est un \"maillon\" de la chaîne. LangChain permet de les assembler facilement.\n",
    "- **Simplification :** Au lieu de gérer manuellement chaque interaction entre ces composants, LangChain fournit des abstractions qui rendent le développement plus rapide et plus propre.\n",
    "\n",
    "\n",
    "### Gemini : Le cerveau qui génère la réponse\n",
    "Gemini est le modèle de langage (LLM) de Google que nous allons utiliser.\n",
    "\n",
    "_Son rôle :_ Une fois que Chroma a récupéré les morceaux de texte les plus pertinents du document/PDF, Gemini va recevoir :\n",
    "- La question.\n",
    "- Les morceaux de texte pertinents.\n",
    "\n",
    "_Sa tâche :_ Il va analyser ces informations et générer une réponse cohérente, synthétisée, et basée sur le contexte fourni. C'est là que la \"Génération\" de RAG entre en jeu.\n",
    "- **API :** Nous interagirons avec Gemini via son API (Interface de Programmation d'Application), ce qui signifie que nous enverrons des requêtes à un service Google pour obtenir des réponses.\n",
    "\n",
    "### Streamlit : L'interface utilisateur simple et rapide\n",
    "Pour interagir avec notre application, nous avons besoin d'une interface.\n",
    "\n",
    "####Qu'est-ce que c'est ?\n",
    "**Streamlit** est une bibliothèque Python qui permet de créer très facilement des applications web interactives pour la science des données et le machine learning.\n",
    "*  _Avantages :_ Pas besoin d'être un expert en développement web (HTML, CSS, JavaScript). Avec quelques lignes de Python, tu peux créer des widgets (champs de texte, boutons) et afficher des résultats.\n",
    "\n",
    "_Notre utilisation :_ Nous l'utiliserons pour créer une boîte de texte où l'utilisateur pourras taper ses questions, un bouton pour les soumettre, et un espace pour afficher la réponse de Gemini et les sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbDKApJPHzQ_"
   },
   "outputs": [],
   "source": [
    "!pip install -q rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7549,
     "status": "ok",
     "timestamp": 1749898688059,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "QUHCkR0exto3",
    "outputId": "1139f342-01fa-46d6-ac78-6e8a38d300b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Étape 4 : Création des Embeddings ---\n",
      "Initialisation du modèle d'embedding : all-MiniLM-L6-v2\n",
      "Modèle d'embedding initialisé avec succès.\n",
      "\n",
      "--- Étape 5 : Création ou chargement du Vectorstore Chroma ---\n",
      "Chargement du vectorstore Chroma existant depuis : ./chroma_db_codetravail\n",
      "Vectorstore Chroma chargé.\n",
      "\n",
      "--- Étape 5.5 : Configuration des Retrievers Hybrides ---\n",
      "Retrievers hybrides (BM25 et Sémantique) configurés avec succès.\n",
      "La recherche utilisera la fusion des rangs.\n",
      "\n",
      "--- Étape 6 : Configuration du LLM (Gemini) ---\n",
      "Chargement du prompt RAG depuis Langchain Hub...\n",
      "Prompt chargé.\n",
      "Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\n",
      "LLM Gemini-Pro initialisé avec succès.\n",
      "\n",
      "--- Étape 7 : Construction de la Chaîne RAG ---\n",
      "\n",
      "--- Étape 6.5 : Chaîne de Transformation de Requête Multi-Requêtes ---\n",
      "Chaîne de génération de requêtes multiples initialisée.\n",
      "\n",
      "--- Étape 7 : Construction de la Chaîne RAG ---\n",
      "Chaîne RAG construite avec succès : Multi-Query Retrieval, Self-Querying, et fusion des rags.\n",
      "\n",
      "--- Étape 8 : Tester la Chaîne RAG ---\n",
      "\n",
      "Question : Salut\n",
      "\n",
      "Réponse Générée par Gemini :\n",
      "I am sorry, but I cannot answer in French. The provided documents do not contain information about the definition of \"Salut\". Therefore, I am unable to provide a relevant answer to your question.\n",
      "\n",
      "Sources (extraits et métadonnées) :\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "# S'assurer que 'documents' n'est pas vide avant de continuer\n",
    "if not documents:\n",
    "    print(\"ATTENTION : La liste 'documents' est vide. Le processus RAG ne peut pas continuer.\")\n",
    "    exit(\"Processus arrêté car aucun document n'a été traité.\")\n",
    "\n",
    "print(\"\\n--- Étape 4 : Création des Embeddings ---\")\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "print(f\"Initialisation du modèle d'embedding : {embedding_model_name}\")\n",
    "\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    print(\"Modèle d'embedding initialisé avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'initialisation du modèle d'embedding : {e}\")\n",
    "    print(\"Veuillez vérifier votre connexion internet et les dépendances 'sentence-transformers'.\")\n",
    "    exit(\"Processus arrêté car le modèle d'embedding n'a pas pu être initialisé.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Étape 5 : Création ou chargement du Vectorstore Chroma ---\")\n",
    "chroma_db_dir = \"./chroma_db_codetravail\"\n",
    "\n",
    "if os.path.exists(chroma_db_dir) and os.listdir(chroma_db_dir):\n",
    "    print(f\"Chargement du vectorstore Chroma existant depuis : {chroma_db_dir}\")\n",
    "    try:\n",
    "        vectorstore = Chroma(persist_directory=chroma_db_dir, embedding_function=embedding_model)\n",
    "        print(\"Vectorstore Chroma chargé.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors du chargement du vectorstore Chroma : {e}\")\n",
    "        print(\"Tentative de recréation du vectorstore...\")\n",
    "        vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "        print(\"Vectorstore Chroma recréé.\")\n",
    "else:\n",
    "    print(f\"Création d'un nouveau vectorstore Chroma dans : {chroma_db_dir}\")\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "    print(\"Vectorstore Chroma créé.\")\n",
    "\n",
    "\n",
    "# --- Configuration des Retrievers Hybrides ---\n",
    "print(\"\\n--- Étape 5.5 : Configuration des Retrievers Hybrides ---\")\n",
    "\n",
    "# 1. Retriever sémantique (Chroma Retriever)\n",
    "semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. Retriever basé sur les mots-clés (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5 # Nombre de documents à récupérer pour BM25\n",
    "\n",
    "# 3. Combinaison des retrievers avec EnsembleRetriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, semantic_retriever],\n",
    "    weights=[0.5, 0.5] # Poids égaux pour commencer\n",
    ")\n",
    "print(\"Retrievers hybrides (BM25 et Sémantique) configurés avec succès.\")\n",
    "print(\"La recherche utilisera la fusion des rangs.\")\n",
    "\n",
    "# LE RETRIEVER PRINCIPAL UTILISÉ DANS LA CHAÎNE EST MAINTENANT LE HYBRIDE\n",
    "retriever = hybrid_retriever\n",
    "\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "# print(f\"Retriever initialisé pour récupérer les {retriever.search_kwargs['k']} documents les plus pertinents.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Étape 6 : Configuration du LLM (Gemini) ---\")\n",
    "print(\"Chargement du prompt RAG depuis Langchain Hub...\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"Prompt chargé.\")\n",
    "\n",
    "print(\"Initialisation du LLM ChatGoogleGenerativeAI (Gemini-Pro)...\")\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.2)\n",
    "    print(\"LLM Gemini-Pro initialisé avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'initialisation du LLM ChatGoogleGenerativeAI : {e}\")\n",
    "    print(\"Assurez-vous que GOOGLE_API_KEY est correctement définie dans les secrets de Colab.\")\n",
    "    print(\"Vérifiez aussi que l'API Generative Language est activée pour votre projet Google Cloud.\")\n",
    "    exit(\"Processus arrêté car le LLM n'a pas pu être initialisé.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Étape 7 : Construction de la Chaîne RAG ---\")\n",
    "\n",
    "# # --- NOUVEAU : Chaîne de Réécriture de Requête ---\n",
    "# print(\"\\n--- Étape 6.5 : Chaîne de Réécriture de Requête ---\")\n",
    "# query_rewriter_template = \"\"\"You are an expert at rephrasing questions to be optimal for retrieving relevant documents.\n",
    "# Given the following user question, rephrase it to be a standalone search query.\n",
    "# If the question is already a good search query, return it as is.\n",
    "\n",
    "# User question: {question}\n",
    "# Rephrased query:\"\"\"\n",
    "\n",
    "# query_rewriter_prompt = ChatPromptTemplate.from_template(query_rewriter_template)\n",
    "# query_rewriting_chain = query_rewriter_prompt | llm | StrOutputParser()\n",
    "# print(\"Chaîne de réécriture de la question initialisée.\")\n",
    "\n",
    "# --- : Chaîne de Génération de Requêtes Multiples --- MULTI_QUERY, RAG FUSION\n",
    "print(\"\\n--- Étape 6.5 : Chaîne de Transformation de Requête Multi-Requêtes ---\")\n",
    "multi_query_template = \"\"\"You are an AI assistant that generates multiple search queries based on a single input question.\n",
    "Your goal is to create diverse queries that cover various aspects of the user's original question,\n",
    "to improve the chances of retrieving relevant documents.\n",
    "Generate 3 distinct search queries related to the user's question, separated by newlines.\n",
    "Do not number the queries.\n",
    "\n",
    "Original question: {question}\n",
    "Search queries:\"\"\"\n",
    "\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(multi_query_template)\n",
    "multi_query_chain = multi_query_prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "print(\"Chaîne de génération de requêtes multiples initialisée.\")\n",
    "\n",
    "# --- DÉFINITION FINALE DE LA CHAÎNE RAG AVEC MULTI-QUERY ET SELF-QUERYING ---\n",
    "print(\"\\n--- Étape 7 : Construction de la Chaîne RAG ---\")\n",
    "\n",
    "# Fonction de formatage des documents\n",
    "def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "    formatted_content = \"\"\n",
    "    unique_pages = set()\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted_content += f\"Contenu source {i+1}:\\n{doc.page_content}\\n\\n\"\n",
    "        if 'page' in doc.metadata:\n",
    "            page_number = doc.metadata['page'] + 1\n",
    "            unique_pages.add(str(page_number))\n",
    "\n",
    "    if unique_pages:\n",
    "        formatted_content += f\"Sources des pages: {', '.join(sorted(list(unique_pages)))}\\n\"\n",
    "\n",
    "    return formatted_content\n",
    "\n",
    "# # --- NOUVELLE DÉFINITION DE LA CHAÎNE RAG AVEC RÉÉCRITURE ET RECHERCHE HYBRIDE ---\n",
    "# # Prépare les inputs : la question originale et la question réécrite pour le retriever\n",
    "# prepare_inputs = {\n",
    "#     \"question_originale\": RunnablePassthrough(),\n",
    "#     \"question_pour_retrieval\": query_rewriting_chain,\n",
    "# }\n",
    "\n",
    "# # La chaîne de récupération utilise la question réécrite et passe le contexte + la question originale\n",
    "# retrieval_chain_with_rewriting = {\n",
    "#     \"context\": (lambda x: x[\"question_pour_retrieval\"]) | retriever, # Le retriever utilise la question réécrite\n",
    "#     \"question\": (lambda x: x[\"question_originale\"]) # Le LLM utilise la question originale\n",
    "# }\n",
    "\n",
    "# # La chaîne de génération de réponse\n",
    "# generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# # La chaîne RAG finale\n",
    "# # L'entrée de rag_chain_with_sources est la question originale.\n",
    "# # prepare_inputs prend cette question et la transforme en dictionnaire.\n",
    "# # retrieval_chain_with_rewriting prend ce dictionnaire et utilise les clés \"question_pour_retrieval\" et \"question_originale\".\n",
    "# rag_chain_with_sources = (\n",
    "#     prepare_inputs\n",
    "#     | RunnableParallel(\n",
    "#         response=retrieval_chain_with_rewriting | generation_chain,\n",
    "#         source_documents=retrieval_chain_with_rewriting | RunnableLambda(lambda x: x[\"context\"])\n",
    "#     )\n",
    "# ).with_config(run_name=\"RAG Chain with Query Transformation and Sources\")\n",
    "\n",
    "# print(\"Chaîne RAG mise à jour avec la transformation de requête et la recherche hybride, prête à inclure les sources.\")\n",
    "\n",
    "# 1. Préparation des inputs: la question originale et la liste des questions générées\n",
    "prepare_inputs = {\n",
    "    \"question_originale\": RunnablePassthrough(), # La question initiale de l'utilisateur\n",
    "    \"questions_pour_retrieval\": multi_query_chain, # La liste des requêtes générées\n",
    "}\n",
    "\n",
    "# 2. Chaîne de récupération : pour chaque requête générée, appeler le retriever, puis aplatir et dédupliquer\n",
    "# Note: 'retriever' est maintenant l'EnsembleRetriever (BM25 + Self-Querying)\n",
    "# retrieval_and_aggregation_chain = (\n",
    "#     RunnableLambda(lambda x: x[\"questions_pour_retrieval\"]) # Prend la liste de questions\n",
    "#     | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries]) # Invoque l'EnsembleRetriever pour chaque requête\n",
    "#     | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist]) # Aplatit la liste de listes de docs\n",
    "#     | RunnableLambda(lambda docs: list(set(docs))) # Supprime les doublons de Document\n",
    "# )\n",
    "\n",
    "# Déduplication des documents\n",
    "retrieval_and_aggregation_chain = (\n",
    "    RunnableLambda(lambda x: x[\"questions_pour_retrieval\"])\n",
    "    | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries])\n",
    "    | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist])\n",
    "    # Logique pour la déduplication :\n",
    "    | RunnableLambda(\n",
    "        lambda docs: list(\n",
    "            {\n",
    "                (doc.page_content, frozenset(doc.metadata.items())): doc\n",
    "                for doc in docs\n",
    "            }.values()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Construction de la chaîne qui fournira le contexte et la question au LLM\n",
    "# Elle prend en entrée le dictionnaire de 'prepare_inputs'\n",
    "context_and_question_for_llm = {\n",
    "    \"context\": retrieval_and_aggregation_chain, # Le contexte est le résultat agrégé du retrieval\n",
    "    \"question\": RunnableLambda(lambda x: x[\"question_originale\"]) # La question pour le LLM (la question l'originale)\n",
    "}\n",
    "\n",
    "# 4. Chaîne de génération de réponse (prompt | llm | output_parser)\n",
    "generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 5. La chaîne RAG finale\n",
    "rag_chain_with_sources = (\n",
    "    prepare_inputs # L'entrée est la question de l'utilisateur\n",
    "    | RunnableParallel( # Exécute la génération de réponse et la récupération des sources en parallèle\n",
    "        response=context_and_question_for_llm | generation_chain, # La branche qui génère la réponse\n",
    "        source_documents=context_and_question_for_llm | RunnableLambda(lambda x: x[\"context\"]) # La branche qui passe les documents sources\n",
    "    )\n",
    ").with_config(run_name=\"RAG Chain with Multi-Query & Self-Query\")\n",
    "\n",
    "print(\"Chaîne RAG construite avec succès : Multi-Query Retrieval, Self-Querying, et fusion des rags.\")\n",
    "\n",
    "print(\"\\n--- Étape 8 : Tester la Chaîne RAG ---\")\n",
    "\n",
    "# question = \"Quelles sont les fonctions du code du travail? Et quelles sont les conditions de représentativité des organisations syndicales ?\"\n",
    "# Testons une question plus conversationnelle pour voir la réécriture en action\n",
    "# question_conv = \"Quelle était la réforme mentionnée au début du document ? Et les conditions de représentativité ?\"\n",
    "\n",
    "question_conv = \"Salut\"\n",
    "question = question_conv\n",
    "\n",
    "print(f\"\\nQuestion : {question}\")\n",
    "\n",
    "try:\n",
    "    result = rag_chain_with_sources.invoke(question)\n",
    "\n",
    "    print(\"\\nRéponse Générée par Gemini :\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "    # print(\"\\nSources (extraits et métadonnées) :\")\n",
    "    # print(format_docs_with_sources(result[\"source_documents\"]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors de l'invocation de la chaîne RAG : {e}\")\n",
    "    print(\"Veuillez vérifier les étapes précédentes (authentification LLM, initialisation de Chroma).\")\n",
    "    print(\"Erreur détaillée:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78_kD_ymheio"
   },
   "source": [
    "# Application streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG7cOKiRcORv"
   },
   "outputs": [],
   "source": [
    "# # Installation des dépendances nécessaires\n",
    "# !pip install -q langchain-google-genai pypdf langchain-chroma faiss-cpu sentence-transformers streamlit langchain_huggingface\n",
    "# !pip install langchain_community langchainhub chromadb langchain langchain_chroma rank_bm25\n",
    "# !pip install -q python-dotenv\n",
    "# !pip install -q transformers\n",
    "# !pip install -q --upgrade langchain\n",
    "\n",
    "# print(\"Toutes les dépendances ont été installées ou mises à jour.\")\n",
    "\n",
    "# # Installation de localtunnel (pour exposer l'application)\n",
    "# !apt-get update\n",
    "# !apt-get install -y nodejs npm\n",
    "# !npm install -g localtunnel\n",
    "\n",
    "# print(\"Localtunnel installé.\")\n",
    "\n",
    "# # Configuration des clés API via Google Colab Secrets\n",
    "# from google.colab import userdata\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
    "#     os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
    "#     os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "#     os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "#     print(\"Clés API chargées depuis les secrets Colab.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ATTENTION : Erreur lors du chargement des clés API depuis les secrets Colab : {e}\")\n",
    "#     print(\"Assurez-vous que 'GOOGLE_API_KEY' et 'LANGCHAIN_API_KEY' sont bien définies dans les secrets et activées pour ce notebook.\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"Google Drive monté.\")\n",
    "\n",
    "# # Vérification du chemin du PDF\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# if not os.path.exists(pdf_path):\n",
    "#     print(f\"ERREUR CRITIQUE : Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
    "# else:\n",
    "#     print(f\"Chemin du PDF vérifié : {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmJOTMw9OsI4"
   },
   "outputs": [],
   "source": [
    "# # %%writefile app.py\n",
    "\n",
    "# import streamlit as st\n",
    "# import os\n",
    "# import re\n",
    "# from typing import List, Optional, Dict, Any\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "# from langchain.schema import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain import hub\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.retrievers import BM25Retriever\n",
    "# from langchain.retrievers import EnsembleRetriever\n",
    "# import sys\n",
    "\n",
    "# # --- Configuration Streamlit (DOIT être la première commande Streamlit) ---\n",
    "# st.set_page_config(page_title=\"Assistant Juridique RAG (Code du Travail)\", layout=\"wide\")\n",
    "\n",
    "# # --- Chemins et configuration ---\n",
    "# pdf_path = \"/content/drive/MyDrive/Colab Notebooks/AI31/lo17_rag/data/data_50_page.pdf\"\n",
    "# # pdf_path = \"data/data_50_page.pdf\"\n",
    "# chroma_db_dir = \"./chroma_db_codetravail\"\n",
    "# embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# # --- Interface Streamlit ---\n",
    "# st.title(\"⚖️ Assistant Juridique\")\n",
    "# st.markdown(\"\"\"\n",
    "# Bienvenue dans votre outil d'aide à la décision juridique. Posez des questions sur le Code du Travail\n",
    "# et obtenez des réponses précises basées sur les documents officiels.\n",
    "# \"\"\")\n",
    "\n",
    "# st.write(\"Chaîne RAG construite avec succès : Multi-Query Retrieval, Self-Querying, et fusion des rags.\")\n",
    "\n",
    "# if \"params\" not in st.session_state:\n",
    "#     st.session_state.params = {\n",
    "#         \"temperature\": 0.2,\n",
    "#         \"top_p\": 10,\n",
    "#         \"model\": \"gemini-2.5-flash-preview-05-20\"\n",
    "#     }\n",
    "\n",
    "# # --- Classe TitleBasedSplitter ---\n",
    "# class TitleBasedSplitter(TextSplitter):\n",
    "#     def __init__(self, pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:)\"):\n",
    "#         super().__init__()\n",
    "#         self.compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "#     def split_documents(self, documents: List[Document], **kwargs) -> List[Document]:\n",
    "#         all_chunks: List[Document] = []\n",
    "#         for doc in documents:\n",
    "#             text = doc.page_content\n",
    "#             metadata = doc.metadata.copy()\n",
    "\n",
    "#             matches = list(self.compiled_pattern.finditer(text))\n",
    "\n",
    "#             if not matches:\n",
    "#                 if text.strip():\n",
    "#                     all_chunks.append(Document(page_content=text.strip(), metadata=metadata))\n",
    "#                 continue\n",
    "\n",
    "#             if matches[0].start() > 0:\n",
    "#                 pre_title_text = text[0:matches[0].start()].strip()\n",
    "#                 if pre_title_text:\n",
    "#                     all_chunks.append(Document(page_content=pre_title_text, metadata=metadata))\n",
    "\n",
    "#             for i in range(len(matches)):\n",
    "#                 start = matches[i].start()\n",
    "#                 end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "#                 chunk_content = text[start:end].strip()\n",
    "\n",
    "#                 if chunk_content:\n",
    "#                     chunk_metadata = metadata.copy()\n",
    "#                     all_chunks.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
    "#         return all_chunks\n",
    "\n",
    "#     def split_text(self, text: str) -> List[str]:\n",
    "#         chunks = []\n",
    "#         matches = list(self.compiled_pattern.finditer(text))\n",
    "#         if not matches:\n",
    "#             return [text.strip()] if text.strip() else []\n",
    "\n",
    "#         if matches[0].start() > 0:\n",
    "#             pre_title_text = text[0:matches[0].start()].strip()\n",
    "#             if pre_title_text:\n",
    "#                 chunks.append(pre_title_text)\n",
    "\n",
    "#         for i in range(len(matches)):\n",
    "#             start = matches[i].start()\n",
    "#             end = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "#             chunk_content = text[start:end].strip()\n",
    "#             if chunk_content:\n",
    "#                 chunks.append(chunk_content)\n",
    "#         return chunks\n",
    "\n",
    "# # --- Classe CodeDuTravailStructureExtractor ---\n",
    "# class CodeDuTravailStructureExtractor(TextSplitter):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         title_pattern: str = r\"(Titre\\s+(?:[IVXLCDM]+(?:er|ème)?|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         chapter_pattern: str = r\"(Chapitre\\s+(?:[IVXLCDM]+(?:er|ème)?|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         section_pattern: str = r\"(Section\\s+(?:\\d+|unique|[A-Za-z\\d\\u00C0-\\u00FF'-]+)\\s*:\\s*.+?)(?=\\n(?:Titre\\s|Chapitre\\s|Section\\s|Article\\s|$))\",\n",
    "#         article_pattern: str = r\"(Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)[\\s\\S]*?(?=Article\\s+((?:L|R|D)\\.\\s*\\d{3}-\\d+(?:-\\d+)?(?:-\\d+)?)|Titre\\s+|Chapitre\\s+|Section\\s+|$))\",\n",
    "#         article_num_capture_group: int = 2,\n",
    "#         keep_separator: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "#         self.title_pattern = re.compile(title_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.chapter_pattern = re.compile(chapter_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.section_pattern = re.compile(section_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.article_pattern = re.compile(article_pattern, re.IGNORECASE | re.DOTALL)\n",
    "#         self.article_num_capture_group = article_num_capture_group\n",
    "\n",
    "#     def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "#         all_chunks: List[Document] = []\n",
    "#         current_book = None\n",
    "#         current_part = None\n",
    "#         current_title = None\n",
    "#         current_chapter = None\n",
    "#         current_section = None\n",
    "\n",
    "#         for doc in documents:\n",
    "#             text = doc.page_content\n",
    "#             page_metadata = doc.metadata.copy()\n",
    "\n",
    "#             article_matches = list(self.article_pattern.finditer(text))\n",
    "\n",
    "#             last_idx = 0\n",
    "#             if article_matches and article_matches[0].start() > 0:\n",
    "#                 pre_article_text = text[0:article_matches[0].start()].strip()\n",
    "#                 if pre_article_text:\n",
    "#                     chunk_metadata = self._extract_hierarchy_metadata(pre_article_text, page_metadata)\n",
    "#                     all_chunks.append(Document(page_content=pre_article_text, metadata=chunk_metadata))\n",
    "#                 last_idx = article_matches[0].start()\n",
    "\n",
    "#             for i in range(len(article_matches)):\n",
    "#                 article_content = article_matches[i].group(0).strip()\n",
    "#                 article_number = article_matches[i].group(self.article_num_capture_group)\n",
    "\n",
    "#                 chunk_metadata = page_metadata.copy()\n",
    "#                 chunk_metadata[\"type\"] = \"Article\"\n",
    "#                 chunk_metadata[\"article_number\"] = article_number\n",
    "\n",
    "#                 chunk_metadata.update(self._extract_hierarchy_metadata(article_content, page_metadata))\n",
    "\n",
    "#                 all_chunks.append(Document(page_content=article_content, metadata=chunk_metadata))\n",
    "#                 last_idx = article_matches[i].end()\n",
    "\n",
    "#             if last_idx < len(text):\n",
    "#                 remaining_text = text[last_idx:].strip()\n",
    "#                 if remaining_text:\n",
    "#                     chunk_metadata = self._extract_hierarchy_metadata(remaining_text, page_metadata)\n",
    "#                     all_chunks.append(Document(page_content=remaining_text, metadata=chunk_metadata))\n",
    "#         return all_chunks\n",
    "\n",
    "\n",
    "#     def _extract_hierarchy_metadata(self, text: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#         metadata = base_metadata.copy()\n",
    "\n",
    "#         title_match = self.title_pattern.search(text)\n",
    "#         if title_match:\n",
    "#             metadata[\"title\"] = title_match.group(1).strip()\n",
    "\n",
    "#         chapter_match = self.chapter_pattern.search(text)\n",
    "#         if chapter_match:\n",
    "#             metadata[\"chapter\"] = chapter_match.group(1).strip()\n",
    "\n",
    "#         section_match = self.section_pattern.search(text)\n",
    "#         if section_match:\n",
    "#             metadata[\"section\"] = section_match.group(1).strip()\n",
    "\n",
    "#         return metadata\n",
    "\n",
    "#     def split_text(self, text: str) -> List[str]:\n",
    "#         chunks = []\n",
    "#         article_splits = self.article_pattern.split(text)\n",
    "\n",
    "#         if len(article_splits) > 0 and not self.article_pattern.match(article_splits[0]):\n",
    "#             if article_splits[0].strip():\n",
    "#                 chunks.append(article_splits[0].strip())\n",
    "#             start_index = 1\n",
    "#         else:\n",
    "#             start_index = 0\n",
    "\n",
    "#         for i in range(start_index, len(article_splits), self.article_num_capture_group + 1):\n",
    "#             if i + self.article_num_capture_group < len(article_splits):\n",
    "#                 article_num = article_splits[i + self.article_num_capture_group -1]\n",
    "#                 article_content = article_splits[i + self.article_num_capture_group]\n",
    "#                 full_article_chunk = f\"Article {article_num.strip()} {article_content.strip()}\"\n",
    "#                 if full_article_chunk.strip():\n",
    "#                     chunks.append(full_article_chunk.strip())\n",
    "#             elif article_splits[i].strip():\n",
    "#                 chunks.append(article_splits[i].strip())\n",
    "#         return chunks\n",
    "\n",
    "# # --- Chargement et Traitement du PDF ---\n",
    "# documents = []\n",
    "# if not os.path.exists(pdf_path):\n",
    "#     st.error(f\"ERREUR: Le fichier PDF '{pdf_path}' n'a pas été trouvé. Veuillez vérifier le chemin.\")\n",
    "#     st.stop() # Arrête l'exécution de l'application Streamlit\n",
    "# else:\n",
    "#     st.write(f\"Chargement du PDF depuis : {pdf_path}\")\n",
    "#     loader = PyPDFLoader(pdf_path)\n",
    "#     docs_from_loader = loader.load()\n",
    "\n",
    "#     if not docs_from_loader:\n",
    "#         st.error(\"Aucun document n'a pu être chargé par PyPDFLoader. Veuillez vérifier le PDF.\")\n",
    "#         st.stop() # Arrête l'exécution de l'application Streamlit\n",
    "#     else:\n",
    "#         st.write(f\"Nombre de pages chargées par PyPDFLoader : {len(docs_from_loader)}\")\n",
    "#         structure_extractor = CodeDuTravailStructureExtractor()\n",
    "#         documents = structure_extractor.split_documents(docs_from_loader)\n",
    "#         st.write(f\"Nombre de documents (chunks enrichis) après extraction de structure : {len(documents)}\")\n",
    "\n",
    "# if not documents:\n",
    "#     st.error(\"Le traitement ne peut pas continuer car aucun document structuré n'a été créé.\")\n",
    "#     st.stop() # Arrête l'exécution de l'application Streamlit\n",
    "\n",
    "# # --- Création des Embeddings ---\n",
    "# try:\n",
    "#     embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "# except Exception as e:\n",
    "#     st.error(f\"ERREUR lors de l'initialisation du modèle d'embedding : {e}\")\n",
    "#     st.info(\"Veuillez vérifier votre connexion internet et les dépendances 'sentence-transformers'.\")\n",
    "#     st.stop() # Arrête l'exécution de l'application Streamlit\n",
    "\n",
    "# # --- Création ou chargement du Vectorstore Chroma ---\n",
    "# try:\n",
    "#     if os.path.exists(chroma_db_dir) and os.listdir(chroma_db_dir):\n",
    "#         try:\n",
    "#             vectorstore = Chroma(persist_directory=chroma_db_dir, embedding_function=embedding_model)\n",
    "#             st.success(\"Base de données vectorielle existante chargée avec succès.\")\n",
    "#         except Exception as e:\n",
    "#             st.warning(f\"Erreur lors du chargement de la base existante : {e}. Création d'une nouvelle base...\")\n",
    "#             vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "#             st.success(\"Nouvelle base de données vectorielle créée avec succès.\")\n",
    "#     else:\n",
    "#         vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=chroma_db_dir)\n",
    "#         st.success(\"Nouvelle base de données vectorielle créée avec succès.\")\n",
    "# except Exception as e:\n",
    "#     st.error(f\"Erreur critique lors de l'initialisation de ChromaDB : {e}\")\n",
    "#     st.info(\"Veuillez vérifier que tous les packages sont correctement installés :\")\n",
    "#     st.code(\"pip install -U langchain-huggingface langchain-chroma chromadb jsonschema\")\n",
    "#     st.stop()\n",
    "\n",
    "# # --- Configuration des Retrievers Hybrides ---\n",
    "# semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": st.session_state.params['top_p']})\n",
    "# bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "# bm25_retriever.k = 5\n",
    "# hybrid_retriever = EnsembleRetriever(\n",
    "#     retrievers=[bm25_retriever, semantic_retriever],\n",
    "#     weights=[0.5, 0.5]\n",
    "# )\n",
    "# retriever = hybrid_retriever\n",
    "# st.write(\"Retrievers hybrides (BM25 et Sémantique) configurés.\")\n",
    "\n",
    "# # --- Configuration du LLM (Gemini) ---\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# try:\n",
    "#     llm = ChatGoogleGenerativeAI(\n",
    "#         model=st.session_state.params['model'],\n",
    "#         temperature=st.session_state.params['temperature'])\n",
    "# except Exception as e:\n",
    "#     st.error(f\"ERREUR lors de l'initialisation du LLM ChatGoogleGenerativeAI : {e}\")\n",
    "#     st.info(\"Assurez-vous que GOOGLE_API_KEY est correctement définie dans les secrets de Colab et que l'API Generative Language est activée.\")\n",
    "#     st.stop()\n",
    "\n",
    "# # --- Chaîne de Génération de Requêtes Multiples ---\n",
    "# multi_query_template = \"\"\"You are an AI assistant that generates multiple search queries based on a single input question. Your goal is to create diverse queries that cover various aspects of the user's original question, to improve the chances of retrieving relevant documents. Generate 3 distinct search queries related to the user's question, separated by newlines. Do not number the queries. Original question: {question} Search queries:\"\"\"\n",
    "# multi_query_prompt = ChatPromptTemplate.from_template(multi_query_template)\n",
    "# multi_query_chain = multi_query_prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "# st.write(\"Chaîne de génération de requêtes multiples initialisée.\")\n",
    "\n",
    "# # --- DÉFINITION FINALE DE LA CHAÎNE RAG AVEC MULTI-QUERY ET SELF-QUERYING ---\n",
    "# def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "#     formatted_content = \"\"\n",
    "#     unique_pages = set()\n",
    "\n",
    "#     for i, doc in enumerate(docs):\n",
    "#         formatted_content += f\"Contenu source {i+1}:\\n{doc.page_content}\\n\\n\"\n",
    "#         if 'page' in doc.metadata:\n",
    "#             page_number = doc.metadata['page'] + 1\n",
    "#             unique_pages.add(str(page_number))\n",
    "\n",
    "#     if unique_pages:\n",
    "#         formatted_content += f\"Sources des pages: {', '.join(sorted(list(unique_pages)))}\\n\"\n",
    "#     return formatted_content\n",
    "\n",
    "# prepare_inputs = {\n",
    "#     \"question_originale\": RunnablePassthrough(),\n",
    "#     \"questions_pour_retrieval\": multi_query_chain,\n",
    "# }\n",
    "\n",
    "# retrieval_and_aggregation_chain = (\n",
    "#     RunnableLambda(lambda x: x[\"questions_pour_retrieval\"])\n",
    "#     | RunnableLambda(lambda queries: [retriever.invoke(q) for q in queries])\n",
    "#     | RunnableLambda(lambda lists_of_docs: [doc for sublist in lists_of_docs for doc in sublist])\n",
    "#     | RunnableLambda(\n",
    "#         lambda docs: list(\n",
    "#             {\n",
    "#                 (doc.page_content, frozenset(doc.metadata.items())): doc\n",
    "#                 for doc in docs\n",
    "#             }.values()\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# context_and_question_for_llm = {\n",
    "#     \"context\": retrieval_and_aggregation_chain,\n",
    "#     \"question\": RunnableLambda(lambda x: x[\"question_originale\"])\n",
    "# }\n",
    "\n",
    "# generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# rag_chain_with_sources = (\n",
    "#     prepare_inputs\n",
    "#     | RunnableParallel(\n",
    "#         response=context_and_question_for_llm | generation_chain,\n",
    "#         source_documents=context_and_question_for_llm | RunnableLambda(lambda x: x[\"context\"])\n",
    "#     )\n",
    "# ).with_config(run_name=\"RAG Chain with Multi-Query & Self-Query\")\n",
    "\n",
    "\n",
    "# if \"messages\" not in st.session_state:\n",
    "#     st.session_state.messages = []\n",
    "\n",
    "# for message in st.session_state.messages:\n",
    "#     with st.chat_message(message[\"role\"]):\n",
    "#         st.markdown(message[\"content\"])\n",
    "\n",
    "# if prompt_input := st.chat_input(\"Posez votre question sur le Code du Travail...\"):\n",
    "#     st.session_state.messages.append({\"role\": \"user\", \"content\": prompt_input})\n",
    "#     with st.chat_message(\"user\"):\n",
    "#         st.markdown(prompt_input)\n",
    "\n",
    "#     with st.chat_message(\"assistant\"):\n",
    "#         with st.spinner(\"Recherche de réponse...\"):\n",
    "#             full_response = rag_chain_with_sources.invoke(prompt_input)\n",
    "#             st.markdown(full_response[\"response\"]) # Accéder à la clé 'response'\n",
    "\n",
    "#             # Afficher les sources\n",
    "#             if \"source_documents\" in full_response and full_response[\"source_documents\"]:\n",
    "#                 st.subheader(\"Sources:\")\n",
    "#                 st.markdown(format_docs_with_sources(full_response[\"source_documents\"]))\n",
    "\n",
    "#         st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response[\"response\"]}) # Stocker la réponse\n",
    "#         # Optionnel: stocker aussi les sources si tu veux les afficher à chaque re-run\n",
    "#         # st.session_state.messages.append({\"role\": \"assistant_sources\", \"content\": format_docs_with_sources(full_response[\"source_documents\"])})\n",
    "\n",
    "\n",
    "# with st.sidebar:\n",
    "#     st.header(\"Options\")\n",
    "#     st.subheader(\"paramètre du modèle\")\n",
    "\n",
    "#     model = st.selectbox(\n",
    "#         \"Modèle Gemini\",\n",
    "#         options=[\n",
    "#             \"gemini-2.5-flash-preview-05-20\",\n",
    "#             \"gemini-1.5-pro\",\n",
    "#             \"gemini-1.5-flash\"\n",
    "#         ],\n",
    "#         index=0,\n",
    "#         help=\"Choisissez le modèle Gemini à utiliser\"\n",
    "#     )\n",
    "\n",
    "#     temperature = st.slider(\n",
    "#         \"Temperature\",\n",
    "#         min_value=0.0,\n",
    "#         max_value=1.0,\n",
    "#         value=st.session_state.params['temperature'],\n",
    "#         step=0.1,\n",
    "#         help=\"Contrôle la créativité des réponses (0 = plus précis, 1 = plus créatif)\"\n",
    "#     )\n",
    "#     top_p = st.slider(\n",
    "#         \"Top_p\",\n",
    "#         min_value=1,\n",
    "#         max_value=15,\n",
    "#         value=st.session_state.params['top_p'],\n",
    "#         step=1,\n",
    "#         help=\"Nombre de documents à récupérer pour la réponse\"\n",
    "#     )\n",
    "\n",
    "#     st.session_state.params['model']=model\n",
    "#     st.session_state.params['temperature'] = temperature\n",
    "#     st.session_state.params['top_p'] = top_p\n",
    "\n",
    "#     if st.button(\"Effacer l'historique du chat\"):\n",
    "#         st.session_state.messages = []\n",
    "#         st.rerun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPnds2rEcjGD"
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import time\n",
    "# import re\n",
    "\n",
    "# print(\"Lancement de l'application Streamlit en arrière-plan...\")\n",
    "# # Lance Streamlit en arrière-plan sur le port 8501\n",
    "# streamlit_process = subprocess.Popen([\n",
    "#     \"streamlit\", \"run\", \"app.py\",\n",
    "#     \"--server.port\", \"8501\",\n",
    "#     \"--server.enableCORS\", \"false\",\n",
    "#     \"--server.enableXsrfProtection\", \"false\"\n",
    "# ])\n",
    "\n",
    "# print(\"Attente du démarrage de Streamlit (environ 5 secondes)...\")\n",
    "# time.sleep(5) # Donne à Streamlit le temps de démarrer\n",
    "\n",
    "# print(\"Création du tunnel localtunnel...\")\n",
    "# # Lance localtunnel pour exposer le port 8501\n",
    "# localtunnel_process = subprocess.Popen(['lt', '--port', '8501'], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "# public_url = None\n",
    "# for _ in range(30):\n",
    "#     line = localtunnel_process.stdout.readline()\n",
    "#     print(f\"localtunnel output: {line.strip()}\")\n",
    "#     match = re.search(r'https?://[^\\s/$.?#].[^\\s]*\\.loca\\.lt', line)\n",
    "#     if match:\n",
    "#         public_url = match.group(0)\n",
    "#         break\n",
    "#     time.sleep(1)\n",
    "\n",
    "# if public_url:\n",
    "#     print(f\"🎉 Votre application Streamlit est disponible à l'adresse : {public_url}\")\n",
    "# else:\n",
    "#     print(\"Erreur : Impossible d'obtenir l'URL de localtunnel. Vérifie la sortie ci-dessus pour les erreurs.\")\n",
    "#     print(\"Vérifie si localtunnel est bien installé (`!npm install -g localtunnel`) et si Streamlit a démarré.\")\n",
    "#     streamlit_process.terminate() # Tente de tuer le processus Streamlit si localtunnel échoue\n",
    "#     localtunnel_process.terminate() # Tente de tuer le processus localtunnel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNRD7DI3p6Og"
   },
   "outputs": [],
   "source": [
    "# if 'streamlit_process' in locals() and streamlit_process.poll() is None:\n",
    "#     streamlit_process.terminate()\n",
    "#     print(\"Processus Streamlit terminé.\")\n",
    "# if 'localtunnel_process' in locals() and localtunnel_process.poll() is None:\n",
    "#     localtunnel_process.terminate()\n",
    "#     print(\"Processus localtunnel terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGGosCV0o5Fr"
   },
   "outputs": [],
   "source": [
    "# !curl https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASo4_Gohn0L"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749897660494,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "lpTpR1tCIIbT",
    "outputId": "85839df3-71b4-47ea-9c34-f910deabe278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de données d'évaluation créé avec 2 questions.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"question\": \"\n",
    "        \"ground_truth_answer\": \"La loi n° 2008-67 du 21 janvier 2008 prévoit une concertation préalable obligatoire pour tout projet de réforme du Gouvernement concernant les relations individuelles et collectives du travail, l'emploi et la formation professionnelle qui relève de la négociation nationale et interprofessionnelle. Cette concertation s'adresse aux organisations syndicales de salariés et d'employeurs représentatives au niveau national et interprofessionnel. Le Gouvernement doit leur communiquer un document d'orientation. Cette procédure n'est pas applicable en cas d'urgence.\",\n",
    "        \"ground_truth_relevant_document_pages\": [0] # Page 1 du document\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Quels sont les sujets principaux de la négociation triennale sur l'égalité professionnelle suite à l'ordonnance de 2017 ?\",\n",
    "        \"ground_truth_answer\": \"L'ordonnance n° 2017-1385 du 22 septembre 2017 a modifié la négociation triennale sur l'égalité professionnelle entre les femmes et les hommes. Cette négociation porte notamment sur les conditions d'accès à l'emploi, à la formation et à la promotion professionnelle, ainsi que sur les conditions de travail et d'emploi, en particulier celles des salariés à temps partiel. Lorsque les mesures de rattrapage portent sur des mesures salariales, leur mise en œuvre est suivie dans le cadre de la négociation annuelle obligatoire sur les salaires.\",\n",
    "        \"ground_truth_relevant_document_pages\": [277] # Page 278 du document\n",
    "    }\n",
    "    # {\n",
    "    #     \"question\": \"Énumérez les sept critères légaux de représentativité des organisations syndicales de salariés.\",\n",
    "    #     \"ground_truth_answer\": \"Les sept critères légaux de représentativité des organisations syndicales de salariés sont : 1) Le respect des valeurs républicaines, 2) L'indépendance, 3) La transparence financière, 4) Une ancienneté minimale de deux ans dans le champ professionnel et géographique de la négociation, 5) L'audience (mesurée par les résultats aux élections professionnelles), 6) L'influence (caractérisée par l'activité et l'expérience), 7) Les effectifs d'adhérents et les cotisations.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [25] # Page 25 du document (Basé sur L. 2121-1)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelle est la mission des inspecteurs du travail et quel article du Code du travail la définit ?\",\n",
    "    #     \"ground_truth_answer\": \"Les inspecteurs du travail ont pour mission de veiller à l'application des dispositions légales relatives au régime du travail. Ils exercent un rôle de contrôle et de conseil. L'article L. 8112-1 du Code du travail définit cette mission.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [10] # Page 10 du document (Basé sur L. 8112-1)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"En dehors de l'égalité professionnelle, quels autres sujets sont abordés dans la négociation triennale ?\",\n",
    "    #     \"ground_truth_answer\": \"Outre l'égalité professionnelle, la négociation triennale aborde également les conditions de travail et la gestion prévisionnelle des emplois et des parcours professionnels (GPEC), notamment au travers du paragraphe 2 de l'article L. 2241-11 sur les conditions de travail et gestion prévisionnelle des emplois.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5] # Page 5 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"La loi n° 2020-1525 du 7 décembre 2020 est mentionnée en page 1. Quel est son impact sur le dialogue social ?\",\n",
    "    #     \"ground_truth_answer\": \"La loi n° 2020-1525 du 7 décembre 2020 est mentionnée en page 1 en lien avec le chapitre préliminaire sur le dialogue social. Cependant, les extraits de la page 1 ne décrivent pas en détail l'impact spécifique de cette loi sur le dialogue social, mais la situent dans ce contexte.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1] # Page 1 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quand le Gouvernement peut-il déroger à la procédure de concertation préalable pour une réforme du travail, et que doit-il faire dans ce cas ?\",\n",
    "    #     \"ground_truth_answer\": \"Le présent article (L.1) n'est pas applicable en cas d'urgence. Lorsque le Gouvernement décide de mettre en œuvre un projet de réforme en l'absence de procédure de concertation en raison de l'urgence, il doit faire connaître cette décision aux organisations syndicales et d'employeurs en la motivant dans un document transmis avant de prendre toute mesure.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1] # Page 1 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelles informations sont nécessaires à la négociation sur l'égalité professionnelle ?\",\n",
    "    #     \"ground_truth_answer\": \"Les informations nécessaires à la négociation sur l'égalité professionnelle entre les femmes et les hommes sont déterminées par voie réglementaire.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5] # Page 5 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Qu'est-ce que l'audience électorale dans le cadre de la représentativité syndicale, et à quel article fait-elle référence pour la négociation annuelle obligatoire ?\",\n",
    "    #     \"ground_truth_answer\": \"L'audience électorale est un critère de représentativité des organisations syndicales, mesuré notamment lors des élections professionnelles. La mise en œuvre des mesures de rattrapage salariales issues de la négociation sur l'égalité professionnelle est suivie dans le cadre de la négociation annuelle obligatoire sur les salaires, prévue à l'article L. 2241-8 du Code du travail.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [5, 25] # Pages 5 et 25 (exemple de multi-sources)\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quelle est l'importance de la transparence financière pour une organisation syndicale ?\",\n",
    "    #     \"ground_truth_answer\": \"La transparence financière est l'un des sept critères de représentativité des organisations syndicales de salariés, essentiel pour garantir leur légitimité et leur intégrité.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [25] # Page 25 du document\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Comment la loi de 2008 et l'ordonnance de 2017 ont-elles affecté le Code du travail concernant le dialogue social et l'égalité professionnelle ?\",\n",
    "    #     \"ground_truth_answer\": \"La loi n° 2008-67 du 21 janvier 2008 a introduit des dispositions concernant le chapitre préliminaire sur le dialogue social, notamment la concertation préalable pour les réformes du Gouvernement. L'ordonnance n° 2017-1385 du 22 septembre 2017, quant à elle, a modifié les mesures tendant à assurer l'égalité professionnelle entre les femmes et les hommes, impactant les négociations triennales sur ce sujet, y compris les conditions d'accès à l'emploi et de travail.\",\n",
    "    #     \"ground_truth_relevant_document_pages\": [1, 5] # Exemple de question nécessitant de multiples sources\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(f\"Jeu de données d'évaluation créé avec {len(evaluation_dataset)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21335,
     "status": "ok",
     "timestamp": 1749897681820,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "n1cRh_C2Lw0I",
    "outputId": "534f67f2-3630-4f3e-d05f-73e64c9baaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m184.3/190.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5215,
     "status": "ok",
     "timestamp": 1749897687037,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "m-e171sWQjbQ",
    "outputId": "16d70241-d0c9-448e-944e-abe9bcadefae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ragas\n",
      "Version: 0.2.15\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: appdirs, datasets, diskcache, langchain, langchain-community, langchain-core, langchain_openai, nest-asyncio, numpy, openai, pydantic, tiktoken\n",
      "Required-by: \n",
      "---\n",
      "Name: datasets\n",
      "Version: 3.6.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: ragas, torchtune\n"
     ]
    }
   ],
   "source": [
    "!pip show ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5391,
     "status": "ok",
     "timestamp": 1749897692431,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "tlXJiET6IXjE",
    "outputId": "5034fe27-c126-485b-9638-7d6d8b5a153e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Évaluation du Retrieval ---\n",
      "  Q1: 'Selon la loi de 2008, quelle est la procédure de concertation préalabl...'\n",
      "    GT Pages: [0], Retrieved Pages: [0, 21, 22, 37, 61, 68, 78, 79, 85, 92, 107, 181, 182, 187, 222, 255, 256]\n",
      "    Hit: True, Recall: 1.00, MRR: 1.00\n",
      "  Q2: 'Quels sont les sujets principaux de la négociation triennale sur l'éga...'\n",
      "    GT Pages: [277], Retrieved Pages: [12, 39, 114, 118, 121, 169, 170, 216, 217, 222, 228, 229, 253, 255, 261, 264, 272, 275, 276, 277]\n",
      "    Hit: True, Recall: 1.00, MRR: 1.00\n",
      "\n",
      "--- Résultats Généraux du Retrieval ---\n",
      "Average Hit Rate: 1.00\n",
      "Average Recall: 1.00\n",
      "Average MRR: 1.00\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda # Pour la conversion de la chaîne en liste\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def evaluate_retrieval(retriever_instance, eval_data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Évalue les métriques de récupération (Hit Rate, Recall, MRR) d'un retriever.\n",
    "\n",
    "    Args:\n",
    "        retriever_instance: L'instance du retriever LangChain à évaluer (ton hybrid_retriever).\n",
    "        eval_data: Le jeu de données d'évaluation.\n",
    "    \"\"\"\n",
    "    hit_rates = []\n",
    "    recalls_at_k = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    print(\"\\n--- Évaluation du Retrieval ---\")\n",
    "    for i, item in enumerate(eval_data):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth_pages = set(item[\"ground_truth_relevant_document_pages\"])\n",
    "\n",
    "        try:\n",
    "            # Il est primordial de s'assurer que la question est un dictionnaire si la chaîne d'entrée l'attend\n",
    "            # Si le rag_chain_with_sources.invoke(question) prend une string,\n",
    "            # alors le retriever.invoke() prendra aussi la string.\n",
    "            # Cependant, avec Multi-Query, le retriever est appelé via la chaîne interne.\n",
    "            # Pour l'évaluation du retriever seul, nous devons lui passer la question directe.\n",
    "            # Ici, nous allons simuler l'appel à la chaîne de multi-query + retriever agrégé.\n",
    "\n",
    "            # Étape de multi-query\n",
    "            # L'on s'assure que multi_query_chain attend un dictionnaire comme entrée si elle fait partie d'une RunnableSequence\n",
    "            generated_queries = multi_query_chain.invoke({\"question\": question})\n",
    "\n",
    "            # Exécution de l'EnsembleRetriever pour chaque query et agrégation\n",
    "            all_retrieved_docs = []\n",
    "            for q_gen in generated_queries:\n",
    "                # C'est ici que l'EnsembleRetriever (qui contient SelfQueryRetriever) est invoqué.\n",
    "                # Il prend directement la requête en string.\n",
    "                # Le retriever_instance est l'EnsembleRetriever\n",
    "                retrieved_for_query = retriever_instance.invoke(q_gen)\n",
    "                all_retrieved_docs.extend(retrieved_for_query)\n",
    "\n",
    "            # Déduplication des documents récupérés\n",
    "            # Un set de tuples (page_content, metadata_tuple) pour être sûr de la déduplication\n",
    "            unique_docs = []\n",
    "            seen_doc_ids = set() # Utilise un identifiant unique si disponible, sinon page_content\n",
    "            for doc in all_retrieved_docs:\n",
    "                doc_id = (doc.page_content, frozenset(doc.metadata.items())) # Crée un identifiant unique basé sur content + metadata\n",
    "                if doc_id not in seen_doc_ids:\n",
    "                    unique_docs.append(doc)\n",
    "                    seen_doc_ids.add(doc_id)\n",
    "\n",
    "            retrieved_docs = unique_docs\n",
    "\n",
    "            # Extraction des pages des documents récupérés\n",
    "            retrieved_pages = set([doc.metadata.get('page', -1) for doc in retrieved_docs if 'page' in doc.metadata])\n",
    "\n",
    "            # --- Calcul des métriques ---\n",
    "            # Hit Rate\n",
    "            # Vérifie si AU MOINS une page récupérée est dans les pages attendues\n",
    "            hit = any(p in ground_truth_pages for p in retrieved_pages)\n",
    "            hit_rates.append(1 if hit else 0)\n",
    "\n",
    "            # Recall@k (k est le nombre total de documents pertinents dans le ground truth)\n",
    "            # Mesure la proportion de documents pertinents attendus qui ont été effectivement récupérés.\n",
    "            if len(ground_truth_pages) > 0:\n",
    "                relevant_retrieved = len(ground_truth_pages.intersection(retrieved_pages))\n",
    "                recalls_at_k.append(relevant_retrieved / len(ground_truth_pages))\n",
    "            else: # Si pas de documents pertinents attendus, recall est 1 si 0 documents sont récupérés\n",
    "                recalls_at_k.append(1 if len(retrieved_pages) == 0 else 0)\n",
    "\n",
    "            # MRR (Mean Reciprocal Rank)\n",
    "            # Mesure où le premier document pertinent apparaît dans le classement.\n",
    "            mrr_score = 0\n",
    "            # On cherche la première page pertinente dans la liste ORDONNÉE des documents récupérés\n",
    "            # L'ordre ici dépend de la fusion des rangs de l'EnsembleRetriever\n",
    "            for rank, doc in enumerate(retrieved_docs):\n",
    "                if doc.metadata.get('page', -1) in ground_truth_pages:\n",
    "                    mrr_score = 1.0 / (rank + 1)\n",
    "                    break\n",
    "            mrr_scores.append(mrr_score)\n",
    "\n",
    "            print(f\"  Q{i+1}: '{question[:70]}...'\")\n",
    "            print(f\"    GT Pages: {sorted(list(ground_truth_pages))}, Retrieved Pages: {sorted(list(retrieved_pages))}\")\n",
    "            print(f\"    Hit: {hit}, Recall: {recalls_at_k[-1]:.2f}, MRR: {mrr_scores[-1]:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur lors de l'évaluation de la question '{question[:70]}...': {e}\")\n",
    "            hit_rates.append(0)\n",
    "            recalls_at_k.append(0)\n",
    "            mrr_scores.append(0)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Résultats Généraux du Retrieval ---\")\n",
    "    # On évite la division par zéro si eval_data est vide\n",
    "    num_questions = len(eval_data)\n",
    "    if num_questions > 0:\n",
    "        print(f\"Average Hit Rate: {sum(hit_rates) / num_questions:.2f}\")\n",
    "        print(f\"Average Recall: {sum(recalls_at_k) / num_questions:.2f}\")\n",
    "        print(f\"Average MRR: {sum(mrr_scores) / num_questions:.2f}\")\n",
    "    else:\n",
    "        print(\"Aucune question dans le jeu de données d'évaluation.\")\n",
    "\n",
    "\n",
    "# --- Appel de la fonction d'évaluation après la définition de toutes les chaînes ---\n",
    "# L'on s'assure que 'retriever' et 'multi_query_chain' sont définis avant cet appel.\n",
    "evaluate_retrieval(retriever, evaluation_dataset)\n",
    "\n",
    "\n",
    "# --- Intégration de RAGAS pour l'évaluation de la génération ---\n",
    "# NB: ragas doit être bien installer - !pip install -q ragas datasets\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ContextRelevance,\n",
    "    AnswerCorrectness\n",
    ")\n",
    "\n",
    "def evaluate_generation_with_ragas(rag_chain, eval_data: List[Dict], llm_model, embedding_model_ragas):\n",
    "    \"\"\"\n",
    "    Évalue la génération de la réponse RAG en utilisant RAGAS.\n",
    "    \"\"\"\n",
    "    data_for_ragas = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [], # Liste de listes de strings (contenu des docs)\n",
    "        \"ground_truth\": [] # Les réponses de référence humaines\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Préparation des données pour RAGAS (exécution du RAG pour chaque question) ---\")\n",
    "    for i, item in enumerate(eval_data):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth_answer = item.get(\"ground_truth_answer\", None)\n",
    "\n",
    "        try:\n",
    "            # L'entrée de rag_chain_with_sources est directement la string de la question\n",
    "            result = rag_chain.invoke(question) # Appelle du RAG complet\n",
    "\n",
    "            data_for_ragas[\"question\"].append(question)\n",
    "            data_for_ragas[\"answer\"].append(result.get(\"response\", \"No response generated\"))\n",
    "            # ragas attend une liste de strings pour le contexte\n",
    "            data_for_ragas[\"contexts\"].append([doc.page_content for doc in result.get(\"source_documents\", [])])\n",
    "            data_for_ragas[\"ground_truth\"].append(ground_truth_answer)\n",
    "            print(f\"  Processed Q{i+1}: '{question[:70]}...'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERREUR lors de l'exécution du RAG pour la question '{question[:70]}...': {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Ajout des valeurs vides pour ne pas casser RAGAS\n",
    "            data_for_ragas[\"question\"].append(question)\n",
    "            data_for_ragas[\"answer\"].append(\"ERROR: RAG chain failed\")\n",
    "            data_for_ragas[\"contexts\"].append([])\n",
    "            data_for_ragas[\"ground_truth\"].append(ground_truth_answer)\n",
    "\n",
    "    # S'assurer qu'il y a des données avant de créer le Dataset\n",
    "    if not data_for_ragas[\"question\"]:\n",
    "         print(\"Aucune donnée n'a pu être préparée pour RAGAS.\")\n",
    "         return\n",
    "\n",
    "    ragas_dataset = Dataset.from_dict(data_for_ragas)\n",
    "\n",
    "    print(\"\\n--- Exécution de l'évaluation RAGAS ---\")\n",
    "    # Liste des métriques à évaluer\n",
    "    metrics_to_evaluate = [\n",
    "        Faithfulness,\n",
    "        ContextRelevance,\n",
    "        AnswerCorrectness\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result_ragas = evaluate(\n",
    "            ragas_dataset,\n",
    "            metrics=metrics_to_evaluate,\n",
    "            llm=llm_model, # Utilisation du LLM pour les métriques basées sur LLM-as-a-Judge\n",
    "            embeddings=embedding_model_ragas, # Utilisation du modèle d'embedding pour les métriques basées sur embedding-as-a-Judge\n",
    "        )\n",
    "\n",
    "        df_results_ragas = result_ragas.to_dataframe()\n",
    "        print(\"\\n--- Résultats RAGAS Moyens ---\")\n",
    "        print(df_results_ragas.mean(numeric_only=True))\n",
    "        print(\"\\n--- Détail des Résultats RAGAS ---\")\n",
    "        print(df_results_ragas)\n",
    "\n",
    "    except Exception as e:\n",
    "          print(f\"ERREUR lors de l'exécution de l'évaluation RAGAS: {e}\")\n",
    "          import traceback\n",
    "          traceback.print_exc() # Afficher la trace complète de l'erreur pour le debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9051,
     "status": "ok",
     "timestamp": 1749897701487,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "RlHH9s0tvbAV",
    "outputId": "704eec54-2d1b-45d0-f40f-09adeefe5c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Préparation des données pour RAGAS (exécution du RAG pour chaque question) ---\n",
      "  Processed Q1: 'Selon la loi de 2008, quelle est la procédure de concertation préalabl...'\n",
      "  Processed Q2: 'Quels sont les sujets principaux de la négociation triennale sur l'éga...'\n",
      "\n",
      "--- Exécution de l'évaluation RAGAS ---\n",
      "ERREUR lors de l'exécution de l'évaluation RAGAS: 'property' object has no attribute 'get'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-17-3883995461>\", line 176, in evaluate_generation_with_ragas\n",
      "    result_ragas = evaluate(\n",
      "                   ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/_analytics.py\", line 227, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/evaluation.py\", line 176, in evaluate\n",
      "    validate_required_columns(dataset, metrics)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ragas/validation.py\", line 60, in validate_required_columns\n",
      "    required_columns = set(m.required_columns.get(metric_type, []))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'property' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "# Appel de la fonction d'évaluation RAGAS\n",
    "# L'on s'assure que rag_chain_with_sources, llm, et embedding_model sont définis.\n",
    "\n",
    "if 'rag_chain_with_sources' in globals() and 'llm' in globals() and 'embedding_model' in globals() and 'evaluation_dataset' in globals():\n",
    "    evaluate_generation_with_ragas(rag_chain_with_sources, evaluation_dataset, llm, embedding_model)\n",
    "else:\n",
    "    print(\"Impossible d'exécuter l'évaluation RAGAS : variables nécessaires non définies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1749897701491,
     "user": {
      "displayName": "Ruben Mougoué",
      "userId": "11449985091763533040"
     },
     "user_tz": -120
    },
    "id": "xvOOo1YKY_4a",
    "outputId": "a9193e10-a3b5-48e0-9070-6571bd6845aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 0, 'page_label': '1', 'chapter': 'Chapitre préliminaire : Dialogue social. \\nPartie législative'}\n",
      "{'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-03T20:04:09+00:00', 'source': '/content/drive/MyDrive/Projet_AI31/Code_du_travail-23-300.pdf', 'total_pages': 278, 'page': 4, 'page_label': '5'}\n"
     ]
    }
   ],
   "source": [
    "if documents:\n",
    "    print(documents[0].metadata) # Regardez la clé 'page' pour le premier document\n",
    "    print(documents[4].metadata) # Regardez la clé 'page' pour le cinquième document (si disponible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhBxVVzpJlF"
   },
   "source": [
    "## UTILITE DU PROJET\n",
    "1. **Qu'est-ce qu'une application RAG ? (Retrieval Augmented Generation)**\n",
    "Imaginons une immense bibliothèque (dans notre cas, un PDF de 2775 pages sur le droit du travail) et qu'un utilisateur pose une question très spécifique.\n",
    "\n",
    "**Sans RAG :** Si l'utilisateur posait cette question à un grand modèle de langage (LLM) comme Gemini sans RAG, il essayerait de répondre uniquement avec ce qu'il a appris pendant son entraînement. Le problème, c'est que Gemini n'a pas été spécifiquement entraîné sur le document de droit du travail. Il pourrait inventer des choses (\"halluciner\") ou donner des réponses génériques, car il ne \"connaît\" pas les détails du documents et donc du droit du travail.\n",
    "\n",
    "**Avec RAG :** Le RAG résout ce problème en ajoutant une étape cruciale : _la recherche d'informations (Retrieval)_ avant la _génération de la réponse (Generation)_.\n",
    "\n",
    "- **Recherche (Retrieval) :** Quand l'utilisateur poses une question, l'application RAG va d'abord chercher les passages les plus pertinents du document (corpus) qui sont susceptibles de contenir la réponse.\n",
    "- **Augmentation (Augmented) :** Ces passages pertinents sont ensuite donnés au LLM (Gemini) en même temps que ta question. Le LLM est alors \"augmenté\" avec des connaissances spécifiques.\n",
    "- **Génération (Generation) :** Fort de ces informations contextuelles, le LLM peut maintenant générer une réponse précise et basée sur les faits extraits de ton document."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
